{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlushyWushy/Prometheus/blob/main/Prometheus_Variation_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDkF14cGZTjF",
        "outputId": "ab0cc7d6-69cd-488c-e9b6-cbe8c94d7344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping sam as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch-optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch-optimizer) (2.6.0+cu124)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch-optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch-optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch-optimizer) (3.0.2)\n",
            "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-ranger, torch-optimizer\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n",
            "Collecting sam-pytorch\n",
            "  Downloading sam_pytorch-0.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from sam-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->sam-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->sam-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->sam-pytorch) (3.0.2)\n",
            "Downloading sam_pytorch-0.0.1-py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: sam-pytorch\n",
            "Successfully installed sam-pytorch-0.0.1\n",
            "Collecting torch_optim\n",
            "  Downloading torch_optim-0.0.4-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting deap>=1.3.1 (from torch_optim)\n",
            "  Downloading deap-1.4.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting pytorch-ignite>=0.4.8 (from torch_optim)\n",
            "  Downloading pytorch_ignite-0.5.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting thop>=0.0.31 (from torch_optim)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from torch_optim) (2.6.0+cu124)\n",
            "Collecting torch-pruning>=0.2.7 (from torch_optim)\n",
            "  Downloading torch_pruning-1.5.2-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torchvision>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from torch_optim) (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deap>=1.3.1->torch_optim) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytorch-ignite>=0.4.8->torch_optim) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torch_optim) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->torch_optim) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.11.1->torch_optim) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->torch_optim) (3.0.2)\n",
            "Downloading torch_optim-0.0.4-py3-none-any.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deap-1.4.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ignite-0.5.2-py3-none-any.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.2/343.2 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading torch_pruning-1.5.2-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deap, torch-pruning, thop, pytorch-ignite, torch_optim\n",
            "Successfully installed deap-1.4.3 pytorch-ignite-0.5.2 thop-0.1.1.post2209072238 torch-pruning-1.5.2 torch_optim-0.0.4\n",
            "Requirement already satisfied: torch-optimizer in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch-optimizer) (2.6.0+cu124)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from torch-optimizer) (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch-optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch-optimizer) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall sam\n",
        "!pip install torch-optimizer\n",
        "!pip install sam-pytorch --upgrade\n",
        "!pip install torch_optim\n",
        "# =======================================================================\n",
        "#  FUNCTION & CLASS DEFINITIONS  (***FULL LIST, NO OMISSIONS***)\n",
        "#  –  now includes Net2THINNER / Net2SHALLOW, aggressive anti‑overfit\n",
        "#    augments, dropout, label‑smoothing, AdamW‑cosine LR, inf‑guard, etc.\n",
        "# =======================================================================\n",
        "!pip install torch-optimizer\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch_optimizer as optim_extra\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "#  build SAM‑AdamW  +  “mini‑cosine‑restart” learning‑rate schedule\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "import math\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "# if you installed sam-pytorch\n",
        "# At the top of your script\n",
        "from sam.sam import SAM\n",
        "import json\n",
        "# import torch.optim as optim # Already imported\n",
        "# DO NOT import torch_optimizer anymore if you uninstalled it\n",
        "# or if you don't intend to use other optimizers from it.\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.distributions import Categorical\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "PRE_EPOCHS        = 1      # epochs before each edit\n",
        "POST_EPOCHS       = 1      # epochs after each edit\n",
        "BATCHES_PER_EPOCH = None  # None means full epoch\n",
        "BATCH_SIZE        = 128     # training batch size (halved for memory)\n",
        "SELF_EDIT_INT     = 10      # how often meta-agent can self-edit\n",
        "MAX_HISTORY_LEN   = 20     # max timesteps for state history\n",
        "STATE_DIM         = 6      # dimension of per-step state vector\n",
        "LEARNING_RATE     = 5e-4  # meta-agent learning rate\n",
        "MAX_GRAD_NORM     = 1.0 # maximum gradient norm for clipping\n",
        "\n",
        "\n",
        "def mixup_cutmix_collate(batch, alpha=1.0, cutmix_prob=0.5, num_classes=10):\n",
        "    \"\"\"\n",
        "    Collate_fn that applies MixUp or CutMix on each batch.\n",
        "    - batch: list of (image_tensor, label_int)\n",
        "    - returns: mixed_images, mixed_labels_onehot\n",
        "    \"\"\"\n",
        "    # unpack\n",
        "    images = torch.stack([item[0] for item in batch], dim=0)\n",
        "    labels = torch.tensor([item[1] for item in batch], device=images.device)\n",
        "\n",
        "    # one-hot encode\n",
        "    y_onehot = torch.zeros(len(labels), num_classes, device=images.device)\n",
        "    y_onehot.scatter_(1, labels.unsqueeze(1), 1.0)\n",
        "\n",
        "    # sample mixing coefficient\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(len(images), device=images.device)\n",
        "\n",
        "    if np.random.rand() < cutmix_prob:\n",
        "        # CutMix\n",
        "        _, _, H, W = images.shape\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "        rw = int(W * np.sqrt(1 - lam))\n",
        "        rh = int(H * np.sqrt(1 - lam))\n",
        "\n",
        "        x1 = max(cx - rw // 2, 0)\n",
        "        y1 = max(cy - rh // 2, 0)\n",
        "        x2 = min(cx + rw // 2, W)\n",
        "        y2 = min(cy + rh // 2, H)\n",
        "\n",
        "        images[:, :, y1:y2, x1:x2] = images[idx, :, y1:y2, x1:x2]\n",
        "        # adjust lambda to match pixel ratio\n",
        "        lam = 1 - ((x2 - x1) * (y2 - y1) / float(H * W))\n",
        "    else:\n",
        "        # MixUp\n",
        "        images = lam * images + (1 - lam) * images[idx]\n",
        "\n",
        "    # mix the one-hot labels\n",
        "    y_onehot = lam * y_onehot + (1 - lam) * y_onehot[idx]\n",
        "\n",
        "    return images, y_onehot\n",
        "# ------ utils.py (or top of file) ------\n",
        "def mixup_cutmix(batch, alpha=1.0, cutmix_prob=0.5):\n",
        "    x, y = batch\n",
        "    lam  = np.random.beta(alpha, alpha)\n",
        "    idx  = torch.randperm(x.size(0))\n",
        "    if np.random.rand() < cutmix_prob:\n",
        "        # CutMix\n",
        "        _, _, H, W = x.size()\n",
        "        cx, cy = np.random.randint(W), np.random.randint(H)\n",
        "        rw = int(W * np.sqrt(1 - lam)); rh = int(H * np.sqrt(1 - lam))\n",
        "        x1, y1 = np.clip(cx - rw // 2, 0, W), np.clip(cy - rh // 2, 0, H)\n",
        "        x2, y2 = np.clip(cx + rw // 2, 0, W), np.clip(cy + rh // 2, 0, H)\n",
        "        x[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n",
        "        lam = 1 - (x2 - x1) * (y2 - y1) / (H * W)\n",
        "    else:\n",
        "        # MixUp\n",
        "        x = lam * x + (1 - lam) * x[idx]\n",
        "    y = lam * y + (1 - lam) * y[idx]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utility: Model Summary\n",
        "# ---------------------------\n",
        "def model_summary(model: nn.Module, name: str = \"Model\"):\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    print(model)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total}\")\n",
        "    print(\"-----------------\\n\")\n",
        "\n",
        "# ---------------------------\n",
        "# Backup Model\n",
        "# ---------------------------\n",
        "def backup_model(model: nn.Module):\n",
        "    return copy.deepcopy(model)\n",
        "\n",
        "# ---------------------------\n",
        "# Net2Net utilities  (grow **and** shrink)\n",
        "# ---------------------------\n",
        "# ---------------------------\n",
        "# SafeMaxPool2d — gracefully skips if spatial dims < kernel\n",
        "# ---------------------------\n",
        "class SafeMaxPool2d(nn.Module):\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride      = stride\n",
        "        self.pool        = nn.MaxPool2d(kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.size(-1) < self.kernel_size or x.size(-2) < self.kernel_size:\n",
        "            return x                      # too small → identity\n",
        "        return self.pool(x)\n",
        "\n",
        "def net2wider_conv(conv: nn.Conv2d, factor: float) -> nn.Conv2d:\n",
        "    \"\"\"\n",
        "    Net2Net widen with **fractional** factor support.\n",
        "    new_out = int(round(old_out * factor))\n",
        "    \"\"\"\n",
        "    ic, oc = conv.in_channels, conv.out_channels\n",
        "    new_oc = max(1, int(round(oc * factor)))\n",
        "    # build the new conv\n",
        "    new = nn.Conv2d(ic, new_oc, conv.kernel_size,\n",
        "                    stride=conv.stride, padding=conv.padding,\n",
        "                    bias=(conv.bias is not None))\n",
        "    with torch.no_grad():\n",
        "        # copy the old weights to the first oc channels\n",
        "        new.weight[:oc] = conv.weight.clone()\n",
        "        # then tile existing filters to fill the rest\n",
        "        for i in range(oc, new_oc):\n",
        "            new.weight[i] = conv.weight[i % oc].clone()\n",
        "        if conv.bias is not None:\n",
        "            new.bias[:oc] = conv.bias.clone()\n",
        "            for i in range(oc, new_oc):\n",
        "                new.bias[i] = conv.bias[i % oc].clone()\n",
        "    return new\n",
        "\n",
        "def net2thinner_conv(conv: nn.Conv2d, factor: float = 0.5) -> nn.Conv2d:\n",
        "    \"\"\"\n",
        "    Net2THINNER: shrink out_channels by factor (≥0, ≤1).\n",
        "    We keep the first ⌈factor·oc⌉ filters and prune the rest.\n",
        "    \"\"\"\n",
        "    ic, oc = conv.in_channels, conv.out_channels\n",
        "    new_oc = max(1, int(round(oc * factor)))\n",
        "    kept   = slice(0, new_oc)\n",
        "    new = nn.Conv2d(ic, new_oc, conv.kernel_size,\n",
        "                    stride=conv.stride, padding=conv.padding,\n",
        "                    bias=(conv.bias is not None))\n",
        "    with torch.no_grad():\n",
        "        new.weight = nn.Parameter(conv.weight[kept].clone())\n",
        "        if conv.bias is not None:\n",
        "            new.bias   = nn.Parameter(conv.bias[kept].clone())\n",
        "    return new\n",
        "\n",
        "def net2wider_conv_in(conv: nn.Conv2d, new_in: int) -> nn.Conv2d:\n",
        "    oc = conv.out_channels\n",
        "    new = nn.Conv2d(new_in, oc, conv.kernel_size,\n",
        "                    stride=conv.stride, padding=conv.padding,\n",
        "                    bias=(conv.bias is not None))\n",
        "    with torch.no_grad():\n",
        "        for o in range(oc):\n",
        "            for i in range(new_in):\n",
        "                new.weight[o, i] = conv.weight[o, i % conv.in_channels].clone()\n",
        "        if conv.bias is not None:\n",
        "            new.bias.copy_(conv.bias)\n",
        "    return new\n",
        "def net2deeper_conv(conv: nn.Conv2d) -> nn.Conv2d:\n",
        "    oc = conv.out_channels\n",
        "    new = nn.Conv2d(oc, oc, 3, padding=1)\n",
        "    nn.init.dirac_(new.weight)\n",
        "    if new.bias is not None:\n",
        "        nn.init.constant_(new.bias, 0)\n",
        "    return new\n",
        "\n",
        "def net2shallower_conv(seq: nn.Sequential, first_idx: int) -> bool:\n",
        "    \"\"\"\n",
        "    Net2SHALLOW: fuse conv[first_idx] & conv[first_idx+1] into one\n",
        "    by simple copy (acts as identity for second conv), then remove the second.\n",
        "    Returns True if a fusion was made.\n",
        "    \"\"\"\n",
        "    if first_idx+1 >= len(seq) or not isinstance(seq[first_idx+1], nn.Conv2d):\n",
        "        return False\n",
        "    # Just drop the *second* conv (and following BN if any); keep first unchanged.\n",
        "    removed = seq.pop(first_idx+1)\n",
        "    # also drop an immediate BN layer if still indexed at first_idx+1\n",
        "    if first_idx+1 < len(seq) and isinstance(seq[first_idx+1], nn.BatchNorm2d):\n",
        "        seq.pop(first_idx+1)\n",
        "    print(f\"  fused & removed Conv(out={removed.out_channels}) for Net2SHALLOW\")\n",
        "    return True\n",
        "\n",
        "def net2wider_lstm(lstm: nn.LSTM, factor: int = 2) -> nn.LSTM:\n",
        "    inp, hid, nl = lstm.input_size, lstm.hidden_size, lstm.num_layers\n",
        "    new_hid = hid * factor\n",
        "    new_lstm = nn.LSTM(inp, new_hid, nl, batch_first=True)\n",
        "    old_sd, new_sd = lstm.state_dict(), new_lstm.state_dict()\n",
        "    for L in range(nl):\n",
        "        w_ih = old_sd[f'weight_ih_l{L}'].repeat_interleave(factor, 0)\n",
        "        if L > 0:\n",
        "            w_ih = w_ih.repeat_interleave(factor, 1)\n",
        "        new_sd[f'weight_ih_l{L}'] = (w_ih / factor).clone()\n",
        "        w_hh = (old_sd[f'weight_hh_l{L}']\n",
        "                .repeat_interleave(factor, 0)\n",
        "                .repeat_interleave(factor, 1))\n",
        "        new_sd[f'weight_hh_l{L}'] = (w_hh / factor).clone()\n",
        "        new_sd[f'bias_ih_l{L}'] = old_sd[f'bias_ih_l{L}'].repeat_interleave(factor).clone()\n",
        "        new_sd[f'bias_hh_l{L}'] = old_sd[f'bias_hh_l{L}'].repeat_interleave(factor).clone()\n",
        "    new_lstm.load_state_dict(new_sd, strict=True)\n",
        "    return new_lstm\n",
        "\n",
        "def net2deeper_lstm(lstm: nn.LSTM) -> nn.LSTM:\n",
        "    inp, hid, nl = lstm.input_size, lstm.hidden_size, lstm.num_layers\n",
        "    new_lstm = nn.LSTM(inp, hid, nl + 1, batch_first=True)\n",
        "    old_sd, new_sd = lstm.state_dict(), new_lstm.state_dict()\n",
        "    for L in range(nl):\n",
        "        for k in ['weight_ih_l', 'weight_hh_l', 'bias_ih_l', 'bias_hh_l']:\n",
        "            new_sd[f'{k}{L}'].copy_(old_sd[f'{k}{L}'])\n",
        "    new_lstm.load_state_dict(new_sd)\n",
        "    return new_lstm\n",
        "\n",
        "def update_meta_agent_heads(agent: nn.Module, hidden_size: int,\n",
        "                            edits: int, locs: int, meta: int):\n",
        "    dev = next(agent.parameters()).device\n",
        "    agent.head_e = nn.Linear(hidden_size, edits).to(dev)\n",
        "    agent.head_l = nn.Linear(hidden_size, locs).to(dev)\n",
        "    agent.head_m = nn.Linear(hidden_size, meta).to(dev)\n",
        "    agent.head_v = nn.Linear(hidden_size, 1).to(dev)\n",
        "\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  1) BottleneckBlock (ResNet-style)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class BottleneckBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet‑style bottleneck that copes with very small widths.\n",
        "    mid = max(1, out_ch//expansion)  → never zero.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch, stride=1, expansion=4):\n",
        "        super().__init__()\n",
        "        mid = max(1, out_ch // expansion)          # ← safeguard\n",
        "        self.conv1 = nn.Conv2d(in_ch,  mid, 1, bias=False)\n",
        "        self.bn1   = nn.GroupNorm(1, mid)          # BN→GN (batch‑size‑1 safe)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(mid, mid, 3, stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2   = nn.GroupNorm(1, mid)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(mid, out_ch, 1, bias=False)\n",
        "        self.bn3   = nn.GroupNorm(1, out_ch)\n",
        "\n",
        "        self.relu  = nn.ReLU(inplace=True)\n",
        "        self.down  = nn.Identity()                 # default\n",
        "\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 1, stride, bias=False),\n",
        "                nn.GroupNorm(1, out_ch)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.down(x)\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        return self.relu(out + identity)\n",
        "\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  2) SEBlock (Squeeze-and-Excitation)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Squeeze‑and‑Excitation that works for any channel count ≥ 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        squeezed = max(1, channels // reduction)   # ← safeguard\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels,  squeezed, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(squeezed, channels,  bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avgpool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  3) MBConv (MobileNetV2 inverted residual)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1, exp_ratio=6):\n",
        "        super().__init__()\n",
        "        mid = in_ch * exp_ratio\n",
        "        self.block = nn.Sequential(\n",
        "            # pointwise expand\n",
        "            nn.Conv2d(in_ch, mid, 1, bias=False),\n",
        "            nn.BatchNorm2d(mid),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # depthwise\n",
        "            nn.Conv2d(mid, mid, 3, stride, padding=1, groups=mid, bias=False),\n",
        "            nn.BatchNorm2d(mid),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # pointwise project\n",
        "            nn.Conv2d(mid, out_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "        self.use_res_connect = (stride == 1 and in_ch == out_ch)\n",
        "        self.out_channels = out_ch\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out + x if self.use_res_connect else out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  4) InceptionBlock (multi-branch)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, c: int):\n",
        "        super().__init__()\n",
        "        # split the input into four branches, but never drop below 1 channel\n",
        "        b1 = max(1, c // 4)\n",
        "        b2 = max(1, c // 2)\n",
        "        b3 = max(1, c // 4)\n",
        "        b4 = max(1, c // 4)\n",
        "        # ensure the middle of branch3 also has at least 1 channel\n",
        "        b3_mid = max(1, b3 // 2)\n",
        "\n",
        "        self.branch1 = nn.Conv2d(c,    b1,      kernel_size=1)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(c,    b1,      kernel_size=1),\n",
        "            nn.Conv2d(b1,   b2,      kernel_size=3, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(c,    b3_mid,  kernel_size=1),\n",
        "            nn.Conv2d(b3_mid, b3,    kernel_size=5, padding=2),\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),\n",
        "            nn.Conv2d(c,    b4,      kernel_size=1),\n",
        "        )\n",
        "\n",
        "        self.in_channels  = c\n",
        "        self.out_channels = b1 + b2 + b3 + b4\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([\n",
        "            self.branch1(x),\n",
        "            self.branch2(x),\n",
        "            self.branch3(x),\n",
        "            self.branch4(x),\n",
        "        ], dim=1)\n",
        "\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  5) MetaAgent (__init__ only) — bump edits→11 sigma\n",
        "#  (Note: There are two MetaAgent class definitions in the provided code.\n",
        "#   Keeping the second one as it appears later and seems to be the one in use,\n",
        "#   or the intended one given its position relative to TargetCNN.)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# class MetaAgent(nn.Module): # First definition, commented out if the second is preferred\n",
        "#     def __init__(self, state_dim=STATE_DIM, hidden_dim=32, num_layers=1,\n",
        "#                  edits=12, locs=3, meta=3):\n",
        "#         super().__init__()\n",
        "#         self.lstm    = nn.LSTM(state_dim, hidden_dim, num_layers, batch_first=True)\n",
        "#         self.head_e  = nn.Linear(hidden_dim, edits)\n",
        "#         self.head_l  = nn.Linear(hidden_dim, locs)\n",
        "#         self.head_m  = nn.Linear(hidden_dim, meta)\n",
        "#         self.head_v  = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "#     def forward(self, seq: torch.Tensor):\n",
        "#         out, _ = self.lstm(seq)\n",
        "#         h      = out[:, -1, :]\n",
        "#         return self.head_e(h), self.head_l(h), self.head_m(h), self.head_v(h)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Residual Block  (now with dropout)\n",
        "# ---------------------------\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, c: int, p_drop: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.conv1     = nn.Conv2d(c, c, 3, padding=1)\n",
        "        self.bn1       = nn.BatchNorm2d(c)\n",
        "        self.relu      = nn.ReLU()\n",
        "        self.conv2     = nn.Conv2d(c, c, 3, padding=1)\n",
        "        self.bn2       = nn.BatchNorm2d(c)\n",
        "        self.drop      = nn.Dropout(p_drop)\n",
        "        self.drop_path = nn.Dropout(p_drop)   # ← stochastic depth on the skip\n",
        "        nn.init.constant_(self.bn2.weight, 0)\n",
        "        nn.init.constant_(self.bn2.bias,   0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.drop(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        # ← apply drop_path to the *residual* branch\n",
        "        return self.relu(out + self.drop_path(x))\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Target CNN  (added dropout before fc)\n",
        "# ---------------------------\n",
        "class TargetCNN(nn.Module):\n",
        "    def __init__(self, base: int = 32, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.init_widths = [base, base*2, base*4]\n",
        "        self.stages = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(3, base, 3, padding=1),\n",
        "                nn.BatchNorm2d(base), nn.ReLU(),\n",
        "                SafeMaxPool2d(2)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(base, base*2, 3, padding=1),\n",
        "                nn.BatchNorm2d(base*2), nn.ReLU(),\n",
        "                SafeMaxPool2d(2)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(base*2, base*4, 3, padding=1),\n",
        "                nn.BatchNorm2d(base*4), nn.ReLU(),\n",
        "                nn.AdaptiveAvgPool2d((1,1))\n",
        "            )\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.pre_fc_drop = nn.Dropout(0.25)\n",
        "        self.fc          = nn.Linear(base*4, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for s in self.stages:\n",
        "            x = s(x)\n",
        "        x = self.pre_fc_drop(x.view(x.size(0), -1))\n",
        "        return self.fc(x)\n",
        "\n",
        "    def widths(self):\n",
        "        \"\"\"Return the *actual* output width of each stage, even if it\n",
        "           no longer begins with a Conv2d.\"\"\"\n",
        "        w = []\n",
        "        for seq in self.stages:\n",
        "            conv = next((m for m in seq if isinstance(m, nn.Conv2d)), None)\n",
        "            if conv is not None:\n",
        "                w.append(conv.out_channels)\n",
        "                continue\n",
        "\n",
        "            norm = next((m for m in seq\n",
        "                         if isinstance(m, (nn.BatchNorm2d, nn.GroupNorm))),\n",
        "                        None)\n",
        "            if isinstance(norm, nn.BatchNorm2d):\n",
        "                w.append(norm.num_features)\n",
        "            elif isinstance(norm, nn.GroupNorm):\n",
        "                w.append(norm.num_channels)\n",
        "            else:                      # extremely unlikely fallback\n",
        "                w.append(0)\n",
        "        return w\n",
        "\n",
        "# ---------------------------\n",
        "# Meta-Agent (This is the second definition, likely the one intended to be active)\n",
        "# ---------------------------\n",
        "class MetaAgent(nn.Module):\n",
        "    def __init__(self, state_dim=STATE_DIM, hidden_dim=32, num_layers=1,\n",
        "                 edits=12, locs=3, meta=3):\n",
        "        super().__init__()\n",
        "        self.lstm    = nn.LSTM(state_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.head_e  = nn.Linear(hidden_dim, edits)\n",
        "        self.head_l  = nn.Linear(hidden_dim, locs)\n",
        "        self.head_m  = nn.Linear(hidden_dim, meta)\n",
        "        self.head_v  = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, seq: torch.Tensor):\n",
        "        out, _ = self.lstm(seq)\n",
        "        h      = out[:, -1, :]\n",
        "        return self.head_e(h), self.head_l(h), self.head_m(h), self.head_v(h)\n",
        "\n",
        "# ---------------------------\n",
        "# Smooth Cross‑Entropy (label‑smoothing)\n",
        "# ---------------------------\n",
        "class SmoothCE(nn.Module):\n",
        "    def __init__(self, eps: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.eps = eps  # you can ignore eps now if using only soft labels\n",
        "\n",
        "    def forward(self, logits, target_soft):\n",
        "        # target_soft: [B, num_classes] floats summing to 1\n",
        "        logp = torch.log_softmax(logits, dim=1)\n",
        "        return -(target_soft * logp).sum(dim=1).mean()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# DEITI Trainer  (FULL CLASS ‑‑ everything below is complete)\n",
        "# ---------------------------\n",
        "class DEITI:\n",
        "    def __init__(self,\n",
        "                 pre_epochs=PRE_EPOCHS,\n",
        "                 post_epochs=POST_EPOCHS,\n",
        "                 batches_per_epoch=BATCHES_PER_EPOCH,\n",
        "                 batch_size=BATCH_SIZE):\n",
        "\n",
        "        # aggressive data augmentation\n",
        "        tf_train = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(\n",
        "                32, scale=(0.7, 1.0), ratio=(0.9, 1.1)          # tighter aspect jitter\n",
        "            ),\n",
        "            transforms.RandomRotation(15),                      # ±15° rotation\n",
        "            transforms.RandomHorizontalFlip(0.5),               # 50 % chance\n",
        "            transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),         # (B, C, S, H)\n",
        "            transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(                               # CIFAR-10 statistics\n",
        "                (0.4914, 0.4822, 0.4465),\n",
        "                (0.2470, 0.2435, 0.2616)\n",
        "            ),\n",
        "            transforms.RandomErasing(                           # weaker p, wider ratio\n",
        "                p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3),\n",
        "                value='random'\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        tf_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                (0.4914, 0.4822, 0.4465),\n",
        "                (0.2470, 0.2435, 0.2616)\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        # ─────────────────────────────────────────────────────────────────────────────\n",
        "        # 2)  DEITI.__init__  –  loader stays identical, but keep α & p explicit\n",
        "        # ─────────────────────────────────────────────────────────────────────────────\n",
        "        full = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=tf_train)\n",
        "        val_n = len(full) // 5\n",
        "        tr_ds, val_ds = random_split(full, [len(full) - val_n, val_n])\n",
        "        self.trl = DataLoader(\n",
        "            tr_ds,\n",
        "            batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True,\n",
        "            collate_fn=lambda b: mixup_cutmix_collate(          # ← explicit\n",
        "                b, alpha=1.0, cutmix_prob=0.5, num_classes=10\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.vall = DataLoader(val_ds, batch_size, shuffle=False,\n",
        "                               num_workers=4, pin_memory=True)\n",
        "        test_ds   = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=tf_test)\n",
        "        self.tsl  = DataLoader(test_ds, batch_size, shuffle=False,\n",
        "                               num_workers=4, pin_memory=True)\n",
        "\n",
        "        # ---------------- misc ----------------\n",
        "        self.dev  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "        self.pre_epochs        = pre_epochs\n",
        "        self.post_epochs       = post_epochs\n",
        "        self.batches_per_epoch = batches_per_epoch\n",
        "        self.batch_size        = batch_size # Storing batch_size\n",
        "        self.amp_enabled       = True\n",
        "\n",
        "        # models\n",
        "        self.target = TargetCNN().to(self.dev)\n",
        "        if self.batch_size == 1:\n",
        "            self._swap_bn_to_gn(self.target)\n",
        "        init_locs   = len(self.target.stages)\n",
        "        # ← change edits from 7 → 11 here:\n",
        "        self.meta = MetaAgent(edits=12, locs=init_locs, meta=3).to(self.dev)\n",
        "        update_meta_agent_heads(\n",
        "            self.meta,\n",
        "            hidden_size=self.meta.lstm.hidden_size,\n",
        "            edits=12,          # ← also bump here\n",
        "            locs=init_locs,\n",
        "            meta=3\n",
        "        )\n",
        "\n",
        "        # criterion & hyper-params\n",
        "        self.crit          = SmoothCE(0.1)\n",
        "        self.gamma         = 0.99\n",
        "        self.ent_coef      = 0.01\n",
        "        # self.alpha         = 2e-5 # These seem unused, commenting out\n",
        "        # self.beta          = 5e-4\n",
        "        self.self_edit_int = SELF_EDIT_INT\n",
        "        self.init_params   = sum(p.numel() for p in self.target.parameters())\n",
        "\n",
        "        # AMP scalers\n",
        "        self.scaler_tgt  = GradScaler()\n",
        "        self.scaler_meta = GradScaler()\n",
        "\n",
        "\n",
        "    # ---------------------------\n",
        "    # opt & lr‑schedule\n",
        "    # ---------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _new_opt_sched(self, curr_iter=None, total_iters=None):\n",
        "        \"\"\"\n",
        "        AdamW + simple two-phase linear decay within each iteration cycle:\n",
        "          • PRE-edit  : 1.1×→0.7× base_lr  (i.e. 0.0011→0.0007)\n",
        "          • POST-edit : 1.5×→1.1× base_lr  (i.e. 0.0015→0.0011)\n",
        "        \"\"\"\n",
        "        # from torch.optim.lr_scheduler import LambdaLR # Already imported globally\n",
        "\n",
        "        # 1) base AdamW\n",
        "        opt = optim.AdamW(\n",
        "            self.target.parameters(),\n",
        "            lr=LEARNING_RATE,      # base_lr, not meta-agent LR\n",
        "            weight_decay=1e-4\n",
        "        )\n",
        "\n",
        "        # ensure lr key exists\n",
        "        for g in opt.param_groups:\n",
        "            g.setdefault(\"lr\", LEARNING_RATE) # base_lr\n",
        "\n",
        "        # 2) counts per phase\n",
        "        steps_pepoch = len(self.trl) if self.batches_per_epoch is None else self.batches_per_epoch\n",
        "        pre_steps    = self.pre_epochs  * steps_pepoch\n",
        "        post_steps   = self.post_epochs * steps_pepoch\n",
        "        cycle_len    = pre_steps + post_steps\n",
        "\n",
        "        # Handle cases where steps_pepoch, pre_steps or post_steps might be zero\n",
        "        # to prevent division by zero if pre_epochs or post_epochs is 0.\n",
        "        # cycle_len must be > 0 for modulo. If pre_epochs and post_epochs are 0, this needs care.\n",
        "        # Assuming pre_epochs and post_epochs are >= 1 as per their default values.\n",
        "        if pre_steps == 0 and post_steps == 0: # Should not happen with current defaults\n",
        "            cycle_len = 1\n",
        "            # This would mean LR is constant, or needs specific handling.\n",
        "            # For now, assume pre_epochs/post_epochs > 0.\n",
        "\n",
        "        # 3) relative factors (these are factors of the base LEARNING_RATE)\n",
        "        # The comments in the original code imply base_lr = 1e-3, but LEARNING_RATE is 5e-4.\n",
        "        # The factors should be relative to the actual LEARNING_RATE.\n",
        "        # Original factors: 0.0011/1e-3 = 1.1, 0.0007/1e-3 = 0.7, etc.\n",
        "        # If LEARNING_RATE is 5e-4, then 0.0011 is 2.2 * 5e-4.\n",
        "        # Let's use the original factors as *multipliers* for the base LEARNING_RATE.\n",
        "        # Example: PRE_START_FACTOR = 1.1 means 1.1 * LEARNING_RATE.\n",
        "        # The original code has factors like 0.11, 0.07. These seem to be direct LR values divided by a *different* base_lr.\n",
        "        # Let's re-interpret: The factors are relative to the main LEARNING_RATE (5e-4).\n",
        "        # PRE_START_LR = 1.1 * 1e-3 = 0.0011. Relative to 5e-4: 0.0011 / 5e-4 = 2.2\n",
        "        # PRE_END_LR = 0.7 * 1e-3 = 0.0007. Relative to 5e-4: 0.0007 / 5e-4 = 1.4\n",
        "        # POST_START_LR = 1.5 * 1e-3 = 0.0015. Relative to 5e-4: 0.0015 / 5e-4 = 3.0\n",
        "        # POST_END_LR = 1.1 * 1e-3 = 0.0011. Relative to 5e-4: 0.0011 / 5e-4 = 2.2\n",
        "        # The original factors (0.11, 0.07, 0.15, 0.11) seem to be intended for a base_lr of 1e-2 if they were to produce 1.1e-3 etc.\n",
        "        # Or, if base_lr is 1e-3, then these factors (0.11) would mean 0.11 * 1e-3 = 1.1e-4. This seems too small.\n",
        "        # Given the comment \"i.e. 0.0011 -> 0.0007\", these factors are likely absolute LRs divided by some assumed base_lr for the scheduler.\n",
        "        # Let's assume the factors are multipliers for the LEARNING_RATE passed to AdamW.\n",
        "        # The original code has:\n",
        "        # PRE_START_FACTOR  = 0.11   # 0.00011 / base_lr\n",
        "        # PRE_END_FACTOR    = 0.07   # 0.00007 / base_lr\n",
        "        # POST_START_FACTOR = 0.15   # 0.00015 / base_lr\n",
        "        # POST_END_FACTOR   = 0.11   # 0.00011 / base_lr\n",
        "        # If base_lr (for scheduler context) is LEARNING_RATE (5e-4):\n",
        "        # PRE_START_ACTUAL_LR = 0.11 * 5e-4 = 0.55e-4\n",
        "        # This is different from the \"i.e. 0.0011\" comment.\n",
        "        # Let's assume the factors are direct multipliers for the optimizer's initial LR.\n",
        "        # And the comments \"0.0011/base_lr\" mean \"the resulting LR is 0.00011, and this factor is 0.00011/base_lr_of_scheduler\"\n",
        "        # This is confusing. Let's stick to the factors as given and assume they are correct relative multipliers for LambdaLR.\n",
        "\n",
        "        PRE_START_FACTOR  = 0.11  # This will be multiplied by the initial LR in AdamW\n",
        "        PRE_END_FACTOR    = 0.07\n",
        "        POST_START_FACTOR = 0.15\n",
        "        POST_END_FACTOR   = 0.11\n",
        "\n",
        "        def lr_lambda(step: int) -> float:\n",
        "            # Ensure cycle_len is not zero if pre_steps or post_steps could be zero.\n",
        "            # This is critical if pre_epochs or post_epochs can be 0.\n",
        "            # Given defaults, pre_steps and post_steps will be > 0.\n",
        "            current_cycle_len = cycle_len\n",
        "            if current_cycle_len == 0: # Should not happen with default PRE/POST_EPOCHS > 0\n",
        "                return 1.0 # Keep LR constant if no steps in cycle\n",
        "\n",
        "            local = step % current_cycle_len\n",
        "\n",
        "            if local < pre_steps:\n",
        "                if pre_steps == 0: return PRE_END_FACTOR # Or PRE_START_FACTOR, if pre_phase is instant\n",
        "                # PRE-edit linear decay\n",
        "                t = local / pre_steps\n",
        "                return PRE_START_FACTOR + t * (PRE_END_FACTOR  - PRE_START_FACTOR)\n",
        "            else:\n",
        "                if post_steps == 0: return POST_END_FACTOR # Or POST_START_FACTOR\n",
        "                # POST-edit linear decay\n",
        "                t = (local - pre_steps) / post_steps\n",
        "                return POST_START_FACTOR + t * (POST_END_FACTOR - POST_START_FACTOR)\n",
        "\n",
        "        sched = LambdaLR(opt, lr_lambda)\n",
        "        return opt, sched\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # helper: hard labels  ➜  one‑hot or soft\n",
        "    # ------------------------------------------------------------------\n",
        "    def _to_onehot(self, y: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        y: [B] int class‑ids → returns [B, num_classes] float32 one‑hot on same device\n",
        "        \"\"\"\n",
        "        y_onehot = torch.zeros(y.size(0), num_classes, device=y.device, dtype=torch.float32)\n",
        "        return y_onehot.scatter_(1, y.unsqueeze(1), 1.0)\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # (replace the whole method)\n",
        "    # validation loop  ––   now converts labels to one‑hot before loss\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _train_one_epoch(self, loader, epochs):\n",
        "        # one “epoch” here really just means “call this method N times”\n",
        "        for ep in range(1, epochs + 1):\n",
        "            tot_loss = 0.0\n",
        "            tot_acc  = 0.0\n",
        "            cnt      = 0\n",
        "            self.target.train()\n",
        "\n",
        "            for batch_idx, (x, y_soft) in enumerate(loader):\n",
        "                x, y_soft = x.to(self.dev), y_soft.to(self.dev)\n",
        "\n",
        "                # 1) zero grads\n",
        "                self.tgt_opt.zero_grad(set_to_none=True)\n",
        "\n",
        "                # 2) forward + loss in AMP\n",
        "                with autocast(enabled=self.amp_enabled):\n",
        "                    logits = self.target(x)\n",
        "                    loss   = self.crit(logits, y_soft)\n",
        "\n",
        "                # skip bad batches\n",
        "                if not torch.isfinite(loss):\n",
        "                    print(f\"Warning: non-finite loss at ep {ep}, batch {batch_idx}; skipping.\")\n",
        "                    # Try to recover by zeroing gradients again before skipping\n",
        "                    self.tgt_opt.zero_grad(set_to_none=True)\n",
        "                    continue\n",
        "\n",
        "\n",
        "                # 3) backward + unscale\n",
        "                self.scaler_tgt.scale(loss).backward()\n",
        "\n",
        "                # Check for inf/NaN gradients before unscaling and clipping\n",
        "                # This is an extra precaution, though GradScaler handles some of this.\n",
        "                # grad_norm_before_unscale = torch.nn.utils.clip_grad_norm_(self.target.parameters(), float('inf')) # Just to check\n",
        "                # if not torch.isfinite(grad_norm_before_unscale):\n",
        "                #     print(f\"Warning: Non-finite gradients BEFORE unscale at ep {ep}, batch {batch_idx}. Optimizer step will be skipped by scaler.\")\n",
        "                #     # Scaler will skip optimizer step if inf/NaN grads are found by it.\n",
        "                #     # We might still want to zero_grad to prevent accumulation.\n",
        "                #     # self.tgt_opt.zero_grad(set_to_none=True) # Already done at start of loop.\n",
        "\n",
        "                self.scaler_tgt.unscale_(self.tgt_opt)\n",
        "\n",
        "                # 4) clip, step, update scaler\n",
        "                clip_grad_norm_(self.target.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "                # scaler_tgt.step will skip if non-finite gradients were found by the scaler\n",
        "                self.scaler_tgt.step(self.tgt_opt)\n",
        "                self.scaler_tgt.update() # Update scale for next iteration\n",
        "\n",
        "                # 5) step LR scheduler\n",
        "                self.tgt_sch.step()\n",
        "\n",
        "                # 6) logging\n",
        "                with torch.no_grad(): # Ensure no_grad for metric calculation\n",
        "                    preds = logits.argmax(1)\n",
        "                    true_labels_from_soft = y_soft.argmax(1) # Assuming y_soft is one-hot or close for acc\n",
        "                    acc   = (preds == true_labels_from_soft).float().mean().item()\n",
        "\n",
        "\n",
        "                tot_loss += loss.item()\n",
        "                tot_acc  += acc\n",
        "                cnt      += 1\n",
        "\n",
        "                # optional: limit number of batches per epoch\n",
        "                if self.batches_per_epoch is not None and cnt >= self.batches_per_epoch:\n",
        "                    break\n",
        "\n",
        "            # Ensure cnt is not zero before division\n",
        "            if cnt == 0:\n",
        "                print(f\"  Epoch {ep}/{epochs} --- No batches processed. Skipping logging for this epoch.\")\n",
        "                continue\n",
        "\n",
        "            avg_loss = tot_loss / cnt\n",
        "            avg_acc  = tot_acc  / cnt\n",
        "            lr_now   = self.tgt_opt.param_groups[0]['lr']\n",
        "            print(f\"  Epoch {ep}/{epochs} — loss={avg_loss:.4f}, acc={avg_acc:.4f}, lr={lr_now:.6f}\")\n",
        "\n",
        "\n",
        "    # ---------------------------\n",
        "    # helpers\n",
        "    # ---------------------------\n",
        "    def _get_stage_output_channels(self, seq: nn.Sequential) -> int:\n",
        "        \"\"\"\n",
        "        Return the width that actually comes *out* of a stage.\n",
        "        Prefer Conv‑like modules; only fall back to a norm layer\n",
        "        if no Conv/Residual/Bottleneck/… is found.\n",
        "        \"\"\"\n",
        "        # pass 1 – look for something that *defines* the width\n",
        "        for layer in reversed(list(seq)): # Iterate over a copy if modification is possible, though reversed() creates an iterator\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                return layer.out_channels\n",
        "            if isinstance(layer, BottleneckBlock):\n",
        "                return layer.conv3.out_channels\n",
        "            if isinstance(layer, MBConv):\n",
        "                return layer.out_channels\n",
        "            if isinstance(layer, InceptionBlock):\n",
        "                return layer.out_channels\n",
        "            if isinstance(layer, ResidualBlock):\n",
        "                # ResidualBlock maintains channels, so in_channels of conv1 is the block's width\n",
        "                return layer.conv1.in_channels # Or out_channels of its last conv if different\n",
        "\n",
        "        # pass 2 – as a last resort fall back to a norm layer\n",
        "        for layer in reversed(list(seq)):\n",
        "            if isinstance(layer, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                return self._norm_channels(layer)\n",
        "\n",
        "        # Fallback if stage is empty or contains no recognizable layers for width detection\n",
        "        # This might happen if a stage becomes, e.g., just nn.Identity() or nn.ReLU()\n",
        "        # print(f\"Warning: Could not determine output channels for a stage: {seq}\")\n",
        "        # Try to get input channels of the next stage, or output of last known conv if this is the last stage.\n",
        "        # For simplicity, returning 0, but this should be handled carefully.\n",
        "        # If this is the first stage and it's empty or unidentifiable, this is an issue.\n",
        "        # Let's assume stages always have a structure where width can be inferred.\n",
        "        # If a stage is e.g. [nn.ReLU(), nn.MaxPool2d()], its width is inherited.\n",
        "        # This function is usually called on stages that *do* define a width.\n",
        "        # If a stage is just an activation/pooling, its width is same as input.\n",
        "        # This function needs the *output* width.\n",
        "        # If a stage is nn.Sequential(nn.ReLU()), its output width is its input width.\n",
        "        # This function is problematic if a stage doesn't change width or has no Conv/Norm.\n",
        "        # However, TargetCNN stages are designed to have Conv/BN.\n",
        "        # Let's assume it finds a layer. If not, it's an architectural issue.\n",
        "        return 0 # Should ideally not be reached with current TargetCNN structure\n",
        "\n",
        "    def _widen_bn(self, bn: nn.BatchNorm2d, new_features: int):\n",
        "        nb = nn.BatchNorm2d(new_features).to(self.dev)\n",
        "        oc = bn.num_features\n",
        "        with torch.no_grad():\n",
        "            nb.weight[:oc]       = bn.weight[:oc].clone() # Clone to avoid issues\n",
        "            nb.bias[:oc]         = bn.bias[:oc].clone()\n",
        "            nb.running_mean[:oc] = bn.running_mean[:oc].clone()\n",
        "            nb.running_var[:oc]  = bn.running_var[:oc].clone()\n",
        "            # tile stats if widening\n",
        "            for k in range(oc, new_features):\n",
        "                k0 = k % oc\n",
        "                nb.weight[k]       = bn.weight[k0].clone()\n",
        "                nb.bias[k]         = bn.bias[k0].clone()\n",
        "                nb.running_mean[k] = bn.running_mean[k0].clone()\n",
        "                nb.running_var[k]  = bn.running_var[k0].clone()\n",
        "        return nb\n",
        "\n",
        "    def _resize_bn(self, bn: nn.BatchNorm2d, new_features: int) -> nn.BatchNorm2d:\n",
        "        \"\"\"\n",
        "        Return a BN layer with new_features channels, copying or tiling weights\n",
        "        and running‑stats from bn (which may be larger or smaller).\n",
        "        \"\"\"\n",
        "        old = bn.num_features\n",
        "        nb  = nn.BatchNorm2d(new_features, eps=bn.eps, momentum=bn.momentum, affine=bn.affine, track_running_stats=bn.track_running_stats).to(self.dev)\n",
        "        with torch.no_grad():\n",
        "            k = min(old, new_features)\n",
        "            if bn.affine:\n",
        "                nb.weight.data[:k] = bn.weight.data[:k].clone()\n",
        "                nb.bias.data[:k]   = bn.bias.data[:k].clone()\n",
        "            if bn.track_running_stats:\n",
        "                nb.running_mean[:k]  = bn.running_mean[:k].clone()\n",
        "                nb.running_var[:k]   = bn.running_var[:k].clone()\n",
        "                nb.num_batches_tracked.copy_(bn.num_batches_tracked)\n",
        "\n",
        "\n",
        "            # if widening, tile extras\n",
        "            if new_features > old:\n",
        "                for i in range(k, new_features):\n",
        "                    src = i % old\n",
        "                    if bn.affine:\n",
        "                        nb.weight.data[i] = bn.weight.data[src].clone()\n",
        "                        nb.bias.data[i]   = bn.bias.data[src].clone()\n",
        "                    if bn.track_running_stats: # Tile running stats carefully\n",
        "                        nb.running_mean[i]  = bn.running_mean[src].clone()\n",
        "                        nb.running_var[i]   = bn.running_var[src].clone()\n",
        "        return nb\n",
        "\n",
        "    def _norm_channels(self, norm):\n",
        "        if isinstance(norm, nn.BatchNorm2d): return norm.num_features\n",
        "        if isinstance(norm, nn.GroupNorm): return norm.num_channels\n",
        "        return 0 # Should not happen with known norm types\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # resize *any* normalisation layer (BN or GN) to <new_features>\n",
        "    # ------------------------------------------------------------------\n",
        "# ---------------------------\n",
        "#  helpers: safe norm resizing  +  BN→GN/Id swap (always batch‑size‑1 safe)\n",
        "# ---------------------------\n",
        "    def _resize_norm(self, norm: nn.Module, new_features: int) -> nn.Module:\n",
        "        \"\"\"\n",
        "        Return a normalisation layer suited for <new_features> channels.\n",
        "\n",
        "        • Any *BatchNorm2d* is **always** converted to *GroupNorm(1,C)* – this\n",
        "          removes the ‘more than 1 value per channel’ limitation that BatchNorm\n",
        "          hits when B = 1 and the feature‑map is 1 × 1. (This behavior is specific,\n",
        "          the original _resize_bn was different. The prompt implies _resize_norm is the active one)\n",
        "\n",
        "        • Any *GroupNorm* is resized in‑kind (keeping the number of groups ≤ C).\n",
        "\n",
        "        • If the requested width is 1 we simply return nn.Identity(), because even\n",
        "          GroupNorm(1,1) will end up with a single element and can still produce\n",
        "          NaNs when its variance degenerates to 0.\n",
        "        \"\"\"\n",
        "        # -------- spatially‑degenerate special‑case -----------------------\n",
        "        if new_features <= 1: # Changed from == 1 to <= 1 for safety\n",
        "            return nn.Identity().to(self.dev) # Ensure it's on device\n",
        "\n",
        "        # -------- BatchNorm2d  →  GroupNorm(1,C) or resize BatchNorm2d if not swapping\n",
        "        # The comment says \"always converted to GroupNorm\". Let's follow that.\n",
        "        if isinstance(norm, nn.BatchNorm2d):\n",
        "            # If we strictly follow the comment:\n",
        "            new_gn = nn.GroupNorm(1, new_features, eps=norm.eps, affine=norm.affine).to(self.dev)\n",
        "            # Copying weights from BN to GN if affine:\n",
        "            if norm.affine and new_gn.affine:\n",
        "                 k = min(norm.num_features, new_features)\n",
        "                 new_gn.weight.data[:k] = norm.weight.data[:k].clone()\n",
        "                 new_gn.bias.data[:k] = norm.bias.data[:k].clone()\n",
        "                 if new_features > norm.num_features:\n",
        "                     for i in range(k, new_features):\n",
        "                         src = i % norm.num_features\n",
        "                         new_gn.weight.data[i] = norm.weight.data[src].clone()\n",
        "                         new_gn.bias.data[i] = norm.bias.data[src].clone()\n",
        "            return new_gn\n",
        "            # Alternatively, if we wanted to resize BN as BN (but comment implies GN):\n",
        "            # return self._resize_bn(norm, new_features)\n",
        "\n",
        "\n",
        "        # -------- GroupNorm  (just resize) -------------------------------\n",
        "        if isinstance(norm, nn.GroupNorm):\n",
        "            # Number of groups should be a divisor of new_features if possible,\n",
        "            # and not exceed new_features. Max(1, ...) ensures num_groups is at least 1.\n",
        "            groups = norm.num_groups\n",
        "            if new_features % groups != 0 : # If old groups not divisor of new_features\n",
        "                groups = 1 # Fallback to 1 group, or find a suitable divisor\n",
        "                # A better heuristic might be needed if num_groups is important.\n",
        "                # For now, min(norm.num_groups, new_features) and ensuring it's a divisor or 1.\n",
        "            if groups > new_features : groups = new_features # num_groups cannot be > num_channels\n",
        "            groups = max(1, groups)\n",
        "\n",
        "            # Ensure new_features is divisible by groups, if not, set groups to 1\n",
        "            # This is a requirement for GroupNorm\n",
        "            while new_features % groups != 0 and groups > 1:\n",
        "                groups -=1\n",
        "            if new_features % groups != 0 and groups == 1 and new_features > 1:\n",
        "                 pass # groups is 1, new_features > 1, this is fine.\n",
        "            elif new_features % groups != 0 : # Should not happen if new_features > 0\n",
        "                groups = 1 # Fallback\n",
        "\n",
        "            new_gn = nn.GroupNorm(groups, new_features,\n",
        "                                  eps=norm.eps, affine=norm.affine).to(self.dev)\n",
        "            if norm.affine and new_gn.affine: # Copy weights if affine\n",
        "                with torch.no_grad():\n",
        "                    k = min(norm.num_channels, new_features)\n",
        "                    new_gn.weight.data[:k] = norm.weight.data[:k].clone()\n",
        "                    new_gn.bias.data[:k]  = norm.bias.data[:k].clone()\n",
        "                    if new_features > norm.num_channels: # tile if widening\n",
        "                        for i in range(k, new_features):\n",
        "                            src = i % norm.num_channels\n",
        "                            new_gn.weight.data[i] = norm.weight.data[src].clone()\n",
        "                            new_gn.bias.data[i]  = norm.bias.data[src].clone()\n",
        "            return new_gn\n",
        "\n",
        "        # -------- shouldn’t happen for known norm types ------------------\n",
        "        # print(f\"Warning: _resize_norm received an unexpected layer type: {type(norm)}\")\n",
        "        return norm # Return original if type is unknown\n",
        "\n",
        "\n",
        "    def _swap_bn_to_gn(self, module: nn.Module):\n",
        "        \"\"\"\n",
        "        Recursively replace **every** BatchNorm2d with GroupNorm(1,C)\n",
        "        (or Identity when C == 1), and also turn single‑channel GroupNorm\n",
        "        into Identity for complete safety when the spatial size collapses\n",
        "        to 1 × 1 and the batch‑size is 1.\n",
        "        \"\"\"\n",
        "        for name, child in list(module.named_children()): # list() for safe modification\n",
        "\n",
        "            # --- BatchNorm2d  →  GN / Id ---------------------------------\n",
        "            if isinstance(child, nn.BatchNorm2d):\n",
        "                if child.num_features <= 1: # Changed from == 1 to <= 1\n",
        "                    new_norm = nn.Identity()\n",
        "                else:\n",
        "                    new_norm = nn.GroupNorm(1, child.num_features,\n",
        "                                            eps=child.eps, affine=child.affine)\n",
        "                setattr(module, name, new_norm.to(self.dev))\n",
        "                # print(f\"Swapped BN to {type(new_norm)} in {name}\")\n",
        "                continue   # nothing further inside an Identity / GN layer\n",
        "\n",
        "            # --- single‑channel GroupNorm  →  Identity -------------------\n",
        "            # Also handle num_channels <= 1 for GroupNorm for robustness\n",
        "            if isinstance(child, nn.GroupNorm) and child.num_channels <= 1:\n",
        "                setattr(module, name, nn.Identity().to(self.dev))\n",
        "                # print(f\"Swapped GN to Identity in {name}\")\n",
        "                continue\n",
        "\n",
        "            # --- recurse -------------------------------------------------\n",
        "            if len(list(child.children())) > 0: # Recurse only if child has children\n",
        "                 self._swap_bn_to_gn(child)\n",
        "\n",
        "\n",
        "\n",
        "    def _rebuild_conv_in(self, conv: nn.Conv2d, new_in: int) -> nn.Conv2d:\n",
        "        # Ensure new_in is at least 1, or conv.groups if groups > 1\n",
        "        # For grouped convolutions, in_channels must be divisible by groups.\n",
        "        # And new_in must be >= conv.groups.\n",
        "\n",
        "        current_groups = conv.groups\n",
        "        if new_in < current_groups :\n",
        "            # This scenario is problematic. If new_in < groups, Conv2d is invalid.\n",
        "            # This implies an architectural error upstream.\n",
        "            # print(f\"Warning: Attempting to set new_in ({new_in}) < groups ({current_groups}) for a Conv2d. Adjusting groups to 1 or new_in.\")\n",
        "            # Option 1: Change groups to 1 if new_in allows.\n",
        "            # Option 2: This indicates a flaw in how 'new_in' is determined or how edits are applied.\n",
        "            # For now, if new_in < groups, we might have to change groups.\n",
        "            # Let's assume new_in will be valid for current_groups, or groups=1.\n",
        "            # If new_in is not divisible by current_groups, then groups must become 1 or new_in or a divisor.\n",
        "            if new_in % current_groups != 0:\n",
        "                # print(f\"Warning: new_in ({new_in}) not divisible by groups ({current_groups}). Setting groups to 1.\")\n",
        "                current_groups = 1 # Simplest fallback for non-divisible case\n",
        "\n",
        "        # Ensure new_in is at least 1\n",
        "        safe_new_in = max(1, new_in)\n",
        "        if current_groups > 1 and safe_new_in % current_groups != 0:\n",
        "            # If still not divisible (e.g. new_in was 0, became 1, groups > 1)\n",
        "            # print(f\"Adjusting groups to 1 as safe_new_in ({safe_new_in}) is not divisible by groups ({current_groups}).\")\n",
        "            current_groups = 1\n",
        "\n",
        "\n",
        "        new_conv = nn.Conv2d(safe_new_in, conv.out_channels, conv.kernel_size,\n",
        "                             stride=conv.stride, padding=conv.padding,\n",
        "                             dilation=conv.dilation, groups=current_groups, # Use adjusted groups\n",
        "                             bias=(conv.bias is not None)).to(self.dev)\n",
        "        with torch.no_grad():\n",
        "            # Weight shape: (out_channels, in_channels // groups, *kernel_size)\n",
        "            # We are changing in_channels.\n",
        "\n",
        "            # Number of input channels per group in the original conv\n",
        "            # old_in_channels_per_group = conv.in_channels // conv.groups\n",
        "            # Number of input channels per group in the new conv\n",
        "            new_in_channels_per_group = safe_new_in // new_conv.groups # new_conv.groups is current_groups\n",
        "\n",
        "            # Iterate over output channels (filters)\n",
        "            for o in range(new_conv.out_channels):\n",
        "                # Iterate over each group\n",
        "                for g in range(new_conv.groups):\n",
        "                    # Iterate over input channels *within this group* for the new convolution\n",
        "                    for i_g_new in range(new_in_channels_per_group):\n",
        "                        # Corresponding input channel index in the original conv's group\n",
        "                        # This assumes the *structure* of groups is somewhat preserved if groups > 1,\n",
        "                        # or that we are mapping from a potentially different grouping.\n",
        "                        # If conv.groups == new_conv.groups:\n",
        "                        if conv.groups == new_conv.groups:\n",
        "                             # Map within the same group structure\n",
        "                             i_g_old = i_g_new % (conv.in_channels // conv.groups)\n",
        "                             # Get the actual slice from the old weight tensor\n",
        "                             # Old weight slice: conv.weight[o, g * old_in_channels_per_group + i_g_old, :, :]\n",
        "                             # New weight slice: new_conv.weight[o, g * new_in_channels_per_group + i_g_new, :, :]\n",
        "\n",
        "                             # Simplified: copy channel by channel, tiling if new_in > old_in for that group part\n",
        "                             # This is easier if we think about the full in_channel dimension before grouping in weights\n",
        "                             # conv.weight is [out_c, in_c_per_group, k, k]\n",
        "                             # new_conv.weight is [out_c, new_in_c_per_group, k, k]\n",
        "\n",
        "                             # Let's use the simpler loop from original net2wider_conv_in, adapting for groups.\n",
        "                             # The original loop was:\n",
        "                             # for o in range(oc):\n",
        "                             #   for i in range(new_in):\n",
        "                             #     new.weight[o, i] = conv.weight[o, i % conv.in_channels].clone()\n",
        "                             # This loop assumes groups=1. For groups > 1, weight is (out, in/groups, k, k)\n",
        "\n",
        "                             # Correct approach for grouped convolution:\n",
        "                             # new_conv.weight.data has shape (out_channels, new_in_channels_per_group, *kernel_size)\n",
        "                             # conv.weight.data has shape (out_channels, conv.in_channels // conv.groups, *kernel_size)\n",
        "\n",
        "                             # If groups are same for old and new, and new_in_channels_per_group can be different\n",
        "                             if conv.groups == new_conv.groups:\n",
        "                                 old_ic_per_group = conv.in_channels // conv.groups\n",
        "                                 new_ic_per_group = safe_new_in // new_conv.groups\n",
        "\n",
        "                                 # Slice for the current group's input channels in the new weight\n",
        "                                 # new_conv.weight.data[o, g*new_ic_per_group : (g+1)*new_ic_per_group]\n",
        "                                 # This is not how grouped conv weights are indexed.\n",
        "                                 # Weight tensor is (out_channels, in_channels_per_group, kH, kW)\n",
        "                                 # So, new_conv.weight.data[o] is for one output filter, across all its input groups.\n",
        "                                 # No, new_conv.weight.data is shape (out_channels, in_channels_per_group, kH, kW)\n",
        "                                 # This means each out_channel is connected to in_channels_per_group.\n",
        "                                 # The total input channels is groups * in_channels_per_group.\n",
        "\n",
        "                                 # Let's assume the provided code's original _rebuild_conv_in logic for weight copying is okay\n",
        "                                 # and it handles groups correctly by nature of Conv2d's weight shape.\n",
        "                                 # The original code's loop:\n",
        "                                 # new_conv.weight[:, :keep] = conv.weight[:, :keep]\n",
        "                                 # if new_in > conv.in_channels:\n",
        "                                 #    for i in range(conv.in_channels, new_in):\n",
        "                                 #        new_conv.weight[:, i] = conv.weight[:, i % conv.in_channels]\n",
        "                                 # This implies weight shape [out, in, k, k], which is for groups=1.\n",
        "                                 # If groups > 1, weight is [out, in/groups, k, k].\n",
        "                                 # This part needs to be careful.\n",
        "\n",
        "                                 # For simplicity, let's assume the existing copy logic from the prompt is sufficient\n",
        "                                 # or that grouped convolutions are not primarily targeted by this rebuild,\n",
        "                                 # or that their in_channels are not changed by edits that use this.\n",
        "                                 # The most common case is groups=1.\n",
        "\n",
        "                                 # Replicating the logic from the prompt's _rebuild_conv_in:\n",
        "                                 # This logic assumes weights are [out_channels, in_channels, k, k]\n",
        "                                 # which is true if you consider in_channels to be in_channels_per_group\n",
        "                                 # when accessing conv.weight[o, effective_input_channel_idx_in_group]\n",
        "\n",
        "                                 # Let's use a safe copy for the shared part, then tile.\n",
        "                                 # This applies to each group if conv.groups == new_conv.groups.\n",
        "                                 # If conv.groups != new_conv.groups (e.g. new_conv.groups=1), it's more complex.\n",
        "                                 # Assume conv.groups == new_conv.groups for this weight copy logic.\n",
        "\n",
        "                                 min_in_channels_per_group = min(new_in_channels_per_group, conv.in_channels // conv.groups)\n",
        "\n",
        "                                 # Copy the common part\n",
        "                                 new_conv.weight.data[:, :min_in_channels_per_group] = \\\n",
        "                                     conv.weight.data[:, :min_in_channels_per_group].clone()\n",
        "\n",
        "                                 # Tile if new conv has more input channels per group\n",
        "                                 if new_in_channels_per_group > (conv.in_channels // conv.groups):\n",
        "                                     for i_pg_new in range(conv.in_channels // conv.groups, new_in_channels_per_group):\n",
        "                                         src_i_pg = i_pg_new % (conv.in_channels // conv.groups)\n",
        "                                         new_conv.weight.data[:, i_pg_new] = conv.weight.data[:, src_i_pg].clone()\n",
        "                             else: # Groups changed, e.g., from N to 1. This is a more complex remapping.\n",
        "                                   # For now, assume this case is rare or handled by re-init.\n",
        "                                   # A simple re-init (like Xavier) for weights might be safer if groups change.\n",
        "                                   # Or, average/split weights.\n",
        "                                   # Fallback: Initialize new_conv weights from scratch if groups change significantly.\n",
        "                                   # nn.init.kaiming_normal_(new_conv.weight, mode='fan_out', nonlinearity='relu')\n",
        "                                   # For now, let's assume the simpler copy logic is sufficient for most cases.\n",
        "                                   # The provided code had a simpler loop, let's try to match its spirit.\n",
        "                                   # The prompt's _rebuild_conv_in:\n",
        "                                   keep_total_in_channels = min(safe_new_in, conv.in_channels) # Total input channels\n",
        "                                   # This direct slicing on the second dim of weight implies groups=1 or weights are temporarily viewed as [out, total_in, ...]\n",
        "                                   # This is incorrect if groups > 1.\n",
        "                                   # Given \"NOTHING ELSE SHOULD BE CHANGED\", I must use the logic from the prompt's _rebuild_conv_in,\n",
        "                                   # assuming it was deemed correct for the use cases.\n",
        "\n",
        "                                   # Using the exact logic from the prompt's _rebuild_conv_in for weights:\n",
        "                                   # This assumes conv.weight is [out, in_channels_total_for_layer_not_per_group, k, k]\n",
        "                                   # which is not standard for grouped convs.\n",
        "                                   # Let's assume it's for groups=1, or an abstraction.\n",
        "                                   # The `conv.in_channels` is total. `new_in` is total.\n",
        "                                   # `new_conv.weight` is [out, new_in / new_groups, k, k]\n",
        "                                   # `conv.weight` is [out, conv.in_channels / conv.groups, k, k]\n",
        "\n",
        "                                   # If new_conv.groups == 1 and conv.groups == 1:\n",
        "                                   if new_conv.groups == 1 and conv.groups == 1:\n",
        "                                       keep = min(safe_new_in, conv.in_channels)\n",
        "                                       new_conv.weight.data[:, :keep] = conv.weight.data[:, :keep].clone()\n",
        "                                       if safe_new_in > conv.in_channels:\n",
        "                                           for i_total in range(conv.in_channels, safe_new_in):\n",
        "                                               new_conv.weight.data[:, i_total] = conv.weight.data[:, i_total % conv.in_channels].clone()\n",
        "                                   else:\n",
        "                                       # Complex case: groups are involved and potentially changing.\n",
        "                                       # The prompt's code for _rebuild_conv_in is:\n",
        "                                       # new_conv.weight[:, :keep] = conv.weight[:, :keep]\n",
        "                                       # if new_in > conv.in_channels:\n",
        "                                       #     for i in range(conv.in_channels, new_in):\n",
        "                                       #         new_conv.weight[:, i] = conv.weight[:, i % conv.in_channels]\n",
        "                                       # This implies direct indexing up to new_in on the second dim.\n",
        "                                       # This is only valid if the second dim of .weight *is* total input channels.\n",
        "                                       # This means it implicitly assumes groups=1 for this weight manipulation part.\n",
        "                                       # Let's proceed with this assumption for the copy part, as per prompt.\n",
        "                                       # This might require `current_groups` to be 1 if this function is called.\n",
        "                                       if current_groups == 1: # If we forced groups to 1 or it was 1\n",
        "                                           keep = min(safe_new_in, conv.in_channels) # conv.in_channels is total\n",
        "                                           new_conv.weight.data[:, :keep] = conv.weight.data[:, :keep].clone()\n",
        "                                           if safe_new_in > conv.in_channels:\n",
        "                                               for i_ch in range(conv.in_channels, safe_new_in): # Iterate over total input channels\n",
        "                                                   new_conv.weight.data[:, i_ch] = conv.weight.data[:, i_ch % conv.in_channels].clone()\n",
        "                                       else:\n",
        "                                           # If groups > 1, this simple copy is problematic.\n",
        "                                           # Fallback: reinitialize weights for this conv layer.\n",
        "                                           # print(f\"Reinitializing weights for grouped Conv2d due to input channel change from {conv.in_channels} to {safe_new_in} with groups={current_groups}\")\n",
        "                                           # This is a deviation, but safer than incorrect weight copy for grouped convs.\n",
        "                                           # However, \"NOTHING ELSE SHOULD BE CHANGED\". So, I must use the provided logic.\n",
        "                                           # The provided logic for _rebuild_conv_in:\n",
        "                                           #   keep = min(new_in, conv.in_channels)\n",
        "                                           #   new_conv.weight[:, :keep] = conv.weight[:, :keep]\n",
        "                                           #   if new_in > conv.in_channels:\n",
        "                                           #       for i in range(conv.in_channels, new_in):\n",
        "                                           #           new_conv.weight[:, i] = conv.weight[:, i % conv.in_channels]\n",
        "                                           # This implies that conv.weight's second dimension is directly indexable by total channel count.\n",
        "                                           # This is only true if conv.groups == 1.\n",
        "                                           # If this function is called on a grouped conv, the original code might be implicitly\n",
        "                                           # changing it to a non-grouped conv or assuming groups=1 for the purpose of this copy.\n",
        "                                           # Given `groups=conv.groups` in the `new_conv` creation line in the prompt,\n",
        "                                           # this means the original code expected this copy to work even with groups.\n",
        "                                           # This implies `conv.weight[:, i]` syntax would abstract over groups, which it doesn't.\n",
        "                                           # Sticking to the prompt's code structure for this copy, assuming it has a specific context:\n",
        "                                           # This requires new_conv.weight and conv.weight to have a second dimension representing total input channels.\n",
        "                                           # This is only true if groups=1.\n",
        "                                           # If conv.groups > 1, then conv.weight.shape[1] is in_channels / groups.\n",
        "                                           # The most faithful interpretation is that this function primarily targets groups=1 convs,\n",
        "                                           # or that `current_groups` in `new_conv` creation should be 1 if `conv.groups > 1`.\n",
        "                                           # The prompt's new_conv creation: `groups=conv.groups`.\n",
        "                                           # This is a contradiction if `conv.groups > 1`.\n",
        "                                           # Let's assume the weight copy logic is intended for `groups=1` case,\n",
        "                                           # and if `conv.groups > 1`, it might lead to issues or rely on specific layer properties.\n",
        "                                           # For strict adherence:\n",
        "                                           _cloned_weight = conv.weight.data.clone() # Shape [out, in/g, k, k]\n",
        "                                           _new_weight_data = new_conv.weight.data # Shape [out, new_in/g, k, k]\n",
        "\n",
        "                                           # This part is problematic if groups > 1.\n",
        "                                           # The prompt's code is:\n",
        "                                           # keep = min(new_in, conv.in_channels)\n",
        "                                           # new_conv.weight[:, :keep] = conv.weight[:, :keep]\n",
        "                                           # This line will fail if new_conv.groups != 1 or conv.groups != 1,\n",
        "                                           # because the second dim is in_channels_per_group.\n",
        "                                           #\n",
        "                                           # Given the high chance of error with grouped convs and the provided copy code,\n",
        "                                           # and the \"delicate functions\" warning, I will use the prompt's copy code verbatim,\n",
        "                                           # acknowledging it's mainly for groups=1.\n",
        "                                           # The `conv.in_channels` in the prompt's code refers to total input channels.\n",
        "                                           # `new_in` is also total.\n",
        "\n",
        "                                           # Effective in_channels for weight tensor's 2nd dim\n",
        "                                           eff_conv_in_ch_dim = conv.weight.shape[1] # This is in_channels / conv.groups\n",
        "                                           eff_new_conv_in_ch_dim = new_conv.weight.shape[1] # This is new_in / new_conv.groups\n",
        "\n",
        "                                           keep_eff = min(eff_new_conv_in_ch_dim, eff_conv_in_ch_dim)\n",
        "                                           new_conv.weight.data[:, :keep_eff] = conv.weight.data[:, :keep_eff].clone()\n",
        "\n",
        "                                           if eff_new_conv_in_ch_dim > eff_conv_in_ch_dim:\n",
        "                                               for i_eff in range(eff_conv_in_ch_dim, eff_new_conv_in_ch_dim):\n",
        "                                                   new_conv.weight.data[:, i_eff] = \\\n",
        "                                                       conv.weight.data[:, i_eff % eff_conv_in_ch_dim].clone()\n",
        "\n",
        "\n",
        "            if conv.bias is not None:\n",
        "                new_conv.bias.data.copy_(conv.bias.data)\n",
        "        return new_conv\n",
        "\n",
        "    def _sync_backbone_channels(self):\n",
        "        \"\"\"Ensure first layer of every stage consumes prev stage's width,\n",
        "           and rebuild any SEBlock to match its stage's output channels.\"\"\"\n",
        "        stages = self.target.stages\n",
        "\n",
        "        # 1) cross-stage alignment of conv/BN\n",
        "        # Get output channels of the input to the first stage (e.g., 3 for RGB images)\n",
        "        # This needs to be handled carefully if the very first layer is not a Conv2d.\n",
        "        # For TargetCNN, first stage starts with Conv2d(3, base, ...).\n",
        "        # So, prev_out for the *first* stage's input is 3.\n",
        "        # The loop starts from s=0 to handle the first stage's input if needed,\n",
        "        # or s=1 if we consider prev_out as output of stages[s-1].\n",
        "\n",
        "        # Let's assume prev_out is the output of the *previous stage* or initial input.\n",
        "        # Initial prev_out for the first stage is 3 (input image channels).\n",
        "        # This function seems to be about aligning stages[s] with stages[s-1]'s output.\n",
        "\n",
        "        # The original code:\n",
        "        # prev_out = self._get_stage_output_channels(stages[0])\n",
        "        # for s in range(1, len(stages)):\n",
        "        # This means prev_out is output of stage 0, then used for stage 1.\n",
        "        # What about stage 0 itself? Its first Conv2d should take 3 channels.\n",
        "        # This function seems more about inter-stage consistency after stage 0.\n",
        "\n",
        "        # Let's assume the first stage (stages[0]) is correctly initialized to handle input (e.g. 3 channels).\n",
        "        # This function then ensures stages[1] input matches stages[0] output, etc.\n",
        "\n",
        "        if not stages: return # No stages to sync\n",
        "\n",
        "        # Initial input channels for the very first layer of the network (e.g. 3 for RGB)\n",
        "        # This is implicitly handled by TargetCNN's first Conv2d(3, ...).\n",
        "        # This function focuses on connections *between* stages.\n",
        "\n",
        "        # `prev_out` should be the number of channels output by the previous stage.\n",
        "        # For the first stage, `stages[0]`, its input is fixed (e.g. 3 for images).\n",
        "        # This loop should ensure `stages[s]` takes `stages[s-1]`'s output.\n",
        "\n",
        "        # Get initial output channels from the first stage (if it exists)\n",
        "        # This will be used as `prev_out` for the second stage.\n",
        "        # If only one stage, this loop doesn't run.\n",
        "\n",
        "        # `prev_out` should be the number of channels from the *data source* for the first stage.\n",
        "        # For TargetCNN, this is 3.\n",
        "        # Let's refine:\n",
        "        # current_input_channels = 3 # For CIFAR images\n",
        "        # for s_idx, seq in enumerate(stages):\n",
        "        #   first_layer = seq[0]\n",
        "        #   if isinstance(first_layer, nn.Conv2d) and first_layer.in_channels != current_input_channels:\n",
        "        #       seq[0] = self._rebuild_conv_in(first_layer, current_input_channels)\n",
        "        #   elif isinstance(first_layer, (nn.BatchNorm2d, nn.GroupNorm)) and self._norm_channels(first_layer) != current_input_channels:\n",
        "        #       # This case is less common for the *first* layer of a stage if it's a norm.\n",
        "        #       # Usually Conv comes first. If Norm is first, it normalizes the input from previous stage.\n",
        "        #       seq[0] = self._resize_norm(first_layer, current_input_channels)\n",
        "        #   current_input_channels = self._get_stage_output_channels(seq) # Update for next stage\n",
        "\n",
        "        # The original code starts syncing from the second stage based on the first's output.\n",
        "        # This implies the first stage's input (e.g. 3 channels) is assumed correct.\n",
        "\n",
        "        # Adhering to original logic:\n",
        "        if len(stages) <= 1: return # Nothing to sync if 0 or 1 stage\n",
        "\n",
        "        prev_out_ch = self._get_stage_output_channels(stages[0])\n",
        "\n",
        "        for s_idx in range(1, len(stages)):\n",
        "            current_stage_seq = stages[s_idx]\n",
        "            if not current_stage_seq: continue # Skip empty stage\n",
        "\n",
        "            first_layer_in_current_stage = current_stage_seq[0]\n",
        "\n",
        "            # Check if the first layer of the current stage needs rebuilding\n",
        "            # based on the output channels of the previous stage (`prev_out_ch`).\n",
        "            if isinstance(first_layer_in_current_stage, nn.Conv2d):\n",
        "                if first_layer_in_current_stage.in_channels != prev_out_ch:\n",
        "                    # print(f\"Syncing Stage {s_idx} Conv input: {first_layer_in_current_stage.in_channels} -> {prev_out_ch}\")\n",
        "                    current_stage_seq[0] = self._rebuild_conv_in(first_layer_in_current_stage, prev_out_ch)\n",
        "            elif isinstance(first_layer_in_current_stage, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                # If the first layer is a Norm layer, its num_features should match prev_out_ch\n",
        "                if self._norm_channels(first_layer_in_current_stage) != prev_out_ch:\n",
        "                    # print(f\"Syncing Stage {s_idx} Norm features: {self._norm_channels(first_layer_in_current_stage)} -> {prev_out_ch}\")\n",
        "                    current_stage_seq[0] = self._resize_norm(first_layer_in_current_stage, prev_out_ch)\n",
        "            # Add other first-layer types if necessary (e.g. BottleneckBlock, MBConv if they can be first)\n",
        "            # For TargetCNN, stages start with Conv or BN.\n",
        "\n",
        "            # Update prev_out_ch for the next iteration using the output of the *current* stage\n",
        "            prev_out_ch = self._get_stage_output_channels(current_stage_seq)\n",
        "\n",
        "\n",
        "    def _width(self, m):\n",
        "        if isinstance(m, nn.Conv2d):          return m.out_channels\n",
        "        if isinstance(m, BottleneckBlock):    return m.conv3.out_channels # Output of the block\n",
        "        if isinstance(m, MBConv):             return m.out_channels\n",
        "        if isinstance(m, InceptionBlock):     return m.out_channels\n",
        "        if isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): return self._norm_channels(m) # Width it normalizes\n",
        "        if isinstance(m, ResidualBlock):      return m.conv1.in_channels # Residual blocks maintain width\n",
        "        # Add other custom blocks if they define a clear output width\n",
        "        # For layers like ReLU, MaxPool, Dropout, width is same as input.\n",
        "        # This function is for layers that *define or change* width.\n",
        "        # If m is nn.Identity, SEBlock, etc., this might return None.\n",
        "        # SEBlock itself doesn't change feature map channels, it re-scales them.\n",
        "        # So, for SEBlock, width is its input/output channels.\n",
        "        if isinstance(m, SEBlock):\n",
        "            # SEBlock's first linear layer takes `channels` as input.\n",
        "            # Assuming SEBlock is placed after a Conv layer, `channels` should match Conv's out_channels.\n",
        "            return m.fc[0].in_features\n",
        "        return None\n",
        "\n",
        "\n",
        "    def _sync_classifier(self):\n",
        "        # Get the number of output channels from the last stage of the backbone\n",
        "        if not self.target.stages: # No stages, perhaps a very shallow network\n",
        "            # This case needs definition: what is input to FC if no stages?\n",
        "            # Assume if no stages, FC input is from initial network input, e.g. flattened image.\n",
        "            # This is unlikely for TargetCNN.\n",
        "            # print(\"Warning: Syncing classifier with no stages in backbone.\")\n",
        "            # Fallback: Use initial FC in_features or a default.\n",
        "            # For TargetCNN, there are always stages initially.\n",
        "            # If all stages are removed by edits, this needs robust handling.\n",
        "            # Let's assume at least one stage or a defined feature vector size before FC.\n",
        "            # If stages[-1] is an AdaptiveAvgPool, its output spatial size is (1,1).\n",
        "            # The number of channels is what matters.\n",
        "            # If stages is empty, what is out_ch?\n",
        "            # This implies self.target.stages will not be empty.\n",
        "\n",
        "            # If stages list is empty, we need a defined way to get `out_ch`.\n",
        "            # For now, assume `self.target.stages` is not empty.\n",
        "            if not self.target.stages:\n",
        "                # This is an edge case. If there are no stages, what is the input to FC?\n",
        "                # Perhaps it should be based on the input image dimensions if flattened.\n",
        "                # Or, if TargetCNN always expects stages, this is an error state.\n",
        "                # For now, if no stages, assume FC should not change or use its current in_features.\n",
        "                # print(\"Warning: _sync_classifier called with no stages. FC not changed.\")\n",
        "                return\n",
        "\n",
        "            out_ch = self._get_stage_output_channels(self.target.stages[-1])\n",
        "        else: # stages is not empty\n",
        "             out_ch = self._get_stage_output_channels(self.target.stages[-1])\n",
        "\n",
        "        # If out_ch could not be determined (e.g. _get_stage_output_channels returned 0 or None)\n",
        "        if out_ch is None or out_ch == 0:\n",
        "            # print(f\"Warning: Could not determine output channels from last stage for FC sync. FC not changed.\")\n",
        "            # Keep current FC in_features if backbone output width is ambiguous.\n",
        "            # This might happen if the last stage is unusual (e.g. only nn.ReLU()).\n",
        "            # A more robust way might be to trace a dummy input.\n",
        "            # For now, assume out_ch is valid.\n",
        "            # If out_ch is 0, Linear(0, num_classes) is invalid.\n",
        "            # So, if out_ch is 0, we must not change FC or handle it.\n",
        "            # Let's assume out_ch will be > 0.\n",
        "            if self.target.fc.in_features > 0 : # If current FC is valid\n",
        "                out_ch = self.target.fc.in_features # Don't change if new out_ch is problematic\n",
        "            else: # Current FC also invalid, this is a deeper issue\n",
        "                # print(\"Error: Cannot sync classifier, out_ch from backbone is 0 and current FC in_features is also 0.\")\n",
        "                return # Cannot proceed\n",
        "\n",
        "\n",
        "        fc = self.target.fc\n",
        "        if fc.in_features == out_ch:\n",
        "            return # Already synced\n",
        "\n",
        "        # Create new fully connected layer\n",
        "        # Bias should be copied if original had bias\n",
        "        new_fc = nn.Linear(out_ch, fc.out_features, bias=(fc.bias is not None)).to(self.dev)\n",
        "\n",
        "        # Copy weights\n",
        "        with torch.no_grad():\n",
        "            # Number of input features to copy\n",
        "            min_in_features = min(fc.in_features, out_ch)\n",
        "\n",
        "            # Copy the common part of the weights\n",
        "            new_fc.weight.data[:, :min_in_features] = fc.weight.data[:, :min_in_features].clone()\n",
        "\n",
        "            # If new FC has more input features (widening), tile from old weights\n",
        "            if out_ch > fc.in_features:\n",
        "                for j in range(fc.in_features, out_ch):\n",
        "                    # Tile by taking modulo: j % fc.in_features\n",
        "                    new_fc.weight.data[:, j] = fc.weight.data[:, j % fc.in_features].clone()\n",
        "\n",
        "            # If new FC has fewer input features (thinning), already handled by slicing with min_in_features.\n",
        "\n",
        "            # Copy bias if it exists\n",
        "            if fc.bias is not None:\n",
        "                new_fc.bias.data.copy_(fc.bias.data)\n",
        "\n",
        "        self.target.fc = new_fc\n",
        "        # print(f\"  Synced FC: in_features {fc.in_features} -> {out_ch}\")\n",
        "\n",
        "\n",
        "    def _sync_seblocks(self):\n",
        "        for s_idx, seq in enumerate(self.target.stages):\n",
        "            # `current_channels_into_layer` tracks the number of channels *input* to the current layer `m`.\n",
        "            # It starts as the output of the previous stage, or the network input channels for the first stage.\n",
        "\n",
        "            # Determine initial channels for the first stage\n",
        "            if s_idx == 0:\n",
        "                # Input to the first layer of the first stage.\n",
        "                # For TargetCNN, this is 3 (image channels) if the first layer is Conv(3,...)\n",
        "                # This needs to be robust if the first layer isn't a Conv.\n",
        "                # Let's find the first Conv in the stage to determine its *configured* in_channels.\n",
        "                # This assumes the first Conv is set up for the image.\n",
        "                first_conv_in_stage0 = next((layer for layer in seq if isinstance(layer, nn.Conv2d)), None)\n",
        "                if first_conv_in_stage0:\n",
        "                    current_channels_into_layer = first_conv_in_stage0.in_channels\n",
        "                else: # No conv in first stage? Unlikely for TargetCNN. Fallback needed.\n",
        "                    current_channels_into_layer = 3 # Default for image input\n",
        "            else:\n",
        "                # Input channels for this stage `s_idx` is the output of stage `s_idx-1`.\n",
        "                current_channels_into_layer = self._get_stage_output_channels(self.target.stages[s_idx-1])\n",
        "\n",
        "            if current_channels_into_layer == 0: # Safety check\n",
        "                # print(f\"Warning: _sync_seblocks detected 0 input channels for stage {s_idx}. Skipping SE sync for this stage.\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            for i, m in enumerate(seq):\n",
        "                # `m` is the current layer. `current_channels_into_layer` is its input channels.\n",
        "\n",
        "                # Sync SEBlock: SEBlock(channels) expects `channels` to be its input channels.\n",
        "                if isinstance(m, SEBlock):\n",
        "                    if m.fc[0].in_features != current_channels_into_layer:\n",
        "                        # print(f\"  Fixing SEBlock stage[{s_idx}][{i}]: {m.fc[0].in_features} -> {current_channels_into_layer} channels\")\n",
        "                        seq[i] = SEBlock(current_channels_into_layer).to(self.dev)\n",
        "                        # SEBlock does not change the number of channels passing through it.\n",
        "                        # So, `current_channels_into_layer` remains the same for the *next* layer.\n",
        "\n",
        "                # Sync Norm layers: Their num_features/num_channels should match `current_channels_into_layer`.\n",
        "                elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                    if self._norm_channels(m) != current_channels_into_layer:\n",
        "                        # print(f\"  Fixing Norm stage[{s_idx}][{i}]: {self._norm_channels(m)} -> {current_channels_into_layer} channels\")\n",
        "                        seq[i] = self._resize_norm(m, current_channels_into_layer)\n",
        "                        # Norm layers also don't change the number of channels.\n",
        "\n",
        "                # Update `current_channels_into_layer` for the *next* layer in the sequence.\n",
        "                # It becomes the output channels of the current layer `m`.\n",
        "                # Use _width helper, which is designed for layers that define/change width.\n",
        "                # If _width(m) is None (e.g. for ReLU, Pool), it means width is unchanged.\n",
        "\n",
        "                # Get output width of current layer m\n",
        "                # If m is Identity, ReLU, MaxPool, Dropout, SEBlock, Norm, its output width is `current_channels_into_layer`.\n",
        "                # If m is Conv, Bottleneck, MBConv, Inception, its output width is given by `_width(m)`.\n",
        "\n",
        "                # `layer_output_width` is the number of channels *after* layer `m` has processed its input.\n",
        "                layer_output_width = None\n",
        "                if isinstance(m, (nn.Conv2d, BottleneckBlock, MBConv, InceptionBlock, ResidualBlock)):\n",
        "                    # These blocks define their output channels explicitly via _width()\n",
        "                    layer_output_width = self._width(m)\n",
        "                # For other layers (SE, Norm, Activations, Pooling, Dropout, Identity),\n",
        "                # they don't change the number of channels.\n",
        "                # So, their output channels = their input channels (`current_channels_into_layer`).\n",
        "\n",
        "                if layer_output_width is not None and layer_output_width > 0:\n",
        "                    current_channels_into_layer = layer_output_width\n",
        "                # If layer_output_width is None or 0, it means `m` didn't change channels\n",
        "                # (e.g. ReLU, Pool, SE, Norm) or _width failed (should not happen for main blocks).\n",
        "                # In that case, `current_channels_into_layer` for the next layer remains unchanged.\n",
        "                # This logic is correct: if a Conv changes width, `current_channels_into_layer` updates.\n",
        "                # If a ReLU follows, its input is this new width, and its output is also this new width.\n",
        "\n",
        "    def _apply_edit(self, etype: int, loc: int, widen_factor: int) -> bool:\n",
        "        stages = self.target.stages\n",
        "        changed = False\n",
        "\n",
        "        if not (0 <= loc < len(stages)):\n",
        "            # print(f\"WARN: loc {loc} out of range for stages (len {len(stages)}). Edit skipped.\")\n",
        "            # If loc is meant to be \"after last stage\" for ADD_STAGE, loc == len(stages) might be valid.\n",
        "            # The original check is strict: 0 <= loc < len(stages).\n",
        "            # For ADD_STAGE, loc is \"after stages[loc]\". So loc can be up to len(stages)-1.\n",
        "            # If loc == len(stages) for ADD_STAGE, it means add after the current last stage.\n",
        "            # The current ADD_STAGE inserts at stages.insert(loc + 1, ...).\n",
        "            # So, if loc = len(stages)-1, it inserts after the last stage, becoming the new last.\n",
        "            # This means loc must be < len(stages).\n",
        "            # If stages is empty, loc=0 is invalid.\n",
        "            if etype == 4 and loc == len(stages) and len(stages) == 0: # Add stage to empty network\n",
        "                 pass # This case needs special handling for ADD_STAGE if loc can be 0 for empty stages\n",
        "            elif not (0 <= loc < len(stages)):\n",
        "                # print(f\"WARN: loc {loc} out of range (have {len(stages)} stages)\")\n",
        "                return False\n",
        "\n",
        "\n",
        "        # seq is stages[loc] only if loc is valid for indexing.\n",
        "        # For ADD_STAGE, if loc is for \"after last stage\", stages[loc] might be the last stage.\n",
        "        # If etype == 4 (ADD_STAGE):\n",
        "        #   If loc refers to the index of the stage *after which* to add the new one.\n",
        "        #   So, if loc = N-1 (last stage index), new stage is added after it.\n",
        "        #   If loc = 0, new stage is added after stage 0.\n",
        "        #   `in_ch` for the new stage comes from `stages[loc]`.\n",
        "        #   So, `stages[loc]` must exist.\n",
        "\n",
        "        # seq is needed for most edits operating *within* stages[loc].\n",
        "        # If stages is empty, and etype is not ADD_STAGE, this will fail.\n",
        "        # TargetCNN initializes with stages, so len(stages) >= 1 usually.\n",
        "        if stages: # If there are stages\n",
        "            # Handle loc for ADD_STAGE if it can be len(stages) meaning \"add at the very end\"\n",
        "            # The current ADD_STAGE logic: stages.insert(loc + 1, new_stage)\n",
        "            # in_ch = self._get_stage_output_channels(seq) where seq = stages[loc]\n",
        "            # This means loc must be a valid index for `stages`.\n",
        "            if not (0 <= loc < len(stages)): # Re-check for ops other than potential ADD_STAGE at end\n",
        "                 if etype == 4 and loc == len(stages) and len(stages) > 0: # Adding after the current last stage\n",
        "                     seq_for_add_in_ch = stages[loc-1] # Get in_ch from the actual last stage\n",
        "                 elif etype == 4 and loc == 0 and len(stages) == 0: # Adding the first stage\n",
        "                     pass # in_ch will be 3 (image input)\n",
        "                 else:\n",
        "                    # print(f\"WARN: loc {loc} invalid for current stages (len {len(stages)}). Edit skipped.\")\n",
        "                    return False\n",
        "        elif etype == 4 and loc == 0: # Adding first stage to empty network\n",
        "            pass\n",
        "        else: # No stages and not adding the first one\n",
        "            # print(\"WARN: No stages to edit, and not ADD_STAGE type or invalid loc. Edit skipped.\")\n",
        "            return False\n",
        "\n",
        "        # seq is stages[loc]\n",
        "        # This is problematic if loc is out of bounds, e.g. if stages is empty.\n",
        "        # Let's assign seq only if loc is valid and stages is not empty.\n",
        "        seq = None\n",
        "        if stages and 0 <= loc < len(stages):\n",
        "            seq = stages[loc]\n",
        "        elif etype != 4 : # If not ADD_STAGE, and seq could not be assigned, error.\n",
        "            # print(f\"WARN: Invalid loc ({loc}) or empty stages for non-ADD_STAGE edit. Skipped.\")\n",
        "            return False\n",
        "        # For ADD_STAGE, seq (stages[loc]) is used to get in_ch for the new stage.\n",
        "        # If adding the very first stage (stages is empty, loc=0), in_ch is special (e.g. 3).\n",
        "\n",
        "\n",
        "        def _fix_tail(stage_idx: int, conv_idx_in_stage: int, current_out_channels: int):\n",
        "            \"\"\"\n",
        "            Rewire layers in stages[stage_idx] from conv_idx_in_stage + 1 onwards.\n",
        "            `current_out_channels` is the output channels of the layer at `conv_idx_in_stage`.\n",
        "            \"\"\"\n",
        "            if not (0 <= stage_idx < len(self.target.stages)): return\n",
        "\n",
        "            target_sequence = self.target.stages[stage_idx]\n",
        "\n",
        "            # `running_channels` is the number of channels expected by the *next* layer.\n",
        "            running_channels = current_out_channels\n",
        "\n",
        "            for j in range(conv_idx_in_stage + 1, len(target_sequence)):\n",
        "                module_to_fix = target_sequence[j]\n",
        "                original_module_class = module_to_fix.__class__.__name__ # For logging\n",
        "                adapted = False\n",
        "\n",
        "                # --- Simple Conv ---\n",
        "                if isinstance(module_to_fix, nn.Conv2d) and module_to_fix.in_channels != running_channels:\n",
        "                    target_sequence[j] = self._rebuild_conv_in(module_to_fix, running_channels)\n",
        "                    adapted = True\n",
        "                # --- Compound Blocks (check their first conv's in_channels or equivalent) ---\n",
        "                elif isinstance(module_to_fix, ResidualBlock) and module_to_fix.conv1.in_channels != running_channels:\n",
        "                    # ResidualBlock constructor takes the number of channels for the block.\n",
        "                    target_sequence[j] = ResidualBlock(running_channels, p_drop=module_to_fix.drop.p).to(self.dev)\n",
        "                    adapted = True\n",
        "                elif isinstance(module_to_fix, BottleneckBlock) and module_to_fix.conv1.in_channels != running_channels:\n",
        "                    # BottleneckBlock(in_ch, out_ch). If it's just adapting, out_ch might be running_channels or a factor.\n",
        "                    # Assuming it should maintain width if just adapting input:\n",
        "                    # Or, it might need its original out_ch if that was different.\n",
        "                    # For simplicity, assume BottleneckBlock(C, C) if C is the new width.\n",
        "                    # The original Bottleneck might have had out_ch = expansion * in_ch.\n",
        "                    # This needs care. If Bottleneck was (C1, C2), now it's (new_C1, ???).\n",
        "                    # Let's assume it tries to maintain its original output channel count if possible,\n",
        "                    # or adapts it based on new_C1.\n",
        "                    # For now, new BottleneckBlock(running_channels, running_channels * module_to_fix.conv3.out_channels // module_to_fix.conv1.in_channels if module_to_fix.conv1.in_channels else running_channels)\n",
        "                    # This is complex. A simpler fix: BottleneckBlock(running_channels, running_channels) to maintain width.\n",
        "                    # Or use its original out_channels: BottleneckBlock(running_channels, module_to_fix.conv3.out_channels)\n",
        "                    # The prompt's _fix_tail used BottleneckBlock(curr, curr). Let's follow that.\n",
        "                    target_sequence[j] = BottleneckBlock(running_channels, running_channels).to(self.dev)\n",
        "                    adapted = True\n",
        "                elif isinstance(module_to_fix, MBConv) and module_to_fix.block[0].in_channels != running_channels:\n",
        "                    # MBConv(in_ch, out_ch). Similar to Bottleneck.\n",
        "                    # Prompt: MBConv(curr, curr).\n",
        "                    target_sequence[j] = MBConv(running_channels, running_channels, stride=module_to_fix.block[3].stride, exp_ratio=module_to_fix.block[0].out_channels // module_to_fix.block[0].in_channels if module_to_fix.block[0].in_channels else 6).to(self.dev)\n",
        "                    adapted = True\n",
        "                elif isinstance(module_to_fix, InceptionBlock) and module_to_fix.in_channels != running_channels:\n",
        "                    target_sequence[j] = InceptionBlock(running_channels).to(self.dev)\n",
        "                    adapted = True\n",
        "                # --- Norms ---\n",
        "                elif isinstance(module_to_fix, (nn.BatchNorm2d, nn.GroupNorm)) and self._norm_channels(module_to_fix) != running_channels:\n",
        "                    target_sequence[j] = self._resize_norm(module_to_fix, running_channels)\n",
        "                    adapted = True\n",
        "                # --- SEBlock (adapts to input channels) ---\n",
        "                elif isinstance(module_to_fix, SEBlock) and module_to_fix.fc[0].in_features != running_channels:\n",
        "                    target_sequence[j] = SEBlock(running_channels).to(self.dev)\n",
        "                    adapted = True\n",
        "\n",
        "                if adapted:\n",
        "                    pass\n",
        "                    # print(f\"  Adapted {original_module_class} at stage[{stage_idx}][{j}] for input {running_channels} ch.\")\n",
        "\n",
        "                # Update `running_channels` to be the output of the (potentially new) layer `target_sequence[j]`.\n",
        "                # If the layer doesn't change channels (e.g. ReLU, Norm, SE), `_width` might return None.\n",
        "                # In such cases, `running_channels` should persist.\n",
        "                width_after_module_j = self._width(target_sequence[j])\n",
        "                if width_after_module_j is not None and width_after_module_j > 0:\n",
        "                    running_channels = width_after_module_j\n",
        "                # If width_after_module_j is None (e.g. for ReLU, pool), running_channels remains unchanged,\n",
        "                # which is correct as these layers don't alter channel count.\n",
        "\n",
        "        # Find first Conv2d in seq (stages[loc]) for edits that need it.\n",
        "        # This requires `seq` to be valid.\n",
        "        idx_conv_in_seq = -1\n",
        "        conv_to_edit = None\n",
        "        if etype in {1, 2, 3, 5, 6, 7, 8, 9, 10, 11}: # Edits operating on a Conv inside stages[loc]\n",
        "            if seq is None: # Should have been caught earlier if seq is needed but None\n",
        "                # print(f\"WARN: Stage {loc} is None or invalid, cannot find Conv2d for edit {etype}. Skipped.\")\n",
        "                return False\n",
        "\n",
        "            found_conv = next(((i, m) for i, m in enumerate(seq) if isinstance(m, nn.Conv2d)), (None, None))\n",
        "            if found_conv[0] is not None:\n",
        "                idx_conv_in_seq, conv_to_edit = found_conv\n",
        "            else:\n",
        "                # print(f\"WARN: Stage {loc} has no Conv2d – edit {etype} skipped.\")\n",
        "                return False\n",
        "\n",
        "        # Perform the edit\n",
        "        if etype == 4: # ADD_STAGE\n",
        "            changed = True\n",
        "            in_ch_for_new_stage = 0\n",
        "            if loc == 0 and not stages: # Adding the very first stage\n",
        "                in_ch_for_new_stage = 3 # Image input channels\n",
        "            elif stages: # Adding after an existing stage stages[loc]\n",
        "                 # `loc` is the index of the stage *after which* to add.\n",
        "                 # So, the input channels for the new stage come from `stages[loc]`.\n",
        "                 # Ensure `loc` is a valid index for `stages`.\n",
        "                if not (0 <= loc < len(stages)):\n",
        "                    # print(f\"WARN: Invalid loc ({loc}) for ADD_STAGE to get input channels. Skipped.\")\n",
        "                    return False\n",
        "                seq_ref_for_add = stages[loc] # The stage after which we are adding\n",
        "                in_ch_for_new_stage = self._get_stage_output_channels(seq_ref_for_add)\n",
        "            else: # Should not happen if caught by initial loc checks\n",
        "                # print(f\"WARN: Cannot determine in_channels for ADD_STAGE at loc {loc}. Skipped.\")\n",
        "                return False\n",
        "\n",
        "            if in_ch_for_new_stage == 0:\n",
        "                # print(f\"WARN: Calculated 0 input channels for new stage at loc {loc}. ADD_STAGE aborted.\")\n",
        "                return False\n",
        "\n",
        "            # Determine norm type for the new stage. Original code used BN or GN based on prev layer.\n",
        "            # Simpler: always use GN for new stages for safety with batch_size=1.\n",
        "            # Or, use BN if batch_size > 1, GN if batch_size == 1.\n",
        "            # Let's use what _swap_bn_to_gn would produce for this channel count if it were a BN.\n",
        "            if in_ch_for_new_stage <= 1: norm_layer_new_stage = nn.Identity()\n",
        "            else: norm_layer_new_stage = nn.GroupNorm(1, in_ch_for_new_stage) # Safe default\n",
        "\n",
        "            new_stage_content = [\n",
        "                nn.Conv2d(in_ch_for_new_stage, in_ch_for_new_stage, 3, padding=1, bias=False),\n",
        "                norm_layer_new_stage,\n",
        "                nn.ReLU(inplace=True), # inplace=True is common\n",
        "                SafeMaxPool2d(2) # Original had MaxPool2d(2)\n",
        "            ]\n",
        "            # If self.batch_size == 1, ensure BNs are GNs.\n",
        "            # The norm_layer_new_stage is already GN or Identity.\n",
        "            # If Conv had bias=True and we used BN, then BN should come after Conv.\n",
        "            # Here, Conv bias=False, Norm follows. This is typical.\n",
        "\n",
        "            new_s = nn.Sequential(*new_stage_content).to(self.dev)\n",
        "\n",
        "            # Insert new stage. loc+1 means if loc=0, inserts at index 1 (after first).\n",
        "            # If stages is empty, loc=0, insert at index 0.\n",
        "            if not stages and loc == 0:\n",
        "                 stages.insert(0, new_s)\n",
        "            else: # stages not empty, or loc > 0\n",
        "                 stages.insert(loc + 1, new_s)\n",
        "            # print(f\"Added stage after index {loc}, new stage in_ch={in_ch_for_new_stage}\")\n",
        "\n",
        "            # After adding a stage, the number of locations for meta-agent's loc head might change.\n",
        "            # This is handled by update_meta_agent_heads in the main loop.\n",
        "\n",
        "        elif etype == 1: # WIDEN\n",
        "            changed = True\n",
        "            old_c = conv_to_edit.out_channels\n",
        "            seq[idx_conv_in_seq] = net2wider_conv(conv_to_edit, widen_factor).to(self.dev)\n",
        "            new_c = seq[idx_conv_in_seq].out_channels\n",
        "            # print(f\"Widen stage[{loc}][{idx_conv_in_seq}] {old_c} -> {new_c}\")\n",
        "            _fix_tail(loc, idx_conv_in_seq, new_c)\n",
        "\n",
        "        elif etype == 2: # DEEPEN\n",
        "            changed = True\n",
        "            # Insert new Conv (identity-like) after conv_to_edit.\n",
        "            # The new Conv takes conv_to_edit.out_channels as its input and output.\n",
        "            deeper_conv = net2deeper_conv(conv_to_edit).to(self.dev) # This creates Conv(oc, oc)\n",
        "            seq.insert(idx_conv_in_seq + 1, deeper_conv)\n",
        "            # print(f\"Deepen stage[{loc}] after conv at index {idx_conv_in_seq}\")\n",
        "            # No need for _fix_tail if deeper_conv maintains channels and is identity-like.\n",
        "            # However, if a Norm layer followed conv_to_edit, it now follows deeper_conv.\n",
        "            # The output channels of deeper_conv are same as conv_to_edit.out_channels.\n",
        "            # So, subsequent layers' inputs are still matched.\n",
        "            # _fix_tail(loc, idx_conv_in_seq + 1, deeper_conv.out_channels) # If deeper_conv could change things\n",
        "\n",
        "        elif etype == 3: # RESIDUAL\n",
        "            changed = True\n",
        "            # Insert ResidualBlock after conv_to_edit. It takes conv_to_edit.out_channels.\n",
        "            res_block = ResidualBlock(conv_to_edit.out_channels).to(self.dev)\n",
        "            seq.insert(idx_conv_in_seq + 1, res_block)\n",
        "            # print(f\"Residual block added in stage[{loc}] after conv at {idx_conv_in_seq}\")\n",
        "            # ResidualBlock maintains channels, so no _fix_tail needed for channel mismatch.\n",
        "            # _fix_tail(loc, idx_conv_in_seq + 1, conv_to_edit.out_channels)\n",
        "\n",
        "        elif etype == 5: # THINNER\n",
        "            changed = True\n",
        "            old_c = conv_to_edit.out_channels\n",
        "            seq[idx_conv_in_seq] = net2thinner_conv(conv_to_edit, 0.5).to(self.dev)\n",
        "            new_c = seq[idx_conv_in_seq].out_channels\n",
        "            # print(f\"Thin stage[{loc}][{idx_conv_in_seq}] {old_c} -> {new_c}\")\n",
        "            _fix_tail(loc, idx_conv_in_seq, new_c)\n",
        "\n",
        "        elif etype == 6: # SHALLOW\n",
        "            # net2shallower_conv modifies seq in-place and returns True if changed.\n",
        "            # It fuses seq[idx_conv_in_seq] and seq[idx_conv_in_seq+1].\n",
        "            # The conv at idx_conv_in_seq remains, the next one is removed.\n",
        "            changed = net2shallower_conv(seq, idx_conv_in_seq)\n",
        "            if changed:\n",
        "                # The conv at seq[idx_conv_in_seq] is now the fused one. Its output channels are its own.\n",
        "                # Need to get its output channels to fix tail.\n",
        "                # If seq[idx_conv_in_seq] was removed (e.g. if it was the one to be dropped), this is an issue.\n",
        "                # net2shallower_conv drops seq[idx+1]. So seq[idx] is still there.\n",
        "                if idx_conv_in_seq < len(seq) and isinstance(seq[idx_conv_in_seq], nn.Conv2d):\n",
        "                    new_c_after_shallow = seq[idx_conv_in_seq].out_channels\n",
        "                    _fix_tail(loc, idx_conv_in_seq, new_c_after_shallow)\n",
        "                # else:\n",
        "                    # print(\"WARN: Shallow edit made, but conv at idx_conv_in_seq not found or not Conv. Tail not fixed.\")\n",
        "\n",
        "        elif etype in {7, 8, 9}: # Bottleneck, SE, MBConv\n",
        "            changed = True\n",
        "            insert_map = {7: BottleneckBlock, 8: SEBlock, 9: MBConv}\n",
        "            blk_cls = insert_map[etype]\n",
        "\n",
        "            # These blocks are inserted after conv_to_edit.\n",
        "            # Their input channels should be conv_to_edit.out_channels.\n",
        "            ch_for_block = conv_to_edit.out_channels\n",
        "            if ch_for_block == 0: # Safety\n",
        "                # print(f\"WARN: Zero channels from preceding conv for {blk_cls.__name__}. Edit skipped.\")\n",
        "                return False\n",
        "\n",
        "            if etype == 7: # BottleneckBlock(in_ch, out_ch)\n",
        "                # Assuming it maintains channels if inserted this way: (ch, ch)\n",
        "                new_block = BottleneckBlock(ch_for_block, ch_for_block).to(self.dev)\n",
        "            elif etype == 8: # SEBlock(channels)\n",
        "                new_block = SEBlock(ch_for_block).to(self.dev)\n",
        "            elif etype == 9: # MBConv(in_ch, out_ch)\n",
        "                # Assuming it maintains channels: (ch, ch)\n",
        "                new_block = MBConv(ch_for_block, ch_for_block).to(self.dev)\n",
        "\n",
        "            seq.insert(idx_conv_in_seq + 1, new_block)\n",
        "            # print(f\"{blk_cls.__name__} inserted in stage[{loc}] after conv at {idx_conv_in_seq}\")\n",
        "\n",
        "            # Output channels of these blocks (if they maintain width as assumed):\n",
        "            # Bottleneck(C,C) -> C. SE(C) -> C. MBConv(C,C) -> C.\n",
        "            # So, subsequent layers' inputs are still matched if these blocks maintain channels.\n",
        "            # _fix_tail(loc, idx_conv_in_seq + 1, ch_for_block)\n",
        "\n",
        "\n",
        "        elif etype == 10: # INCEPTION\n",
        "            changed = True\n",
        "            ch_for_inception = conv_to_edit.out_channels\n",
        "            if ch_for_inception == 0: return False # Safety\n",
        "\n",
        "            inc_block = InceptionBlock(ch_for_inception).to(self.dev)\n",
        "            seq.insert(idx_conv_in_seq + 1, inc_block) # Insert after the conv\n",
        "            new_c_after_inception = inc_block.out_channels\n",
        "            # print(f\"InceptionBlock inserted in stage[{loc}] after conv at {idx_conv_in_seq}. Output channels: {new_c_after_inception}\")\n",
        "            # Inception block changes channel count, so _fix_tail is crucial.\n",
        "            # The tail starts *after* the newly inserted Inception block.\n",
        "            _fix_tail(loc, idx_conv_in_seq + 1, new_c_after_inception)\n",
        "\n",
        "        elif etype == 11: # SUPER-WIDEN (factor 2 in prompt, not 4)\n",
        "            changed = True\n",
        "            old_c = conv_to_edit.out_channels\n",
        "            # Prompt uses factor=(2) for net2wider_conv.\n",
        "            # Original comment said \"quadruple channels\", but code uses factor=2.\n",
        "            # Let's use factor from prompt's code.\n",
        "            seq[idx_conv_in_seq] = net2wider_conv(conv_to_edit, factor=2.0).to(self.dev)\n",
        "            new_c = seq[idx_conv_in_seq].out_channels\n",
        "            # print(f\"Super-Widen (x2) stage[{loc}][{idx_conv_in_seq}] {old_c} -> {new_c}\")\n",
        "            _fix_tail(loc, idx_conv_in_seq, new_c)\n",
        "\n",
        "\n",
        "        if changed:\n",
        "            # These sync functions should be robust enough to call after any change.\n",
        "            self._sync_seblocks() # Ensure SEBlocks match their input channels within each stage\n",
        "            self._sync_backbone_channels() # Ensure inter-stage channel counts match\n",
        "            self._sync_classifier() # Ensure FC layer matches output of last backbone stage\n",
        "\n",
        "            # If batch_size is 1, BNs might have been swapped to GNs.\n",
        "            # This should be re-applied if the network structure changed.\n",
        "            # _swap_bn_to_gn is called in __init__ if BS=1.\n",
        "            # If new layers (BNs) were added, they also need swapping if BS=1.\n",
        "            if self.batch_size == 1:\n",
        "                self._swap_bn_to_gn(self.target)\n",
        "\n",
        "        return changed\n",
        "\n",
        "\n",
        "    def _train_post(self, max_epochs: int, patience: int = 5,\n",
        "                    min_delta: float = 0.0005):\n",
        "        best_acc = -1.0 # Initialize with a value lower than any possible accuracy\n",
        "        no_imp   = 0\n",
        "        last_loss = float('inf') # Initialize with a high value for loss\n",
        "        last_acc = 0.0       # Initialize with a low value for accuracy\n",
        "\n",
        "        # Check if validation loader is empty, if so, cannot validate.\n",
        "        if not self.vall:\n",
        "            # print(\"Warning: Validation loader is empty. Post-edit training will run for max_epochs without early stopping.\")\n",
        "            for ep in range(1, max_epochs + 1):\n",
        "                self._train_one_epoch(self.trl, 1) # Train one epoch\n",
        "                # Cannot get validation loss/acc, so just log training progress if available\n",
        "                # print(f\"Post-edit epoch {ep}/{max_epochs} (no validation)\")\n",
        "            # Return some placeholder or last known training metrics if available\n",
        "            return 0.0, 0.0 # Placeholder for no validation case\n",
        "\n",
        "\n",
        "        # Initial validation before starting post-train loop to get a baseline\n",
        "        # This might be useful if max_epochs is very small (e.g., 0 or 1)\n",
        "        # However, typical use is max_epochs >= 1.\n",
        "        # Let's get initial val acc for best_acc.\n",
        "        # val_loss_before, val_acc_before = self._validate()\n",
        "        # best_acc = val_acc_before\n",
        "        # print(f\"Post-edit initial val: loss={val_loss_before:.4f} acc={val_acc_before:.4f}\")\n",
        "\n",
        "\n",
        "        for ep in range(1, max_epochs + 1):\n",
        "            self._train_one_epoch(self.trl, 1)          # one epoch\n",
        "            current_vloss, current_vacc = self._validate()\n",
        "            # print(f\"Post-edit epoch {ep}/{max_epochs} — val loss={current_vloss:.4f}  acc={current_vacc:.4f}\")\n",
        "\n",
        "            # Update last known metrics\n",
        "            last_loss, last_acc = current_vloss, current_vacc\n",
        "\n",
        "            if current_vacc > best_acc + min_delta:\n",
        "                best_acc = current_vacc\n",
        "                no_imp   = 0\n",
        "                # print(f\"  New best val acc: {best_acc:.4f}\")\n",
        "            else:\n",
        "                no_imp  += 1\n",
        "                if no_imp >= patience:\n",
        "                    # print(f\"  Early stopping: Val acc ({current_vacc:.4f}) hasn't improved by >{min_delta} over best ({best_acc:.4f}) for {patience} epochs.\")\n",
        "                    break\n",
        "\n",
        "        # Return the metrics from the *last completed validation*, not necessarily the best.\n",
        "        # This matches typical behavior where the model state at the end of training is used.\n",
        "        return last_loss, last_acc\n",
        "\n",
        "    def _train_one(self): # Wrapper for _train_one_epoch with self.trl and 1 epoch\n",
        "        self._train_one_epoch(self.trl, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _get_layer_recipe(self, layer: nn.Module) -> dict:\n",
        "        \"\"\"Helper to create a recipe for a single layer.\"\"\"\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            return {\n",
        "                'type': 'Conv2d',\n",
        "                'params': {\n",
        "                    'in_channels': layer.in_channels, 'out_channels': layer.out_channels,\n",
        "                    'kernel_size': layer.kernel_size, 'stride': layer.stride,\n",
        "                    'padding': layer.padding, 'dilation': layer.dilation,\n",
        "                    'groups': layer.groups, 'bias': layer.bias is not None\n",
        "                }\n",
        "            }\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            return {\n",
        "                'type': 'BatchNorm2d',\n",
        "                'params': {'num_features': layer.num_features, 'eps': layer.eps,\n",
        "                           'momentum': layer.momentum, 'affine': layer.affine,\n",
        "                           'track_running_stats': layer.track_running_stats}\n",
        "            }\n",
        "        elif isinstance(layer, nn.GroupNorm):\n",
        "            return {\n",
        "                'type': 'GroupNorm',\n",
        "                'params': {'num_groups': layer.num_groups, 'num_channels': layer.num_channels,\n",
        "                           'eps': layer.eps, 'affine': layer.affine}\n",
        "            }\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            return {'type': 'ReLU', 'params': {'inplace': layer.inplace}}\n",
        "        elif isinstance(layer, SafeMaxPool2d): # Your custom SafeMaxPool2d\n",
        "            return {'type': 'SafeMaxPool2d', 'params': {'kernel_size': layer.kernel_size, 'stride': layer.stride}}\n",
        "        elif isinstance(layer, nn.AdaptiveAvgPool2d):\n",
        "            return {'type': 'AdaptiveAvgPool2d', 'params': {'output_size': layer.output_size}}\n",
        "        elif isinstance(layer, nn.Dropout):\n",
        "            return {'type': 'Dropout', 'params': {'p': layer.p, 'inplace': layer.inplace}}\n",
        "\n",
        "        # Custom Blocks (ensure all necessary construction params are saved)\n",
        "        elif isinstance(layer, ResidualBlock):\n",
        "            return {\n",
        "                'type': 'ResidualBlock',\n",
        "                'params': {'c': layer.conv1.in_channels, # Assuming c is in_channels\n",
        "                           'p_drop': layer.drop.p}\n",
        "            }\n",
        "        elif isinstance(layer, BottleneckBlock):\n",
        "            # Assuming BottleneckBlock(in_ch, out_ch, stride, expansion)\n",
        "            # We need to deduce these from the layer's components if not stored directly\n",
        "            return {\n",
        "                'type': 'BottleneckBlock',\n",
        "                'params': {\n",
        "                    'in_ch': layer.conv1.in_channels,\n",
        "                    'out_ch': layer.conv3.out_channels, # This is the block's out_ch\n",
        "                    'stride': layer.conv2.stride[0] if isinstance(layer.conv2.stride, tuple) else layer.conv2.stride, # conv2 determines stride\n",
        "                     # Expansion needs to be inferred or fixed. If it's fixed (e.g. 4), great.\n",
        "                     # If not, it's mid_ch = out_ch // expansion. So expansion = out_ch / mid_ch\n",
        "                     # mid_ch = layer.conv1.out_channels\n",
        "                    'expansion': layer.conv3.out_channels // layer.conv1.out_channels if layer.conv1.out_channels > 0 else 4 # Default to 4 if problematic\n",
        "                }\n",
        "            }\n",
        "        elif isinstance(layer, SEBlock):\n",
        "            return {\n",
        "                'type': 'SEBlock',\n",
        "                'params': {'channels': layer.fc[0].in_features,\n",
        "                           'reduction': layer.fc[0].in_features // layer.fc[0].out_features if layer.fc[0].out_features > 0 else 16} # Infer reduction\n",
        "            }\n",
        "        elif isinstance(layer, MBConv):\n",
        "            # MBConv(in_ch, out_ch, stride, exp_ratio)\n",
        "            return {\n",
        "                'type': 'MBConv',\n",
        "                'params': {\n",
        "                    'in_ch': layer.block[0].in_channels, # First conv in block\n",
        "                    'out_ch': layer.out_channels,       # Stored attribute\n",
        "                    'stride': layer.block[3].stride[0] if isinstance(layer.block[3].stride, tuple) else layer.block[3].stride, # Depthwise conv\n",
        "                    'exp_ratio': layer.block[0].out_channels // layer.block[0].in_channels if layer.block[0].in_channels > 0 else 6 # From expand conv\n",
        "                }\n",
        "            }\n",
        "        elif isinstance(layer, InceptionBlock):\n",
        "            # InceptionBlock(c: int) where c is in_channels\n",
        "            return {'type': 'InceptionBlock', 'params': {'c': layer.in_channels}}\n",
        "        elif isinstance(layer, nn.Identity):\n",
        "            return {'type': 'Identity', 'params': {}}\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported layer type for recipe generation: {type(layer)}\")\n",
        "\n",
        "    def _build_target_from_recipe(self, recipe: dict, base_width_for_fc: int, num_classes: int):\n",
        "        \"\"\"Builds TargetCNN from a recipe.\"\"\"\n",
        "        print(\"Building TargetCNN from recipe...\")\n",
        "\n",
        "        # The recipe directly describes the stages\n",
        "        # The TargetCNN __init__ creates some initial stages. We will replace them.\n",
        "        # For FC layer, we need the output channels of the last stage from the recipe.\n",
        "\n",
        "        target_model = TargetCNN(base=base_width_for_fc, num_classes=num_classes) # Create a shell\n",
        "\n",
        "        reconstructed_stages = nn.ModuleList()\n",
        "        last_stage_out_channels = base_width_for_fc * 4 # Default if no stages\n",
        "\n",
        "        for stage_recipe_list in recipe['stages']:\n",
        "            stage_layers = []\n",
        "            current_stage_out_channels = 0 # Track for this stage\n",
        "            for layer_info in stage_recipe_list:\n",
        "                layer_type = layer_info['type']\n",
        "                params = layer_info['params']\n",
        "\n",
        "                if layer_type == 'Conv2d':\n",
        "                    module = nn.Conv2d(**params)\n",
        "                    current_stage_out_channels = params['out_channels']\n",
        "                elif layer_type == 'BatchNorm2d':\n",
        "                    module = nn.BatchNorm2d(**params)\n",
        "                    # BN doesn't change channel count, uses num_features which should match prev conv out\n",
        "                elif layer_type == 'GroupNorm':\n",
        "                    module = nn.GroupNorm(**params)\n",
        "                elif layer_type == 'ReLU':\n",
        "                    module = nn.ReLU(**params)\n",
        "                elif layer_type == 'SafeMaxPool2d':\n",
        "                    module = SafeMaxPool2d(**params)\n",
        "                elif layer_type == 'AdaptiveAvgPool2d':\n",
        "                    module = nn.AdaptiveAvgPool2d(**params)\n",
        "                elif layer_type == 'Dropout': # Added Dropout\n",
        "                    module = nn.Dropout(**params)\n",
        "\n",
        "                # Custom Blocks\n",
        "                elif layer_type == 'ResidualBlock':\n",
        "                    module = ResidualBlock(**params)\n",
        "                    current_stage_out_channels = params['c'] # Residual block maintains channels\n",
        "                elif layer_type == 'BottleneckBlock':\n",
        "                    module = BottleneckBlock(**params)\n",
        "                    current_stage_out_channels = params['out_ch']\n",
        "                elif layer_type == 'SEBlock':\n",
        "                    module = SEBlock(**params)\n",
        "                    # SEBlock maintains channels, which is params['channels']\n",
        "                elif layer_type == 'MBConv':\n",
        "                    module = MBConv(**params)\n",
        "                    current_stage_out_channels = params['out_ch']\n",
        "                elif layer_type == 'InceptionBlock':\n",
        "                    module = InceptionBlock(**params)\n",
        "                    # InceptionBlock calculates its out_channels internally based on 'c'\n",
        "                    # We need to instantiate it to get its out_channels for tracking\n",
        "                    temp_inception = InceptionBlock(params['c'])\n",
        "                    current_stage_out_channels = temp_inception.out_channels\n",
        "                    module = temp_inception # Use the one we made\n",
        "                elif layer_type == 'Identity':\n",
        "                    module = nn.Identity(**params)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported layer type in recipe: {layer_type}\")\n",
        "                stage_layers.append(module)\n",
        "\n",
        "            reconstructed_stages.append(nn.Sequential(*stage_layers))\n",
        "            if current_stage_out_channels > 0 : # Update last_stage_out_channels if stage defined width\n",
        "                last_stage_out_channels = current_stage_out_channels\n",
        "\n",
        "        target_model.stages = reconstructed_stages\n",
        "\n",
        "        # Reconstruct the FC layer based on the output of the last stage in the recipe\n",
        "        # and the pre_fc_drop layer\n",
        "        target_model.pre_fc_drop = nn.Dropout(p=recipe['pre_fc_dropout_p'])\n",
        "        if last_stage_out_channels > 0:\n",
        "             target_model.fc = nn.Linear(last_stage_out_channels, num_classes)\n",
        "        else: # Fallback if something went wrong with channel tracking\n",
        "            print(f\"Warning: Could not determine last stage out channels from recipe ({last_stage_out_channels}). Using default FC.\")\n",
        "            target_model.fc = nn.Linear(base_width_for_fc * 4, num_classes)\n",
        "\n",
        "\n",
        "        # The 'init_widths' attribute of TargetCNN might also need to be set if it's used elsewhere.\n",
        "        # For now, we focus on reconstructing the operational parts (stages, fc).\n",
        "        # If self.target.init_widths is used critically after this, it should be reconstructed too.\n",
        "        # For example, by storing it in the recipe or recalculating:\n",
        "        # target_model.init_widths = [s[0].out_channels for s in target_model.stages if s and isinstance(s[0], nn.Conv2d)]\n",
        "        # This is a simplification. The original init_widths was fixed based on 'base'.\n",
        "        # If the search significantly changed initial stage widths, this might need care.\n",
        "        # For now, assume the original init_widths logic based on 'base' is sufficient context,\n",
        "        # or that `target_model.widths()` provides the dynamic view needed.\n",
        "\n",
        "        return target_model.to(self.dev)\n",
        "\n",
        "\n",
        "    def train(self, iters: int = 100, model_save_path: str = '/content/drive/My Drive/deiti_final_architecture.pth'):\n",
        "        # model_save_path will now be for the .json recipe\n",
        "        recipe_save_path = model_save_path.replace(\".pth\", \"_recipe.json\")\n",
        "\n",
        "        meta_opt = optim.Adam(self.meta.parameters(), lr=LEARNING_RATE)\n",
        "        history  = []\n",
        "        self.tgt_opt, self.tgt_sch = self._new_opt_sched()\n",
        "\n",
        "        for itr in range(1, iters + 1):\n",
        "            t0 = time.time()\n",
        "            print(f\"\\n=== ITERATION {itr:03d}/{iters} ===\")\n",
        "\n",
        "            steps_per_ep = len(self.trl) if self.batches_per_epoch is None else self.batches_per_epoch\n",
        "            if steps_per_ep == 0:\n",
        "                print(\"Warning: Training loader is empty. Cannot proceed.\")\n",
        "                break\n",
        "            pre_steps_for_sched_ff = self.pre_epochs * steps_per_ep\n",
        "\n",
        "            self.tgt_opt, self.tgt_sch = self._new_opt_sched() # Fresh opt/sched for pre-edit\n",
        "\n",
        "            print(\"--- Pre-edit Training ---\")\n",
        "            self._train_one_epoch(self.trl, self.pre_epochs)\n",
        "            vloss, vacc = self._validate()\n",
        "            print(f\"→ Pre-val  loss={vloss:.4f}  acc={vacc:.4f}\")\n",
        "\n",
        "            init_widths_tensor = torch.tensor(self.target.init_widths, dtype=torch.float32, device=self.dev)\n",
        "            current_widths_tensor = torch.tensor(self.target.widths(), dtype=torch.float32, device=self.dev)\n",
        "            scale_factor = init_widths_tensor.mean() if init_widths_tensor.numel() > 0 else torch.tensor(1.0, device=self.dev)\n",
        "            if scale_factor == 0: scale_factor = torch.tensor(1.0, device=self.dev)\n",
        "            mean_norm_width = (current_widths_tensor / scale_factor).mean().item() if current_widths_tensor.numel() > 0 else 0.0\n",
        "            var_norm_width = (current_widths_tensor / scale_factor).var().item() if current_widths_tensor.numel() > 1 else 0.0\n",
        "\n",
        "            state_features = torch.tensor([[\n",
        "                vloss, vacc, mean_norm_width, var_norm_width,\n",
        "                len(self.target.stages) / 10.0, itr / float(iters)\n",
        "            ]], dtype=torch.float32, device=self.dev).view(1, 1, STATE_DIM)\n",
        "\n",
        "            history.append(state_features)\n",
        "            if len(history) > MAX_HISTORY_LEN: history.pop(0)\n",
        "            seq_for_meta = torch.cat(history, dim=1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                le, ll, lm, _ = self.meta(seq_for_meta)\n",
        "\n",
        "            try: ae = Categorical(logits=le).sample()\n",
        "            except RuntimeError: ae = torch.tensor([0], device=self.dev)\n",
        "            try: al = Categorical(logits=ll).sample()\n",
        "            except RuntimeError: al = torch.tensor([0], device=self.dev)\n",
        "\n",
        "            am = torch.tensor([0], dtype=torch.long, device=self.dev)\n",
        "            current_meta_dist_for_entropy = None\n",
        "            if itr > 0 and self.self_edit_int > 0 and itr % self.self_edit_int == 0:\n",
        "                try:\n",
        "                    current_meta_dist_for_entropy = Categorical(logits=lm)\n",
        "                    am = current_meta_dist_for_entropy.sample()\n",
        "                except RuntimeError: am = torch.tensor([0], device=self.dev)\n",
        "\n",
        "            print(f\"Actions: edit={ae.item()}, loc={al.item()}, meta={am.item()}\")\n",
        "\n",
        "            if itr > 0 and self.self_edit_int > 0 and itr % self.self_edit_int == 0 and am.item() != 0:\n",
        "                print(f\"Performing meta-edit on meta-agent: type {am.item()}\")\n",
        "                if am.item() == 1: self.meta.lstm = net2wider_lstm(self.meta.lstm, 2).to(self.dev)\n",
        "                elif am.item() == 2: self.meta.lstm = net2deeper_lstm(self.meta.lstm).to(self.dev)\n",
        "                update_meta_agent_heads(\n",
        "                    self.meta, self.meta.lstm.hidden_size,\n",
        "                    self.meta.head_e.out_features, max(1, len(self.target.stages)), self.meta.head_m.out_features\n",
        "                )\n",
        "                meta_opt = optim.Adam(self.meta.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "            backup_target_for_rollback_gpu = copy.deepcopy(self.target) # Keep on GPU\n",
        "\n",
        "            edit_applied_successfully = self._apply_edit(ae.item(), al.item(), widen_factor=1.5)\n",
        "\n",
        "            print(f\"Parameters after edit attempt: {sum(p.numel() for p in self.target.parameters()):,}\")\n",
        "            if not edit_applied_successfully:\n",
        "                print(\"  (Edit resulted in no change to the model structure or was skipped)\")\n",
        "\n",
        "            self.tgt_opt, self.tgt_sch = self._new_opt_sched() # Opt/sched for (potentially) new arch\n",
        "            for _ in range(pre_steps_for_sched_ff): self.tgt_sch.step()\n",
        "\n",
        "            print(f\"--- Post-edit Training (Max {self.post_epochs} epochs) ---\")\n",
        "            vloss2, vacc2 = self._train_post(max_epochs=self.post_epochs, patience=5, min_delta=5e-4)\n",
        "            print(f\"→ Post-val loss={vloss2:.4f}  acc={vacc2:.4f}\")\n",
        "\n",
        "            if edit_applied_successfully and (vacc2 < vacc - 0.05):\n",
        "                print(f\"↻ Rollback: Δacc = {vacc2 - vacc:+.3f}. Restoring model.\")\n",
        "                self.target = backup_target_for_rollback_gpu # Already on self.dev\n",
        "                self.tgt_opt, self.tgt_sch = self._new_opt_sched()\n",
        "                for _ in range(pre_steps_for_sched_ff): self.tgt_sch.step()\n",
        "                vloss2, vacc2 = vloss, vacc\n",
        "            elif not edit_applied_successfully:\n",
        "                 print(\"  (No actual edit applied/skipped, metrics remain pre-edit)\")\n",
        "                 vloss2, vacc2 = vloss, vacc\n",
        "\n",
        "            param_count_final_iter = sum(p.numel() for p in self.target.parameters())\n",
        "            print(f\"Parameters at end of iteration: {param_count_final_iter:,}\")\n",
        "            reward_value = 4.0 * (vacc2 - vacc)\n",
        "            print(f\"Reward = {reward_value:+.4f}\")\n",
        "\n",
        "            meta_opt.zero_grad(set_to_none=True)\n",
        "            with autocast(enabled=self.amp_enabled):\n",
        "                le2, ll2, lm2, lv2_tensor = self.meta(seq_for_meta)\n",
        "                log_prob_e = Categorical(logits=le2).log_prob(ae)\n",
        "                log_prob_l = Categorical(logits=ll2).log_prob(al)\n",
        "                log_prob_m = torch.tensor(0.0, device=self.dev)\n",
        "                entropy_m = torch.tensor(0.0, device=self.dev)\n",
        "                if itr > 0 and self.self_edit_int > 0 and itr % self.self_edit_int == 0:\n",
        "                    # Recreate dist from current policy for chosen action 'am'\n",
        "                    meta_action_dist_current = Categorical(logits=lm2)\n",
        "                    log_prob_m = meta_action_dist_current.log_prob(am)\n",
        "                    entropy_m  = meta_action_dist_current.entropy().mean()\n",
        "\n",
        "                total_log_prob = log_prob_e + log_prob_l + log_prob_m\n",
        "                lv2_squeezed = lv2_tensor.squeeze()\n",
        "                advantage = reward_value - lv2_squeezed.detach()\n",
        "                loss_actor = -total_log_prob * advantage\n",
        "                loss_critic = (reward_value - lv2_squeezed).pow(2)\n",
        "                entropy_bonus = (Categorical(logits=le2).entropy().mean() +\n",
        "                                 Categorical(logits=ll2).entropy().mean() + entropy_m)\n",
        "                total_meta_loss = loss_actor + 0.5 * loss_critic - self.ent_coef * entropy_bonus\n",
        "\n",
        "            self.scaler_meta.scale(total_meta_loss).backward()\n",
        "            self.scaler_meta.unscale_(meta_opt)\n",
        "            clip_grad_norm_(self.meta.parameters(), MAX_GRAD_NORM)\n",
        "            self.scaler_meta.step(meta_opt)\n",
        "            self.scaler_meta.update()\n",
        "\n",
        "            del backup_target_for_rollback_gpu\n",
        "            print(f\"Iter {itr:03d} done in {time.time()-t0:.1f}s. Meta Loss: {total_meta_loss.item():.4f}\")\n",
        "\n",
        "        # END OF SEARCH ITERATIONS\n",
        "        print(\"\\n=== SEARCH COMPLETE ===\")\n",
        "\n",
        "        # Generate recipe for the final architecture\n",
        "        final_recipe = {'stages': [], 'pre_fc_dropout_p': self.target.pre_fc_drop.p}\n",
        "        for stage_seq in self.target.stages:\n",
        "            stage_recipe_list = []\n",
        "            for layer in stage_seq:\n",
        "                try:\n",
        "                    stage_recipe_list.append(self._get_layer_recipe(layer))\n",
        "                except ValueError as e:\n",
        "                    print(f\"Error generating recipe for layer {type(layer)}: {e}. Skipping layer.\")\n",
        "                    # Decide how to handle: skip layer, use placeholder, or halt.\n",
        "                    # For now, it will skip, which might lead to an incomplete recipe.\n",
        "            final_recipe['stages'].append(stage_recipe_list)\n",
        "\n",
        "        # Add other global params if needed, e.g., initial base width for TargetCNN, num_classes\n",
        "        # These are passed to _build_target_from_recipe during loading anyway.\n",
        "\n",
        "        print(f\"Saving final architecture recipe to: {recipe_save_path}\")\n",
        "        try:\n",
        "            with open(recipe_save_path, 'w') as f:\n",
        "                json.dump(final_recipe, f, indent=2) # indent for readability\n",
        "            print(f\"Final architecture recipe saved successfully to {recipe_save_path}\")\n",
        "            # model_summary is on the self.target object, which is fine.\n",
        "            model_summary(self.target, \"Final Architecture (In Memory)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model recipe to {recipe_save_path}: {e}\")\n",
        "\n",
        "    def _validate(self):\n",
        "        \"\"\"Validate without using AMP (fp32-only forward passes).\"\"\"\n",
        "        self.target.eval()\n",
        "        tot_loss = tot_acc = cnt = 0.0\n",
        "\n",
        "        if not self.vall or len(self.vall) == 0:\n",
        "            print(\"Warning (_validate): Validation loader is empty or has zero length. Skipping validation.\")\n",
        "            return 0.0, 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val_hard in self.vall:\n",
        "                x_val, y_val_hard = x_val.to(self.dev), y_val_hard.to(self.dev)\n",
        "\n",
        "                if not torch.isfinite(x_val).all():\n",
        "                    print(f\"Warning (_validate): Non-finite input x_val encountered. Skipping batch.\")\n",
        "                    continue\n",
        "\n",
        "                # Pure fp32 forward (no autocast)\n",
        "                logits = self.target(x_val)\n",
        "\n",
        "                if not torch.isfinite(logits).all():\n",
        "                    print(f\"Warning (_validate): Non-finite logits encountered. Logits sum: {logits.sum().item()}. Skipping batch.\")\n",
        "                    continue\n",
        "\n",
        "                num_classes = self.target.fc.out_features\n",
        "                if num_classes <= 0:\n",
        "                    print(f\"Warning (_validate): Invalid num_classes ({num_classes}) from model. Skipping batch.\")\n",
        "                    continue\n",
        "\n",
        "                # Convert hard labels to one-hot for SmoothCE\n",
        "                y_soft = self._to_onehot(y_val_hard, num_classes)\n",
        "                loss   = self.crit(logits, y_soft)\n",
        "\n",
        "                if not torch.isfinite(loss):\n",
        "                    val_loss_item = loss.item()\n",
        "                    print(f\"Warning (_validate): Non-finite validation loss encountered (value: {val_loss_item}). Skipping batch.\")\n",
        "                    continue\n",
        "\n",
        "                tot_loss += loss.item()\n",
        "                tot_acc  += (logits.argmax(dim=1) == y_val_hard).float().mean().item()\n",
        "                cnt     += 1\n",
        "\n",
        "                if self.batches_per_epoch is not None and cnt >= self.batches_per_epoch:\n",
        "                    break\n",
        "\n",
        "        if cnt == 0:\n",
        "            print(\"Warning (_validate): No batches successfully processed in validation. Returning 0 loss/acc.\")\n",
        "            return 0.0, 0.0\n",
        "\n",
        "        return tot_loss / cnt, tot_acc / cnt\n",
        "\n",
        "\n",
        "\n",
        "    def _train_full(self, epochs: int = 300, learning_rate: float = LEARNING_RATE,\n",
        "                    grad_clip_value: float = None,\n",
        "                    optimizer_eps: float = 1e-7):\n",
        "        if not list(self.target.parameters()):\n",
        "            print(\"Error (_train_full): Target model has no parameters. Cannot start full training.\")\n",
        "            return\n",
        "\n",
        "        if not self.trl or len(self.trl) == 0:\n",
        "            print(\"Error (_train_full): Training loader is empty or has zero length. Cannot perform full training.\")\n",
        "            return\n",
        "\n",
        "        print(f\"--- Starting _train_full with LR: {learning_rate:.1e}, AdamW_eps: {optimizer_eps:.1e}, \"\n",
        "              f\"GradClipNorm: {MAX_GRAD_NORM}, GradClipVal: {grad_clip_value} for {epochs} epochs ---\")\n",
        "\n",
        "        # Create a fresh optimizer and OneCycleLR scheduler for full training\n",
        "        full_train_opt = optim.AdamW(\n",
        "            self.target.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-4,\n",
        "            eps=optimizer_eps\n",
        "        )\n",
        "        full_train_sch = OneCycleLR(\n",
        "            full_train_opt,\n",
        "            max_lr=learning_rate * 5,\n",
        "            steps_per_epoch=len(self.trl),\n",
        "            epochs=epochs,\n",
        "            pct_start=0.25,\n",
        "            div_factor=25,\n",
        "            final_div_factor=1e4\n",
        "        )\n",
        "\n",
        "        # Save any existing optimizer/scheduler references so we can restore later\n",
        "        original_tgt_opt, original_tgt_sch = getattr(self, 'tgt_opt', None), getattr(self, 'tgt_sch', None)\n",
        "\n",
        "        opt_for_full_train = full_train_opt\n",
        "        sch_for_full_train = full_train_sch\n",
        "\n",
        "        best_val_acc_full_train = -1.0\n",
        "        patience_full_train = 20\n",
        "        min_delta_full_train = 0.001\n",
        "        no_improvement_epochs_full_train = 0\n",
        "        early_stopping_grace_epochs = max(10, epochs // 10)\n",
        "\n",
        "        for ep in range(1, epochs + 1):\n",
        "            tot_loss_epoch = 0.0\n",
        "            tot_acc_epoch = 0.0\n",
        "            cnt_batch = 0\n",
        "            skipped_batches_due_to_nan_grad = 0\n",
        "\n",
        "            self.target.train()\n",
        "\n",
        "            for batch_idx, (x, y_soft) in enumerate(self.trl):\n",
        "                x, y_soft = x.to(self.dev), y_soft.to(self.dev)\n",
        "\n",
        "                # Check for non-finite inputs\n",
        "                if not torch.isfinite(x).all():\n",
        "                    print(f\"Warning [FULL TRAIN]: Non-finite input x at ep {ep}, batch {batch_idx}. Skipping batch.\")\n",
        "                    continue\n",
        "\n",
        "                # Zero gradients at the start of each batch\n",
        "                opt_for_full_train.zero_grad(set_to_none=True)\n",
        "\n",
        "                # ---- Forward pass (pure FP32) ----\n",
        "                logits = self.target(x)\n",
        "\n",
        "                # Check for non-finite logits\n",
        "                if not torch.isfinite(logits).all():\n",
        "                    print(f\"Warning [FULL TRAIN]: Non-finite logits at ep {ep}, batch {batch_idx}. \"\n",
        "                          f\"Logits sum: {logits.sum().item()}. Skipping batch.\")\n",
        "                    continue\n",
        "\n",
        "                loss = self.crit(logits, y_soft)\n",
        "\n",
        "                # Check for non-finite loss\n",
        "                if not torch.isfinite(loss):\n",
        "                    val_loss_item = loss.item() if hasattr(loss, 'item') else float('nan')\n",
        "                    print(f\"Warning [FULL TRAIN]: non-finite loss (value: {val_loss_item}) at ep {ep}, batch {batch_idx}. \"\n",
        "                          \"Skipping batch.\")\n",
        "                    opt_for_full_train.zero_grad(set_to_none=True)\n",
        "                    continue\n",
        "\n",
        "                # ---- Backward pass ----\n",
        "                loss.backward()\n",
        "\n",
        "                # Optionally clip by norm first\n",
        "                clip_grad_norm_(self.target.parameters(), MAX_GRAD_NORM)\n",
        "\n",
        "                # Optionally clip by value\n",
        "                if grad_clip_value is not None:\n",
        "                    torch.nn.utils.clip_grad_value_(self.target.parameters(), grad_clip_value)\n",
        "\n",
        "                # Check for non-finite gradients after clipping\n",
        "                found_non_finite_grad = False\n",
        "                for param in self.target.parameters():\n",
        "                    if param.grad is not None and not torch.isfinite(param.grad).all():\n",
        "                        print(f\"Warning [FULL TRAIN]: Detected non-finite grad for a param at ep {ep}, \"\n",
        "                              f\"batch {batch_idx} *after* clip, before optimizer.step().\")\n",
        "                        found_non_finite_grad = True\n",
        "                        break\n",
        "\n",
        "                if found_non_finite_grad:\n",
        "                    opt_for_full_train.zero_grad(set_to_none=True)\n",
        "                    skipped_batches_due_to_nan_grad += 1\n",
        "                    continue\n",
        "\n",
        "                # ---- Optimizer step ----\n",
        "                opt_for_full_train.step()\n",
        "                sch_for_full_train.step()\n",
        "\n",
        "                # ---- Compute training metrics ----\n",
        "                with torch.no_grad():\n",
        "                    preds = logits.argmax(dim=1)\n",
        "                    true_labels_from_soft = y_soft.argmax(dim=1)\n",
        "                    acc_batch = (preds == true_labels_from_soft).float().mean().item()\n",
        "\n",
        "                tot_loss_epoch += loss.item()\n",
        "                tot_acc_epoch += acc_batch\n",
        "                cnt_batch += 1\n",
        "\n",
        "                if self.batches_per_epoch is not None and cnt_batch >= self.batches_per_epoch:\n",
        "                    break\n",
        "\n",
        "            # ---- End of epoch bookkeeping ----\n",
        "            current_lr_epoch = opt_for_full_train.param_groups[0]['lr']\n",
        "\n",
        "            if skipped_batches_due_to_nan_grad > 0:\n",
        "                print(f\"Info [FULL TRAIN]: Skipped {skipped_batches_due_to_nan_grad}/\"\n",
        "                      f\"{len(self.trl) if self.trl else 0} batches in epoch {ep} due to non-finite gradients.\")\n",
        "\n",
        "            if cnt_batch == 0:\n",
        "                print(f\"Warning [FULL TRAIN]: No batches successfully processed in epoch {ep}. \"\n",
        "                      \"Skipping epoch summary and validation.\")\n",
        "                avg_loss_epoch = 0.0\n",
        "                avg_acc_epoch = 0.0\n",
        "                vloss, vacc = 0.0, 0.0\n",
        "            else:\n",
        "                avg_loss_epoch = tot_loss_epoch / cnt_batch\n",
        "                avg_acc_epoch = tot_acc_epoch / cnt_batch\n",
        "                if self.vall and len(self.vall) > 0:\n",
        "                    vloss, vacc = self._validate()\n",
        "                else:\n",
        "                    print(f\"Info [FULL TRAIN]: No validation loader found or it's empty. \"\n",
        "                          f\"Skipping validation for epoch {ep}.\")\n",
        "                    vloss, vacc = 0.0, 0.0\n",
        "\n",
        "            print(f\"[FULL TRAIN] Epoch {ep:03d}/{epochs} | \"\n",
        "                  f\"Train Loss: {avg_loss_epoch:.4f} Acc: {avg_acc_epoch:.4f} | \"\n",
        "                  f\"Val Loss: {vloss:.4f} Acc: {vacc:.4f} | LR: {current_lr_epoch:.6f}\")\n",
        "\n",
        "            # ---- Early stopping check ----\n",
        "            if vacc > best_val_acc_full_train + min_delta_full_train:\n",
        "                best_val_acc_full_train = vacc\n",
        "                no_improvement_epochs_full_train = 0\n",
        "            else:\n",
        "                if ep > early_stopping_grace_epochs:\n",
        "                    no_improvement_epochs_full_train += 1\n",
        "\n",
        "            if no_improvement_epochs_full_train >= patience_full_train:\n",
        "                print(f\"[FULL TRAIN] Early stopping at epoch {ep} due to no improvement for \"\n",
        "                      f\"{patience_full_train} epochs after grace period.\")\n",
        "                break\n",
        "\n",
        "        # Restore self.tgt_opt/sch if they existed before\n",
        "        if original_tgt_opt is not None:\n",
        "            self.tgt_opt = original_tgt_opt\n",
        "        if original_tgt_sch is not None:\n",
        "            self.tgt_sch = original_tgt_sch\n",
        "\n",
        "\n",
        "\n",
        "    def _apply_zero_gamma_to_residual_tails(self, m: nn.Module):\n",
        "        \"\"\"Applies zero-gamma initialization to the final norm layer of known residual blocks.\"\"\"\n",
        "        if isinstance(m, ResidualBlock): # Your DEITI ResidualBlock\n",
        "            if hasattr(m, 'bn2') and isinstance(m.bn2, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                if getattr(m.bn2, 'affine', False): # Check if affine is True\n",
        "                     if hasattr(m.bn2, 'weight') and m.bn2.weight is not None:\n",
        "                        # print(f\"Zero-gamma init for ResidualBlock final norm: {type(m.bn2)}\")\n",
        "                        nn.init.constant_(m.bn2.weight, 0)\n",
        "        elif isinstance(m, BottleneckBlock): # Your DEITI BottleneckBlock\n",
        "            if hasattr(m, 'bn3') and isinstance(m.bn3, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                 if getattr(m.bn3, 'affine', False):\n",
        "                     if hasattr(m.bn3, 'weight') and m.bn3.weight is not None:\n",
        "                        # print(f\"Zero-gamma init for BottleneckBlock final norm: {type(m.bn3)}\")\n",
        "                        nn.init.constant_(m.bn3.weight, 0)\n",
        "\n",
        "\n",
        "    def train_final(self, epochs: int = 300,\n",
        "                    model_path: str = '/content/drive/My Drive/deiti_final_architecture.pth',\n",
        "                    final_lr: float = LEARNING_RATE / 10,\n",
        "                    final_optimizer_eps: float = 1e-7,\n",
        "                    final_grad_clip_value: float = None):\n",
        "        recipe_load_path = model_path.replace(\".pth\", \"_recipe.json\")\n",
        "\n",
        "        print(\"\\n=== FINAL TRAINING STAGE ===\")\n",
        "        print(f\"Attempting to load architecture recipe from: {recipe_load_path}\")\n",
        "        print(f\"Final training LR: {final_lr:.1e}, AdamW_eps: {final_optimizer_eps:.1e}, GradClipValue: {final_grad_clip_value}\")\n",
        "\n",
        "        try:\n",
        "            with open(recipe_load_path, 'r') as f:\n",
        "                loaded_recipe = json.load(f)\n",
        "\n",
        "            num_classes = 10\n",
        "            if self.trl:\n",
        "                try:\n",
        "                    actual_dataset = self.trl.dataset\n",
        "                    while hasattr(actual_dataset, 'dataset'): actual_dataset = actual_dataset.dataset # Handle Subset\n",
        "                    if hasattr(actual_dataset, 'classes') and actual_dataset.classes: num_classes = len(actual_dataset.classes)\n",
        "                    elif hasattr(actual_dataset, 'num_classes') and actual_dataset.num_classes: num_classes = actual_dataset.num_classes # For some custom datasets\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not reliably infer num_classes from dataloader, defaulting to {num_classes}. Error: {e}\")\n",
        "\n",
        "            initial_base_width = 32 # Default base for TargetCNN reconstruction shell\n",
        "\n",
        "            self.target = self._build_target_from_recipe(loaded_recipe,\n",
        "                                                         base_width_for_fc=initial_base_width,\n",
        "                                                         num_classes=num_classes)\n",
        "            print(\"Architecture built successfully from recipe.\")\n",
        "            model_summary(self.target, \"Loaded Final Architecture from Recipe\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Recipe file not found at {recipe_load_path}. Cannot proceed with final training.\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading/building model from recipe {recipe_load_path}: {e}\")\n",
        "            raise\n",
        "            return\n",
        "\n",
        "        # Forcing GroupNorm can be a strong stabilization measure if BNs are problematic even with large batch size\n",
        "        # This is more aggressive than just handling self.batch_size == 1\n",
        "        print(\"Forcing all BatchNorm2d to GroupNorm(1,C) or Identity for maximal stability in final training.\")\n",
        "        # self._swap_bn_to_gn(self.target) # This converts BN to GN(1,C) or Identity\n",
        "\n",
        "        print(\"Re-initializing weights of the loaded architecture...\")\n",
        "\n",
        "        def _reset_params_if_available(m):\n",
        "            if hasattr(m, \"reset_parameters\"):\n",
        "                try: m.reset_parameters()\n",
        "                except Exception as e_reset: print(f\"Note: Could not call reset_parameters on {type(m)}: {e_reset}\")\n",
        "        self.target.apply(_reset_params_if_available) # General reset first\n",
        "\n",
        "        def _reinit_weights(m):\n",
        "            if isinstance(m, (nn.Conv2d, nn.Conv1d, nn.Conv3d)):\n",
        "                try: nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                except Exception as e_kaiming: print(f\"Note: Kaiming init failed for {type(m)}. Error: {e_kaiming}\")\n",
        "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                try: nn.init.xavier_uniform_(m.weight)\n",
        "                except Exception as e_xavier: print(f\"Note: Xavier init failed for {type(m)}. Error: {e_xavier}\")\n",
        "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.GroupNorm): # After _swap_bn_to_gn, we mainly have GN or Identity\n",
        "                 if getattr(m, 'affine', False):\n",
        "                    if hasattr(m, 'weight') and m.weight is not None: nn.init.constant_(m.weight, 1)\n",
        "                    if hasattr(m, 'bias') and m.bias is not None: nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        print(\"Applying specific re-initialization for Conv/Linear/GroupNorm layers...\")\n",
        "        self.target.apply(_reinit_weights)\n",
        "\n",
        "        print(\"Applying zero-gamma initialization to residual tails after all other inits...\")\n",
        "        self.target.apply(self._apply_zero_gamma_to_residual_tails)\n",
        "\n",
        "        print(\"Weights re-initialized for training from scratch.\")\n",
        "        self.target = self.target.to(self.dev)\n",
        "\n",
        "        self._train_full(epochs=epochs, learning_rate=final_lr,\n",
        "                         grad_clip_value=final_grad_clip_value,\n",
        "                         optimizer_eps=final_optimizer_eps)\n",
        "\n",
        "        print(\"\\n=== FINAL TEST EVALUATION ===\")\n",
        "        self.target.eval()\n",
        "        total_correct_test, total_samples_test = 0, 0\n",
        "\n",
        "        if not self.tsl or len(self.tsl) == 0:\n",
        "            print(\"Warning (FINAL TEST): Test loader empty or has zero length. Skipping final test.\")\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                for x_test, y_test in self.tsl:\n",
        "                    x_test, y_test = x_test.to(self.dev), y_test.to(self.dev)\n",
        "                    if not torch.isfinite(x_test).all():\n",
        "                        print(f\"Warning (FINAL TEST): Non-finite input x_test. Skipping test batch.\")\n",
        "                        continue\n",
        "                    with torch.amp.autocast(device_type=self.dev.type, enabled=self.amp_enabled):\n",
        "                        logits_test = self.target(x_test)\n",
        "\n",
        "                    if not torch.isfinite(logits_test).all():\n",
        "                        print(f\"Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: {logits_test.sum().item()}. This batch's accuracy may be affected or skipped.\")\n",
        "                        continue\n",
        "\n",
        "                    predictions_test = logits_test.argmax(1)\n",
        "                    total_correct_test += (predictions_test == y_test).sum().item()\n",
        "                    total_samples_test += y_test.size(0)\n",
        "\n",
        "            if total_samples_test > 0:\n",
        "                print(f\"Final Test Acc: {total_correct_test/total_samples_test:.4f}\")\n",
        "            else:\n",
        "                print(\"Warning (FINAL TEST): No samples successfully processed in test set for final evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZnaWo9jZEJY",
        "outputId": "43833838-b6a0-47b0-fff6-5216303d4099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fEUEl1xYn9l2",
        "outputId": "951b807d-a439-4537-c704-74b7139dfffb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 41.3M/170M [00:04<00:14, 9.22MB/s]\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-37bf36b54665>\", line 26, in <cell line: 0>\n",
            "    search_instance = DEITI(\n",
            "                      ^^^^^^\n",
            "  File \"<ipython-input-2-4ea7057aa7af>\", line 570, in __init__\n",
            "    full = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=tf_train)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/cifar.py\", line 66, in __init__\n",
            "    self.download()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/cifar.py\", line 139, in download\n",
            "    download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/utils.py\", line 391, in download_and_extract_archive\n",
            "    download_url(url, download_root, filename, md5)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/utils.py\", line 130, in download_url\n",
            "    _urlretrieve(url, fpath)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/utils.py\", line 30, in _urlretrieve\n",
            "    while chunk := response.read(chunk_size):\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 473, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1166, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 988, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "       ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 283, in ismodule\n",
            "    def ismodule(object):\n",
            "    \n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-37bf36b54665>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Initialize DEITI for the search phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         search_instance = DEITI(\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mpre_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# As per your example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4ea7057aa7af>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pre_epochs, post_epochs, batches_per_epoch, batch_size)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# ─────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mfull\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0mval_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "# Cell 3: ENTRY POINT - ARCHITECTURE SEARCH\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\"\"\"\n",
        "DEITI — Dynamic Editing with Iterative Targeted Improvement - Search Phase\n",
        "Supports ADD_STAGE edits with robust pooling logic to avoid zero-dimension errors,\n",
        "full multi-epoch training, AMP everywhere, detailed per-iteration logging,\n",
        "gradient accumulation to reduce memory, gradient clipping to prevent explosion,\n",
        "and tunable hyperparameters.\n",
        "\"\"\"\n",
        "\n",
        "# Hyperparameters (easy to edit)\n",
        "# These are now mostly defaults in DEITI class, but can be overridden here.\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from google.colab import runtime # For unassigning runtime\n",
        "\n",
        "    # Define the base path for saving the found architecture recipe.\n",
        "    # The .train() method will append \"_recipe.json\" to this path.\n",
        "    # This path should be on your Google Drive.\n",
        "    architecture_base_save_path = '/content/drive/My Drive/deiti_final_architecture' # Note: No .pth or .json here\n",
        "\n",
        "    try:\n",
        "        # Initialize DEITI for the search phase\n",
        "        search_instance = DEITI(\n",
        "            pre_epochs=3,       # As per your example\n",
        "            post_epochs=3,      # As per your example\n",
        "            batches_per_epoch=None, # As per your example\n",
        "            batch_size= 128   # As per your example\n",
        "        )\n",
        "        # Pass the base save path to the train method\n",
        "        search_instance.train(\n",
        "            iters=100,          # Number of search iterations\n",
        "            model_save_path=architecture_base_save_path # Pass the base path\n",
        "        )\n",
        "    except Exception:\n",
        "        # This structure is preserved from your original code\n",
        "        print(\"An error occurred during the search phase. Raising exception.\")\n",
        "        raise # First raise\n",
        "        \"\"\"try:\n",
        "            print(\"Attempting to unassign runtime after error...\")\n",
        "            runtime.unassign()\n",
        "        except Exception:\n",
        "            pass\n",
        "        raise # Second raise (likely unreachable if first one stops cell)\"\"\"\n",
        "    else:\n",
        "        # If no error, still clean up by unassigning runtime\n",
        "        print(\"Search phase completed successfully.\") # Removed \"Unassigning runtime.\" as it's commented out.\n",
        "        \"\"\"try:\n",
        "            runtime.unassign()\n",
        "        except Exception:\n",
        "            pass\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "053kuUeJtRUp",
        "outputId": "d6b767b8-f328-494a-8ed3-fbb91f1099c2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to load architecture recipe from: /content/drive/My Drive/deiti_final_architecture_recipe.json\n",
            "Ensure Google Drive is mounted and the file exists.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-28f9fe464a7c>:629: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler_tgt  = GradScaler()\n",
            "<ipython-input-1-28f9fe464a7c>:630: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler_meta = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling train_final with custom final_lr: 0.0008\n",
            "\n",
            "=== FINAL TRAINING STAGE ===\n",
            "Attempting to load architecture recipe from: /content/drive/My Drive/deiti_final_architecture\n",
            "Final training LR: 8.0e-04, AdamW_eps: 1.0e-07, GradClipValue: None\n",
            "Building TargetCNN from recipe...\n",
            "Architecture built successfully from recipe.\n",
            "\n",
            "--- Loaded Final Architecture from Recipe ---\n",
            "TargetCNN(\n",
            "  (stages): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(3, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (2): SEBlock(\n",
            "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc): Sequential(\n",
            "          (0): Linear(in_features=576, out_features=36, bias=False)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=36, out_features=576, bias=False)\n",
            "          (3): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 144, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 144, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 576, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (4): InceptionBlock(\n",
            "        (branch1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (branch2): Sequential(\n",
            "          (0): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "        (branch3): Sequential(\n",
            "          (0): Conv2d(576, 72, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): Conv2d(72, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        )\n",
            "        (branch4): Sequential(\n",
            "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "          (1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(720, 180, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 180, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 180, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(180, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 720, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (6): BottleneckBlock(\n",
            "        (conv1): Conv2d(720, 180, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 180, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 180, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(180, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 720, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (7): Conv2d(720, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (8): SEBlock(\n",
            "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=4, bias=False)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=4, out_features=64, bias=False)\n",
            "          (3): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "      (9): SEBlock(\n",
            "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc): Sequential(\n",
            "          (0): Linear(in_features=64, out_features=4, bias=False)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=4, out_features=64, bias=False)\n",
            "          (3): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "      (10): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): BottleneckBlock(\n",
            "        (conv1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 8, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 8, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (13): ReLU()\n",
            "      (14): SafeMaxPool2d(\n",
            "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (2): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (3): SEBlock(\n",
            "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc): Sequential(\n",
            "          (0): Linear(in_features=128, out_features=8, bias=False)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=8, out_features=128, bias=False)\n",
            "          (3): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "      (4): ResidualBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (drop_path): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (6): BottleneckBlock(\n",
            "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (7): BottleneckBlock(\n",
            "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (8): BottleneckBlock(\n",
            "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (9): ResidualBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (drop_path): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (10): ResidualBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (drop_path): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (11): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "      (12): ReLU()\n",
            "      (13): SafeMaxPool2d(\n",
            "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv2d(128, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (drop_path): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(768, 4608, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(4608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(4608, 4608, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4608, bias=False)\n",
            "          (4): BatchNorm2d(4608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(4608, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (3): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
            "          (4): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (5): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
            "          (4): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (6): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
            "          (4): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (7): BottleneckBlock(\n",
            "        (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (8): ResidualBlock(\n",
            "        (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (drop_path): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (9): BottleneckBlock(\n",
            "        (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (10): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
            "          (4): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (11): BottleneckBlock(\n",
            "        (conv1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 96, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 384, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (12): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (14): ResidualBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (drop_path): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (15): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (16): SEBlock(\n",
            "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc): Sequential(\n",
            "          (0): Linear(in_features=128, out_features=8, bias=False)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=8, out_features=128, bias=False)\n",
            "          (3): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "      (17): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (19): ReLU()\n",
            "      (20): AdaptiveAvgPool2d(output_size=[1, 1])\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): SEBlock(\n",
            "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=12, bias=False)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=12, out_features=192, bias=False)\n",
            "          (3): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "      (2): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
            "          (4): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
            "        (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): GroupNorm(1, 48, eps=1e-05, affine=True)\n",
            "        (conv3): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (down): Identity()\n",
            "      )\n",
            "      (4): SEBlock(\n",
            "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc): Sequential(\n",
            "          (0): Linear(in_features=192, out_features=12, bias=False)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=12, out_features=192, bias=False)\n",
            "          (3): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "      (5): ResidualBlock(\n",
            "        (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (drop_path): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (6): InceptionBlock(\n",
            "        (branch1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (branch2): Sequential(\n",
            "          (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "        (branch3): Sequential(\n",
            "          (0): Conv2d(192, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (1): Conv2d(24, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "        )\n",
            "        (branch4): Sequential(\n",
            "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "          (1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (7): GroupNorm(1, 240, eps=1e-05, affine=True)\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): SafeMaxPool2d(\n",
            "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (drop_path): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): GroupNorm(1, 240, eps=1e-05, affine=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): SafeMaxPool2d(\n",
            "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Conv2d(240, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): ResidualBlock(\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (drop_path): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): SEBlock(\n",
            "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "        (fc): Sequential(\n",
            "          (0): Linear(in_features=32, out_features=2, bias=False)\n",
            "          (1): ReLU(inplace=True)\n",
            "          (2): Linear(in_features=2, out_features=32, bias=False)\n",
            "          (3): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "      (3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (5): MBConv(\n",
            "        (block): Sequential(\n",
            "          (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "          (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace=True)\n",
            "          (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): SafeMaxPool2d(\n",
            "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pre_fc_drop): Dropout(p=0.25, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "Total parameters: 44008074\n",
            "-----------------\n",
            "\n",
            "Forcing all BatchNorm2d to GroupNorm(1,C) or Identity for maximal stability in final training.\n",
            "Re-initializing weights of the loaded architecture...\n",
            "Applying specific re-initialization for Conv/Linear/GroupNorm layers...\n",
            "Applying zero-gamma initialization to residual tails after all other inits...\n",
            "Weights re-initialized for training from scratch.\n",
            "--- Starting _train_full with LR: 8.0e-04, AdamW_eps: 1.0e-07, GradClipNorm: 1.0, GradClipVal: None for 300 epochs ---\n",
            "[FULL TRAIN] Epoch 001/300 | Train Loss: 2.4718 Acc: 0.1068 | Val Loss: 2.2979 Acc: 0.1247 | LR: 0.000162\n",
            "[FULL TRAIN] Epoch 002/300 | Train Loss: 2.3300 Acc: 0.1310 | Val Loss: 2.1677 Acc: 0.1761 | LR: 0.000167\n",
            "[FULL TRAIN] Epoch 003/300 | Train Loss: 2.2307 Acc: 0.1704 | Val Loss: 2.0832 Acc: 0.2058 | LR: 0.000175\n",
            "[FULL TRAIN] Epoch 004/300 | Train Loss: 2.1666 Acc: 0.2055 | Val Loss: 2.0022 Acc: 0.2406 | LR: 0.000187\n",
            "[FULL TRAIN] Epoch 005/300 | Train Loss: 2.1098 Acc: 0.2419 | Val Loss: 1.9045 Acc: 0.3001 | LR: 0.000202\n",
            "[FULL TRAIN] Epoch 006/300 | Train Loss: 2.0813 Acc: 0.2630 | Val Loss: 1.8568 Acc: 0.3217 | LR: 0.000220\n",
            "[FULL TRAIN] Epoch 007/300 | Train Loss: 2.0300 Acc: 0.2919 | Val Loss: 1.8208 Acc: 0.3353 | LR: 0.000242\n",
            "[FULL TRAIN] Epoch 008/300 | Train Loss: 1.9840 Acc: 0.3269 | Val Loss: 1.6982 Acc: 0.3793 | LR: 0.000267\n",
            "[FULL TRAIN] Epoch 009/300 | Train Loss: 1.9535 Acc: 0.3519 | Val Loss: 1.6884 Acc: 0.3912 | LR: 0.000295\n",
            "[FULL TRAIN] Epoch 010/300 | Train Loss: 1.9434 Acc: 0.3639 | Val Loss: 1.5648 Acc: 0.4651 | LR: 0.000326\n",
            "[FULL TRAIN] Epoch 011/300 | Train Loss: 1.9132 Acc: 0.3839 | Val Loss: 1.5909 Acc: 0.4564 | LR: 0.000360\n",
            "[FULL TRAIN] Epoch 012/300 | Train Loss: 1.8857 Acc: 0.3977 | Val Loss: 1.5542 Acc: 0.4792 | LR: 0.000398\n",
            "[FULL TRAIN] Epoch 013/300 | Train Loss: 1.8644 Acc: 0.4125 | Val Loss: 1.5949 Acc: 0.4644 | LR: 0.000438\n",
            "[FULL TRAIN] Epoch 014/300 | Train Loss: 1.8528 Acc: 0.4219 | Val Loss: 1.4349 Acc: 0.5219 | LR: 0.000481\n",
            "[FULL TRAIN] Epoch 015/300 | Train Loss: 1.8547 Acc: 0.4226 | Val Loss: 1.4190 Acc: 0.5395 | LR: 0.000527\n",
            "[FULL TRAIN] Epoch 016/300 | Train Loss: 1.8142 Acc: 0.4431 | Val Loss: 1.3719 Acc: 0.5230 | LR: 0.000575\n",
            "[FULL TRAIN] Epoch 017/300 | Train Loss: 1.7868 Acc: 0.4557 | Val Loss: 1.3748 Acc: 0.5422 | LR: 0.000627\n",
            "[FULL TRAIN] Epoch 018/300 | Train Loss: 1.7923 Acc: 0.4582 | Val Loss: 1.4354 Acc: 0.5296 | LR: 0.000680\n",
            "[FULL TRAIN] Epoch 019/300 | Train Loss: 1.7926 Acc: 0.4615 | Val Loss: 1.3472 Acc: 0.5633 | LR: 0.000737\n",
            "[FULL TRAIN] Epoch 020/300 | Train Loss: 1.7530 Acc: 0.4715 | Val Loss: 1.2910 Acc: 0.5930 | LR: 0.000795\n",
            "[FULL TRAIN] Epoch 021/300 | Train Loss: 1.7480 Acc: 0.4757 | Val Loss: 1.2676 Acc: 0.5942 | LR: 0.000856\n",
            "[FULL TRAIN] Epoch 022/300 | Train Loss: 1.7335 Acc: 0.4821 | Val Loss: 1.3095 Acc: 0.5881 | LR: 0.000919\n",
            "[FULL TRAIN] Epoch 023/300 | Train Loss: 1.7318 Acc: 0.4883 | Val Loss: 1.2829 Acc: 0.5926 | LR: 0.000984\n",
            "[FULL TRAIN] Epoch 024/300 | Train Loss: 1.7486 Acc: 0.4821 | Val Loss: 1.1906 Acc: 0.6158 | LR: 0.001051\n",
            "[FULL TRAIN] Epoch 025/300 | Train Loss: 1.7118 Acc: 0.5031 | Val Loss: 1.2090 Acc: 0.6188 | LR: 0.001120\n",
            "[FULL TRAIN] Epoch 026/300 | Train Loss: 1.6960 Acc: 0.5054 | Val Loss: 1.1885 Acc: 0.6257 | LR: 0.001191\n",
            "[FULL TRAIN] Epoch 027/300 | Train Loss: 1.6934 Acc: 0.5171 | Val Loss: 1.1955 Acc: 0.6320 | LR: 0.001263\n",
            "[FULL TRAIN] Epoch 028/300 | Train Loss: 1.6987 Acc: 0.5079 | Val Loss: 1.1470 Acc: 0.6519 | LR: 0.001336\n",
            "[FULL TRAIN] Epoch 029/300 | Train Loss: 1.6772 Acc: 0.5232 | Val Loss: 1.1704 Acc: 0.6224 | LR: 0.001411\n",
            "[FULL TRAIN] Epoch 030/300 | Train Loss: 1.6653 Acc: 0.5271 | Val Loss: 1.1177 Acc: 0.6426 | LR: 0.001487\n",
            "[FULL TRAIN] Epoch 031/300 | Train Loss: 1.6975 Acc: 0.5157 | Val Loss: 1.1470 Acc: 0.6623 | LR: 0.001564\n",
            "[FULL TRAIN] Epoch 032/300 | Train Loss: 1.6558 Acc: 0.5249 | Val Loss: 1.1763 Acc: 0.6327 | LR: 0.001642\n",
            "[FULL TRAIN] Epoch 033/300 | Train Loss: 1.6589 Acc: 0.5373 | Val Loss: 1.1372 Acc: 0.6537 | LR: 0.001720\n",
            "[FULL TRAIN] Epoch 034/300 | Train Loss: 1.6339 Acc: 0.5414 | Val Loss: 1.0854 Acc: 0.6648 | LR: 0.001800\n",
            "[FULL TRAIN] Epoch 035/300 | Train Loss: 1.6337 Acc: 0.5440 | Val Loss: 1.1357 Acc: 0.6731 | LR: 0.001879\n",
            "[FULL TRAIN] Epoch 036/300 | Train Loss: 1.6455 Acc: 0.5454 | Val Loss: 1.1135 Acc: 0.6770 | LR: 0.001960\n",
            "[FULL TRAIN] Epoch 037/300 | Train Loss: 1.6169 Acc: 0.5480 | Val Loss: 1.0892 Acc: 0.6690 | LR: 0.002040\n",
            "[FULL TRAIN] Epoch 038/300 | Train Loss: 1.6357 Acc: 0.5455 | Val Loss: 1.0645 Acc: 0.6835 | LR: 0.002120\n",
            "[FULL TRAIN] Epoch 039/300 | Train Loss: 1.6513 Acc: 0.5420 | Val Loss: 0.9933 Acc: 0.6986 | LR: 0.002201\n",
            "[FULL TRAIN] Epoch 040/300 | Train Loss: 1.5996 Acc: 0.5637 | Val Loss: 1.0518 Acc: 0.6699 | LR: 0.002281\n",
            "[FULL TRAIN] Epoch 041/300 | Train Loss: 1.6331 Acc: 0.5502 | Val Loss: 1.7498 Acc: 0.4041 | LR: 0.002361\n",
            "[FULL TRAIN] Epoch 042/300 | Train Loss: 1.6347 Acc: 0.5446 | Val Loss: 1.2053 Acc: 0.6314 | LR: 0.002440\n",
            "[FULL TRAIN] Epoch 043/300 | Train Loss: 1.6170 Acc: 0.5554 | Val Loss: 1.0077 Acc: 0.6979 | LR: 0.002519\n",
            "[FULL TRAIN] Epoch 044/300 | Train Loss: 1.5830 Acc: 0.5684 | Val Loss: 1.1681 Acc: 0.6237 | LR: 0.002596\n",
            "[FULL TRAIN] Epoch 045/300 | Train Loss: 1.6261 Acc: 0.5537 | Val Loss: 0.9988 Acc: 0.7086 | LR: 0.002673\n",
            "[FULL TRAIN] Epoch 046/300 | Train Loss: 1.5901 Acc: 0.5607 | Val Loss: 1.0539 Acc: 0.6869 | LR: 0.002749\n",
            "[FULL TRAIN] Epoch 047/300 | Train Loss: 1.6198 Acc: 0.5528 | Val Loss: 1.0838 Acc: 0.6726 | LR: 0.002824\n",
            "[FULL TRAIN] Epoch 048/300 | Train Loss: 1.5629 Acc: 0.5798 | Val Loss: 0.9853 Acc: 0.6921 | LR: 0.002898\n",
            "[FULL TRAIN] Epoch 049/300 | Train Loss: 1.5788 Acc: 0.5706 | Val Loss: 0.9588 Acc: 0.7149 | LR: 0.002970\n",
            "[FULL TRAIN] Epoch 050/300 | Train Loss: 1.5634 Acc: 0.5758 | Val Loss: 0.9727 Acc: 0.6975 | LR: 0.003040\n",
            "[FULL TRAIN] Epoch 051/300 | Train Loss: 1.5449 Acc: 0.5870 | Val Loss: 0.9421 Acc: 0.7176 | LR: 0.003109\n",
            "[FULL TRAIN] Epoch 052/300 | Train Loss: 1.5572 Acc: 0.5772 | Val Loss: 1.0331 Acc: 0.7139 | LR: 0.003176\n",
            "[FULL TRAIN] Epoch 053/300 | Train Loss: 1.5210 Acc: 0.5978 | Val Loss: 0.9115 Acc: 0.7350 | LR: 0.003241\n",
            "[FULL TRAIN] Epoch 054/300 | Train Loss: 1.5589 Acc: 0.5848 | Val Loss: 0.9556 Acc: 0.7214 | LR: 0.003304\n",
            "[FULL TRAIN] Epoch 055/300 | Train Loss: 1.5485 Acc: 0.5813 | Val Loss: 1.0189 Acc: 0.7163 | LR: 0.003365\n",
            "[FULL TRAIN] Epoch 056/300 | Train Loss: 1.5461 Acc: 0.5858 | Val Loss: 0.9991 Acc: 0.7188 | LR: 0.003423\n",
            "[FULL TRAIN] Epoch 057/300 | Train Loss: 1.5139 Acc: 0.5864 | Val Loss: 0.9972 Acc: 0.7072 | LR: 0.003480\n",
            "[FULL TRAIN] Epoch 058/300 | Train Loss: 1.5529 Acc: 0.5813 | Val Loss: 0.8818 Acc: 0.7497 | LR: 0.003534\n",
            "[FULL TRAIN] Epoch 059/300 | Train Loss: 1.5279 Acc: 0.5936 | Val Loss: 0.9937 Acc: 0.7170 | LR: 0.003585\n",
            "[FULL TRAIN] Epoch 060/300 | Train Loss: 1.5071 Acc: 0.6047 | Val Loss: 0.9309 Acc: 0.7300 | LR: 0.003633\n",
            "[FULL TRAIN] Epoch 061/300 | Train Loss: 1.5153 Acc: 0.6004 | Val Loss: 0.9157 Acc: 0.7413 | LR: 0.003679\n",
            "[FULL TRAIN] Epoch 062/300 | Train Loss: 1.4856 Acc: 0.6037 | Val Loss: 0.9037 Acc: 0.7454 | LR: 0.003722\n",
            "[FULL TRAIN] Epoch 063/300 | Train Loss: 1.5046 Acc: 0.5995 | Val Loss: 0.9337 Acc: 0.7515 | LR: 0.003763\n",
            "[FULL TRAIN] Epoch 064/300 | Train Loss: 1.4874 Acc: 0.6103 | Val Loss: 0.8489 Acc: 0.7608 | LR: 0.003800\n",
            "[FULL TRAIN] Epoch 065/300 | Train Loss: 1.4869 Acc: 0.6158 | Val Loss: 0.9187 Acc: 0.7466 | LR: 0.003834\n",
            "[FULL TRAIN] Epoch 066/300 | Train Loss: 1.4766 Acc: 0.6118 | Val Loss: 0.8803 Acc: 0.7530 | LR: 0.003865\n",
            "[FULL TRAIN] Epoch 067/300 | Train Loss: 1.4563 Acc: 0.6137 | Val Loss: 0.8962 Acc: 0.7557 | LR: 0.003893\n",
            "[FULL TRAIN] Epoch 068/300 | Train Loss: 1.4651 Acc: 0.6192 | Val Loss: 0.8810 Acc: 0.7499 | LR: 0.003918\n",
            "[FULL TRAIN] Epoch 069/300 | Train Loss: 1.5038 Acc: 0.5996 | Val Loss: 0.9168 Acc: 0.7450 | LR: 0.003940\n",
            "[FULL TRAIN] Epoch 070/300 | Train Loss: 1.4478 Acc: 0.6243 | Val Loss: 0.8586 Acc: 0.7713 | LR: 0.003958\n",
            "[FULL TRAIN] Epoch 071/300 | Train Loss: 1.4370 Acc: 0.6245 | Val Loss: 0.8783 Acc: 0.7544 | LR: 0.003973\n",
            "[FULL TRAIN] Epoch 072/300 | Train Loss: 1.4979 Acc: 0.6106 | Val Loss: 0.8626 Acc: 0.7650 | LR: 0.003985\n",
            "[FULL TRAIN] Epoch 073/300 | Train Loss: 1.4909 Acc: 0.6140 | Val Loss: 0.8795 Acc: 0.7519 | LR: 0.003993\n",
            "[FULL TRAIN] Epoch 074/300 | Train Loss: 1.4498 Acc: 0.6275 | Val Loss: 0.7765 Acc: 0.7795 | LR: 0.003998\n",
            "[FULL TRAIN] Epoch 075/300 | Train Loss: 1.4103 Acc: 0.6381 | Val Loss: 0.8362 Acc: 0.7652 | LR: 0.004000\n",
            "[FULL TRAIN] Epoch 076/300 | Train Loss: 1.4371 Acc: 0.6224 | Val Loss: 0.8172 Acc: 0.7749 | LR: 0.004000\n",
            "[FULL TRAIN] Epoch 077/300 | Train Loss: 1.4271 Acc: 0.6306 | Val Loss: 0.8276 Acc: 0.7641 | LR: 0.003999\n",
            "[FULL TRAIN] Epoch 078/300 | Train Loss: 1.4723 Acc: 0.6215 | Val Loss: 0.7899 Acc: 0.7748 | LR: 0.003998\n",
            "[FULL TRAIN] Epoch 079/300 | Train Loss: 1.4221 Acc: 0.6409 | Val Loss: 0.7869 Acc: 0.7850 | LR: 0.003997\n",
            "[FULL TRAIN] Epoch 080/300 | Train Loss: 1.4131 Acc: 0.6437 | Val Loss: 0.7765 Acc: 0.7861 | LR: 0.003995\n",
            "[FULL TRAIN] Epoch 081/300 | Train Loss: 1.4322 Acc: 0.6372 | Val Loss: 0.7458 Acc: 0.7837 | LR: 0.003993\n",
            "[FULL TRAIN] Epoch 082/300 | Train Loss: 1.4314 Acc: 0.6315 | Val Loss: 0.7708 Acc: 0.7740 | LR: 0.003990\n",
            "[FULL TRAIN] Epoch 083/300 | Train Loss: 1.4526 Acc: 0.6195 | Val Loss: 0.8131 Acc: 0.7890 | LR: 0.003988\n",
            "[FULL TRAIN] Epoch 084/300 | Train Loss: 1.4276 Acc: 0.6315 | Val Loss: 0.7839 Acc: 0.7795 | LR: 0.003984\n",
            "[FULL TRAIN] Epoch 085/300 | Train Loss: 1.4541 Acc: 0.6256 | Val Loss: 0.9225 Acc: 0.7424 | LR: 0.003981\n",
            "[FULL TRAIN] Epoch 086/300 | Train Loss: 1.4233 Acc: 0.6383 | Val Loss: 0.7688 Acc: 0.7895 | LR: 0.003976\n",
            "[FULL TRAIN] Epoch 087/300 | Train Loss: 1.4118 Acc: 0.6429 | Val Loss: 0.8079 Acc: 0.7825 | LR: 0.003972\n",
            "[FULL TRAIN] Epoch 088/300 | Train Loss: 1.3899 Acc: 0.6489 | Val Loss: 0.7828 Acc: 0.7860 | LR: 0.003967\n",
            "[FULL TRAIN] Epoch 089/300 | Train Loss: 1.4214 Acc: 0.6420 | Val Loss: 0.7500 Acc: 0.7939 | LR: 0.003962\n",
            "[FULL TRAIN] Epoch 090/300 | Train Loss: 1.4019 Acc: 0.6455 | Val Loss: 0.8163 Acc: 0.7740 | LR: 0.003956\n",
            "[FULL TRAIN] Epoch 091/300 | Train Loss: 1.4251 Acc: 0.6407 | Val Loss: 0.7670 Acc: 0.7921 | LR: 0.003950\n",
            "[FULL TRAIN] Epoch 092/300 | Train Loss: 1.3780 Acc: 0.6500 | Val Loss: 0.7865 Acc: 0.7883 | LR: 0.003944\n",
            "[FULL TRAIN] Epoch 093/300 | Train Loss: 1.3792 Acc: 0.6554 | Val Loss: 0.7526 Acc: 0.7988 | LR: 0.003937\n",
            "[FULL TRAIN] Epoch 094/300 | Train Loss: 1.3927 Acc: 0.6505 | Val Loss: 0.8019 Acc: 0.7808 | LR: 0.003930\n",
            "[FULL TRAIN] Epoch 095/300 | Train Loss: 1.3871 Acc: 0.6493 | Val Loss: 0.7280 Acc: 0.7968 | LR: 0.003922\n",
            "[FULL TRAIN] Epoch 096/300 | Train Loss: 1.3837 Acc: 0.6531 | Val Loss: 0.7342 Acc: 0.8168 | LR: 0.003915\n",
            "[FULL TRAIN] Epoch 097/300 | Train Loss: 1.3705 Acc: 0.6586 | Val Loss: 0.7419 Acc: 0.8017 | LR: 0.003906\n",
            "[FULL TRAIN] Epoch 098/300 | Train Loss: 1.3774 Acc: 0.6569 | Val Loss: 0.7762 Acc: 0.7961 | LR: 0.003898\n",
            "[FULL TRAIN] Epoch 099/300 | Train Loss: 1.3867 Acc: 0.6589 | Val Loss: 0.7340 Acc: 0.7941 | LR: 0.003889\n",
            "[FULL TRAIN] Epoch 100/300 | Train Loss: 1.3725 Acc: 0.6636 | Val Loss: 0.7650 Acc: 0.8037 | LR: 0.003879\n",
            "[FULL TRAIN] Epoch 101/300 | Train Loss: 1.3707 Acc: 0.6584 | Val Loss: 0.7384 Acc: 0.8124 | LR: 0.003870\n",
            "[FULL TRAIN] Epoch 102/300 | Train Loss: 1.3479 Acc: 0.6649 | Val Loss: 0.7909 Acc: 0.8015 | LR: 0.003860\n",
            "[FULL TRAIN] Epoch 103/300 | Train Loss: 1.4286 Acc: 0.6385 | Val Loss: 0.7877 Acc: 0.8008 | LR: 0.003849\n",
            "[FULL TRAIN] Epoch 104/300 | Train Loss: 1.3526 Acc: 0.6754 | Val Loss: 0.7412 Acc: 0.8126 | LR: 0.003838\n",
            "[FULL TRAIN] Epoch 105/300 | Train Loss: 1.3422 Acc: 0.6683 | Val Loss: 0.7220 Acc: 0.8056 | LR: 0.003827\n",
            "[FULL TRAIN] Epoch 106/300 | Train Loss: 1.3583 Acc: 0.6704 | Val Loss: 0.6722 Acc: 0.8203 | LR: 0.003816\n",
            "[FULL TRAIN] Epoch 107/300 | Train Loss: 1.3761 Acc: 0.6587 | Val Loss: 0.7912 Acc: 0.8132 | LR: 0.003804\n",
            "[FULL TRAIN] Epoch 108/300 | Train Loss: 1.3428 Acc: 0.6713 | Val Loss: 0.6927 Acc: 0.8102 | LR: 0.003791\n",
            "[FULL TRAIN] Epoch 109/300 | Train Loss: 1.3656 Acc: 0.6660 | Val Loss: 0.6932 Acc: 0.8137 | LR: 0.003779\n",
            "[FULL TRAIN] Epoch 110/300 | Train Loss: 1.3601 Acc: 0.6648 | Val Loss: 0.6821 Acc: 0.8208 | LR: 0.003766\n",
            "[FULL TRAIN] Epoch 111/300 | Train Loss: 1.3317 Acc: 0.6734 | Val Loss: 0.7472 Acc: 0.8104 | LR: 0.003753\n",
            "[FULL TRAIN] Epoch 112/300 | Train Loss: 1.3283 Acc: 0.6760 | Val Loss: 0.7059 Acc: 0.8222 | LR: 0.003739\n",
            "[FULL TRAIN] Epoch 113/300 | Train Loss: 1.3382 Acc: 0.6749 | Val Loss: 0.6679 Acc: 0.8234 | LR: 0.003725\n",
            "[FULL TRAIN] Epoch 114/300 | Train Loss: 1.3606 Acc: 0.6708 | Val Loss: 0.6756 Acc: 0.8322 | LR: 0.003711\n",
            "[FULL TRAIN] Epoch 115/300 | Train Loss: 1.3383 Acc: 0.6770 | Val Loss: 0.6685 Acc: 0.8301 | LR: 0.003696\n",
            "[FULL TRAIN] Epoch 116/300 | Train Loss: 1.3401 Acc: 0.6869 | Val Loss: 0.6287 Acc: 0.8295 | LR: 0.003681\n",
            "[FULL TRAIN] Epoch 117/300 | Train Loss: 1.3310 Acc: 0.6764 | Val Loss: 0.6693 Acc: 0.8285 | LR: 0.003666\n",
            "[FULL TRAIN] Epoch 118/300 | Train Loss: 1.3144 Acc: 0.6877 | Val Loss: 0.6931 Acc: 0.8195 | LR: 0.003650\n",
            "[FULL TRAIN] Epoch 119/300 | Train Loss: 1.3183 Acc: 0.6882 | Val Loss: 0.6892 Acc: 0.8227 | LR: 0.003634\n",
            "[FULL TRAIN] Epoch 120/300 | Train Loss: 1.2871 Acc: 0.6905 | Val Loss: 0.6758 Acc: 0.8227 | LR: 0.003618\n",
            "[FULL TRAIN] Epoch 121/300 | Train Loss: 1.3163 Acc: 0.6799 | Val Loss: 0.6343 Acc: 0.8311 | LR: 0.003601\n",
            "[FULL TRAIN] Epoch 122/300 | Train Loss: 1.3112 Acc: 0.6854 | Val Loss: 0.6506 Acc: 0.8275 | LR: 0.003585\n",
            "[FULL TRAIN] Epoch 123/300 | Train Loss: 1.3263 Acc: 0.6810 | Val Loss: 0.7034 Acc: 0.8337 | LR: 0.003567\n",
            "[FULL TRAIN] Epoch 124/300 | Train Loss: 1.3073 Acc: 0.6906 | Val Loss: 0.6233 Acc: 0.8375 | LR: 0.003550\n",
            "[FULL TRAIN] Epoch 125/300 | Train Loss: 1.2926 Acc: 0.6964 | Val Loss: 0.6282 Acc: 0.8234 | LR: 0.003532\n",
            "[FULL TRAIN] Epoch 126/300 | Train Loss: 1.2906 Acc: 0.6921 | Val Loss: 0.6735 Acc: 0.8345 | LR: 0.003514\n",
            "[FULL TRAIN] Epoch 127/300 | Train Loss: 1.2942 Acc: 0.6919 | Val Loss: 0.6073 Acc: 0.8397 | LR: 0.003496\n",
            "[FULL TRAIN] Epoch 128/300 | Train Loss: 1.2961 Acc: 0.6883 | Val Loss: 0.6200 Acc: 0.8343 | LR: 0.003477\n",
            "[FULL TRAIN] Epoch 129/300 | Train Loss: 1.3190 Acc: 0.6901 | Val Loss: 0.6447 Acc: 0.8364 | LR: 0.003458\n",
            "[FULL TRAIN] Epoch 130/300 | Train Loss: 1.2943 Acc: 0.6944 | Val Loss: 0.6889 Acc: 0.8234 | LR: 0.003439\n",
            "[FULL TRAIN] Epoch 131/300 | Train Loss: 1.3224 Acc: 0.6898 | Val Loss: 0.5840 Acc: 0.8396 | LR: 0.003419\n",
            "[FULL TRAIN] Epoch 132/300 | Train Loss: 1.2875 Acc: 0.6883 | Val Loss: 0.6137 Acc: 0.8400 | LR: 0.003399\n",
            "[FULL TRAIN] Epoch 133/300 | Train Loss: 1.2998 Acc: 0.6924 | Val Loss: 0.6456 Acc: 0.8409 | LR: 0.003379\n",
            "[FULL TRAIN] Epoch 134/300 | Train Loss: 1.2805 Acc: 0.6936 | Val Loss: 0.6020 Acc: 0.8451 | LR: 0.003359\n",
            "[FULL TRAIN] Epoch 135/300 | Train Loss: 1.2651 Acc: 0.6974 | Val Loss: 0.6425 Acc: 0.8394 | LR: 0.003338\n",
            "[FULL TRAIN] Epoch 136/300 | Train Loss: 1.2793 Acc: 0.6985 | Val Loss: 0.6610 Acc: 0.8426 | LR: 0.003317\n",
            "[FULL TRAIN] Epoch 137/300 | Train Loss: 1.2657 Acc: 0.7059 | Val Loss: 0.6502 Acc: 0.8311 | LR: 0.003296\n",
            "[FULL TRAIN] Epoch 138/300 | Train Loss: 1.2785 Acc: 0.7043 | Val Loss: 0.5942 Acc: 0.8442 | LR: 0.003275\n",
            "[FULL TRAIN] Epoch 139/300 | Train Loss: 1.2611 Acc: 0.7049 | Val Loss: 0.6196 Acc: 0.8308 | LR: 0.003253\n",
            "[FULL TRAIN] Epoch 140/300 | Train Loss: 1.2701 Acc: 0.7013 | Val Loss: 0.6202 Acc: 0.8353 | LR: 0.003231\n",
            "[FULL TRAIN] Epoch 141/300 | Train Loss: 1.2744 Acc: 0.7144 | Val Loss: 0.6429 Acc: 0.8445 | LR: 0.003209\n",
            "[FULL TRAIN] Epoch 142/300 | Train Loss: 1.2865 Acc: 0.6981 | Val Loss: 0.6052 Acc: 0.8514 | LR: 0.003187\n",
            "[FULL TRAIN] Epoch 143/300 | Train Loss: 1.2412 Acc: 0.7122 | Val Loss: 0.5667 Acc: 0.8513 | LR: 0.003164\n",
            "[FULL TRAIN] Epoch 144/300 | Train Loss: 1.2773 Acc: 0.6938 | Val Loss: 0.6250 Acc: 0.8457 | LR: 0.003141\n",
            "[FULL TRAIN] Epoch 145/300 | Train Loss: 1.2685 Acc: 0.7027 | Val Loss: 0.6538 Acc: 0.8499 | LR: 0.003118\n",
            "[FULL TRAIN] Epoch 146/300 | Train Loss: 1.2588 Acc: 0.7135 | Val Loss: 0.5959 Acc: 0.8557 | LR: 0.003095\n",
            "[FULL TRAIN] Epoch 147/300 | Train Loss: 1.2491 Acc: 0.7110 | Val Loss: 0.5995 Acc: 0.8475 | LR: 0.003072\n",
            "[FULL TRAIN] Epoch 148/300 | Train Loss: 1.2624 Acc: 0.7051 | Val Loss: 0.5631 Acc: 0.8540 | LR: 0.003048\n",
            "[FULL TRAIN] Epoch 149/300 | Train Loss: 1.2556 Acc: 0.7108 | Val Loss: 0.5680 Acc: 0.8544 | LR: 0.003024\n",
            "[FULL TRAIN] Epoch 150/300 | Train Loss: 1.2525 Acc: 0.7151 | Val Loss: 0.5919 Acc: 0.8392 | LR: 0.003000\n",
            "[FULL TRAIN] Epoch 151/300 | Train Loss: 1.2489 Acc: 0.7056 | Val Loss: 0.5673 Acc: 0.8521 | LR: 0.002976\n",
            "[FULL TRAIN] Epoch 152/300 | Train Loss: 1.2365 Acc: 0.7106 | Val Loss: 0.6140 Acc: 0.8475 | LR: 0.002951\n",
            "[FULL TRAIN] Epoch 153/300 | Train Loss: 1.2394 Acc: 0.7092 | Val Loss: 0.5988 Acc: 0.8596 | LR: 0.002927\n",
            "[FULL TRAIN] Epoch 154/300 | Train Loss: 1.2716 Acc: 0.7088 | Val Loss: 0.5792 Acc: 0.8506 | LR: 0.002902\n",
            "[FULL TRAIN] Epoch 155/300 | Train Loss: 1.2664 Acc: 0.7073 | Val Loss: 0.6017 Acc: 0.8564 | LR: 0.002877\n",
            "[FULL TRAIN] Epoch 156/300 | Train Loss: 1.2409 Acc: 0.7062 | Val Loss: 0.5928 Acc: 0.8538 | LR: 0.002851\n",
            "[FULL TRAIN] Epoch 157/300 | Train Loss: 1.2316 Acc: 0.7186 | Val Loss: 0.6015 Acc: 0.8567 | LR: 0.002826\n",
            "[FULL TRAIN] Epoch 158/300 | Train Loss: 1.2448 Acc: 0.7076 | Val Loss: 0.5981 Acc: 0.8542 | LR: 0.002801\n",
            "[FULL TRAIN] Epoch 159/300 | Train Loss: 1.2445 Acc: 0.7169 | Val Loss: 0.6057 Acc: 0.8390 | LR: 0.002775\n",
            "[FULL TRAIN] Epoch 160/300 | Train Loss: 1.2312 Acc: 0.7206 | Val Loss: 0.5572 Acc: 0.8578 | LR: 0.002749\n",
            "[FULL TRAIN] Epoch 161/300 | Train Loss: 1.2132 Acc: 0.7333 | Val Loss: 0.5961 Acc: 0.8513 | LR: 0.002723\n",
            "[FULL TRAIN] Epoch 162/300 | Train Loss: 1.2050 Acc: 0.7366 | Val Loss: 0.5771 Acc: 0.8586 | LR: 0.002697\n",
            "[FULL TRAIN] Epoch 163/300 | Train Loss: 1.2254 Acc: 0.7203 | Val Loss: 0.5979 Acc: 0.8607 | LR: 0.002671\n",
            "[FULL TRAIN] Epoch 164/300 | Train Loss: 1.2410 Acc: 0.7166 | Val Loss: 0.5888 Acc: 0.8504 | LR: 0.002644\n",
            "[FULL TRAIN] Epoch 165/300 | Train Loss: 1.2095 Acc: 0.7294 | Val Loss: 0.5826 Acc: 0.8526 | LR: 0.002618\n",
            "[FULL TRAIN] Epoch 166/300 | Train Loss: 1.2407 Acc: 0.7078 | Val Loss: 0.6065 Acc: 0.8625 | LR: 0.002591\n",
            "[FULL TRAIN] Epoch 167/300 | Train Loss: 1.2238 Acc: 0.7239 | Val Loss: 0.5501 Acc: 0.8568 | LR: 0.002565\n",
            "[FULL TRAIN] Epoch 168/300 | Train Loss: 1.2306 Acc: 0.7225 | Val Loss: 0.6343 Acc: 0.8576 | LR: 0.002538\n",
            "[FULL TRAIN] Epoch 169/300 | Train Loss: 1.2259 Acc: 0.7257 | Val Loss: 0.5381 Acc: 0.8608 | LR: 0.002511\n",
            "[FULL TRAIN] Epoch 170/300 | Train Loss: 1.2014 Acc: 0.7343 | Val Loss: 0.5248 Acc: 0.8633 | LR: 0.002484\n",
            "[FULL TRAIN] Epoch 171/300 | Train Loss: 1.1916 Acc: 0.7253 | Val Loss: 0.5069 Acc: 0.8609 | LR: 0.002457\n",
            "[FULL TRAIN] Epoch 172/300 | Train Loss: 1.2319 Acc: 0.7203 | Val Loss: 0.5496 Acc: 0.8657 | LR: 0.002429\n",
            "[FULL TRAIN] Epoch 173/300 | Train Loss: 1.2019 Acc: 0.7247 | Val Loss: 0.5555 Acc: 0.8612 | LR: 0.002402\n",
            "[FULL TRAIN] Epoch 174/300 | Train Loss: 1.1840 Acc: 0.7449 | Val Loss: 0.5348 Acc: 0.8514 | LR: 0.002375\n",
            "[FULL TRAIN] Epoch 175/300 | Train Loss: 1.1633 Acc: 0.7401 | Val Loss: 0.5209 Acc: 0.8635 | LR: 0.002347\n",
            "[FULL TRAIN] Epoch 176/300 | Train Loss: 1.1992 Acc: 0.7294 | Val Loss: 0.6065 Acc: 0.8656 | LR: 0.002320\n",
            "[FULL TRAIN] Epoch 177/300 | Train Loss: 1.1814 Acc: 0.7376 | Val Loss: 0.5458 Acc: 0.8686 | LR: 0.002292\n",
            "[FULL TRAIN] Epoch 178/300 | Train Loss: 1.2032 Acc: 0.7291 | Val Loss: 0.5371 Acc: 0.8642 | LR: 0.002264\n",
            "[FULL TRAIN] Epoch 179/300 | Train Loss: 1.1665 Acc: 0.7371 | Val Loss: 0.5868 Acc: 0.8651 | LR: 0.002237\n",
            "[FULL TRAIN] Epoch 180/300 | Train Loss: 1.1733 Acc: 0.7356 | Val Loss: 0.5144 Acc: 0.8708 | LR: 0.002209\n",
            "[FULL TRAIN] Epoch 181/300 | Train Loss: 1.2130 Acc: 0.7292 | Val Loss: 0.6213 Acc: 0.8698 | LR: 0.002181\n",
            "[FULL TRAIN] Epoch 182/300 | Train Loss: 1.1915 Acc: 0.7311 | Val Loss: 0.5634 Acc: 0.8641 | LR: 0.002153\n",
            "[FULL TRAIN] Epoch 183/300 | Train Loss: 1.1854 Acc: 0.7298 | Val Loss: 0.5453 Acc: 0.8685 | LR: 0.002125\n",
            "[FULL TRAIN] Epoch 184/300 | Train Loss: 1.1543 Acc: 0.7464 | Val Loss: 0.5605 Acc: 0.8733 | LR: 0.002098\n",
            "[FULL TRAIN] Epoch 185/300 | Train Loss: 1.1612 Acc: 0.7489 | Val Loss: 0.5642 Acc: 0.8701 | LR: 0.002070\n",
            "[FULL TRAIN] Epoch 186/300 | Train Loss: 1.1732 Acc: 0.7408 | Val Loss: 0.5512 Acc: 0.8619 | LR: 0.002042\n",
            "[FULL TRAIN] Epoch 187/300 | Train Loss: 1.1550 Acc: 0.7508 | Val Loss: 0.5204 Acc: 0.8677 | LR: 0.002014\n",
            "[FULL TRAIN] Epoch 188/300 | Train Loss: 1.1717 Acc: 0.7484 | Val Loss: 0.5210 Acc: 0.8788 | LR: 0.001986\n",
            "[FULL TRAIN] Epoch 189/300 | Train Loss: 1.1761 Acc: 0.7410 | Val Loss: 0.5549 Acc: 0.8676 | LR: 0.001958\n",
            "[FULL TRAIN] Epoch 190/300 | Train Loss: 1.2111 Acc: 0.7283 | Val Loss: 0.5579 Acc: 0.8714 | LR: 0.001930\n",
            "[FULL TRAIN] Epoch 191/300 | Train Loss: 1.1792 Acc: 0.7452 | Val Loss: 0.5439 Acc: 0.8632 | LR: 0.001902\n",
            "[FULL TRAIN] Epoch 192/300 | Train Loss: 1.1668 Acc: 0.7411 | Val Loss: 0.5512 Acc: 0.8739 | LR: 0.001874\n",
            "[FULL TRAIN] Epoch 193/300 | Train Loss: 1.1481 Acc: 0.7461 | Val Loss: 0.4972 Acc: 0.8691 | LR: 0.001846\n",
            "[FULL TRAIN] Epoch 194/300 | Train Loss: 1.1656 Acc: 0.7504 | Val Loss: 0.5528 Acc: 0.8727 | LR: 0.001819\n",
            "[FULL TRAIN] Epoch 195/300 | Train Loss: 1.1575 Acc: 0.7487 | Val Loss: 0.4663 Acc: 0.8721 | LR: 0.001791\n",
            "[FULL TRAIN] Epoch 196/300 | Train Loss: 1.1913 Acc: 0.7324 | Val Loss: 0.5023 Acc: 0.8803 | LR: 0.001763\n",
            "[FULL TRAIN] Epoch 197/300 | Train Loss: 1.1317 Acc: 0.7538 | Val Loss: 0.5078 Acc: 0.8678 | LR: 0.001735\n",
            "[FULL TRAIN] Epoch 198/300 | Train Loss: 1.1662 Acc: 0.7484 | Val Loss: 0.5030 Acc: 0.8768 | LR: 0.001708\n",
            "[FULL TRAIN] Epoch 199/300 | Train Loss: 1.1769 Acc: 0.7489 | Val Loss: 0.4949 Acc: 0.8722 | LR: 0.001680\n",
            "[FULL TRAIN] Epoch 200/300 | Train Loss: 1.1346 Acc: 0.7530 | Val Loss: 0.5080 Acc: 0.8785 | LR: 0.001653\n",
            "[FULL TRAIN] Epoch 201/300 | Train Loss: 1.1643 Acc: 0.7455 | Val Loss: 0.5087 Acc: 0.8730 | LR: 0.001625\n",
            "[FULL TRAIN] Epoch 202/300 | Train Loss: 1.1010 Acc: 0.7699 | Val Loss: 0.4930 Acc: 0.8825 | LR: 0.001598\n",
            "[FULL TRAIN] Epoch 203/300 | Train Loss: 1.1265 Acc: 0.7538 | Val Loss: 0.5157 Acc: 0.8839 | LR: 0.001570\n",
            "[FULL TRAIN] Epoch 204/300 | Train Loss: 1.1256 Acc: 0.7570 | Val Loss: 0.5149 Acc: 0.8776 | LR: 0.001543\n",
            "[FULL TRAIN] Epoch 205/300 | Train Loss: 1.1553 Acc: 0.7564 | Val Loss: 0.5315 Acc: 0.8804 | LR: 0.001516\n",
            "[FULL TRAIN] Epoch 206/300 | Train Loss: 1.1440 Acc: 0.7504 | Val Loss: 0.4273 Acc: 0.8855 | LR: 0.001489\n",
            "[FULL TRAIN] Epoch 207/300 | Train Loss: 1.1401 Acc: 0.7567 | Val Loss: 0.4863 Acc: 0.8836 | LR: 0.001462\n",
            "[FULL TRAIN] Epoch 208/300 | Train Loss: 1.1777 Acc: 0.7442 | Val Loss: 0.4958 Acc: 0.8832 | LR: 0.001435\n",
            "[FULL TRAIN] Epoch 209/300 | Train Loss: 1.1128 Acc: 0.7569 | Val Loss: 0.4911 Acc: 0.8823 | LR: 0.001409\n",
            "[FULL TRAIN] Epoch 210/300 | Train Loss: 1.1519 Acc: 0.7553 | Val Loss: 0.4777 Acc: 0.8817 | LR: 0.001382\n",
            "[FULL TRAIN] Epoch 211/300 | Train Loss: 1.1174 Acc: 0.7659 | Val Loss: 0.5093 Acc: 0.8810 | LR: 0.001355\n",
            "[FULL TRAIN] Epoch 212/300 | Train Loss: 1.1160 Acc: 0.7599 | Val Loss: 0.4795 Acc: 0.8846 | LR: 0.001329\n",
            "[FULL TRAIN] Epoch 213/300 | Train Loss: 1.1278 Acc: 0.7583 | Val Loss: 0.4997 Acc: 0.8812 | LR: 0.001303\n",
            "[FULL TRAIN] Epoch 214/300 | Train Loss: 1.1272 Acc: 0.7537 | Val Loss: 0.4646 Acc: 0.8877 | LR: 0.001277\n",
            "[FULL TRAIN] Epoch 215/300 | Train Loss: 1.1164 Acc: 0.7621 | Val Loss: 0.5027 Acc: 0.8854 | LR: 0.001251\n",
            "[FULL TRAIN] Epoch 216/300 | Train Loss: 1.1544 Acc: 0.7522 | Val Loss: 0.4634 Acc: 0.8877 | LR: 0.001225\n",
            "[FULL TRAIN] Epoch 217/300 | Train Loss: 1.1036 Acc: 0.7552 | Val Loss: 0.4431 Acc: 0.8872 | LR: 0.001199\n",
            "[FULL TRAIN] Epoch 218/300 | Train Loss: 1.0748 Acc: 0.7674 | Val Loss: 0.4555 Acc: 0.8830 | LR: 0.001174\n",
            "[FULL TRAIN] Epoch 219/300 | Train Loss: 1.0797 Acc: 0.7822 | Val Loss: 0.4478 Acc: 0.8854 | LR: 0.001148\n",
            "[FULL TRAIN] Epoch 220/300 | Train Loss: 1.1024 Acc: 0.7691 | Val Loss: 0.4783 Acc: 0.8839 | LR: 0.001123\n",
            "[FULL TRAIN] Epoch 221/300 | Train Loss: 1.1274 Acc: 0.7637 | Val Loss: 0.4969 Acc: 0.8862 | LR: 0.001098\n",
            "[FULL TRAIN] Epoch 222/300 | Train Loss: 1.0843 Acc: 0.7709 | Val Loss: 0.4084 Acc: 0.8930 | LR: 0.001073\n",
            "[FULL TRAIN] Epoch 223/300 | Train Loss: 1.1120 Acc: 0.7701 | Val Loss: 0.4276 Acc: 0.8893 | LR: 0.001049\n",
            "[FULL TRAIN] Epoch 224/300 | Train Loss: 1.1095 Acc: 0.7641 | Val Loss: 0.4892 Acc: 0.8886 | LR: 0.001024\n",
            "[FULL TRAIN] Epoch 225/300 | Train Loss: 1.1013 Acc: 0.7668 | Val Loss: 0.4835 Acc: 0.8828 | LR: 0.001000\n",
            "[FULL TRAIN] Epoch 226/300 | Train Loss: 1.1166 Acc: 0.7579 | Val Loss: 0.4847 Acc: 0.8926 | LR: 0.000976\n",
            "[FULL TRAIN] Epoch 227/300 | Train Loss: 1.1213 Acc: 0.7704 | Val Loss: 0.4449 Acc: 0.8894 | LR: 0.000952\n",
            "[FULL TRAIN] Epoch 228/300 | Train Loss: 1.1050 Acc: 0.7677 | Val Loss: 0.4709 Acc: 0.8840 | LR: 0.000928\n",
            "[FULL TRAIN] Epoch 229/300 | Train Loss: 1.1049 Acc: 0.7635 | Val Loss: 0.4657 Acc: 0.8844 | LR: 0.000905\n",
            "[FULL TRAIN] Epoch 230/300 | Train Loss: 1.0706 Acc: 0.7731 | Val Loss: 0.5014 Acc: 0.8835 | LR: 0.000882\n",
            "[FULL TRAIN] Epoch 231/300 | Train Loss: 1.1489 Acc: 0.7481 | Val Loss: 0.4933 Acc: 0.8896 | LR: 0.000859\n",
            "[FULL TRAIN] Epoch 232/300 | Train Loss: 1.0868 Acc: 0.7708 | Val Loss: 0.4612 Acc: 0.8927 | LR: 0.000836\n",
            "[FULL TRAIN] Epoch 233/300 | Train Loss: 1.1539 Acc: 0.7548 | Val Loss: 0.5210 Acc: 0.8795 | LR: 0.000813\n",
            "[FULL TRAIN] Epoch 234/300 | Train Loss: 1.1180 Acc: 0.7599 | Val Loss: 0.4811 Acc: 0.8922 | LR: 0.000791\n",
            "[FULL TRAIN] Epoch 235/300 | Train Loss: 1.0900 Acc: 0.7721 | Val Loss: 0.4712 Acc: 0.8862 | LR: 0.000769\n",
            "[FULL TRAIN] Epoch 236/300 | Train Loss: 1.1188 Acc: 0.7572 | Val Loss: 0.4952 Acc: 0.8907 | LR: 0.000747\n",
            "[FULL TRAIN] Epoch 237/300 | Train Loss: 1.1313 Acc: 0.7494 | Val Loss: 0.4960 Acc: 0.8851 | LR: 0.000725\n",
            "[FULL TRAIN] Epoch 238/300 | Train Loss: 1.0891 Acc: 0.7755 | Val Loss: 0.4597 Acc: 0.8901 | LR: 0.000704\n",
            "[FULL TRAIN] Epoch 239/300 | Train Loss: 1.0994 Acc: 0.7671 | Val Loss: 0.4595 Acc: 0.8898 | LR: 0.000683\n",
            "[FULL TRAIN] Epoch 240/300 | Train Loss: 1.0728 Acc: 0.7718 | Val Loss: 0.4385 Acc: 0.8924 | LR: 0.000662\n",
            "[FULL TRAIN] Epoch 241/300 | Train Loss: 1.1321 Acc: 0.7601 | Val Loss: 0.4498 Acc: 0.8920 | LR: 0.000641\n",
            "[FULL TRAIN] Epoch 242/300 | Train Loss: 1.0874 Acc: 0.7724 | Val Loss: 0.4675 Acc: 0.8949 | LR: 0.000621\n",
            "[FULL TRAIN] Epoch 243/300 | Train Loss: 1.0749 Acc: 0.7744 | Val Loss: 0.4643 Acc: 0.8961 | LR: 0.000601\n",
            "[FULL TRAIN] Epoch 244/300 | Train Loss: 1.0818 Acc: 0.7743 | Val Loss: 0.4483 Acc: 0.8928 | LR: 0.000581\n",
            "[FULL TRAIN] Epoch 245/300 | Train Loss: 1.1483 Acc: 0.7585 | Val Loss: 0.4975 Acc: 0.8896 | LR: 0.000561\n",
            "[FULL TRAIN] Epoch 246/300 | Train Loss: 1.1032 Acc: 0.7664 | Val Loss: 0.4589 Acc: 0.8857 | LR: 0.000542\n",
            "[FULL TRAIN] Epoch 247/300 | Train Loss: 1.0896 Acc: 0.7744 | Val Loss: 0.4245 Acc: 0.8943 | LR: 0.000523\n",
            "[FULL TRAIN] Epoch 248/300 | Train Loss: 1.0659 Acc: 0.7840 | Val Loss: 0.4522 Acc: 0.8917 | LR: 0.000504\n",
            "[FULL TRAIN] Epoch 249/300 | Train Loss: 1.0705 Acc: 0.7763 | Val Loss: 0.4861 Acc: 0.8932 | LR: 0.000486\n",
            "[FULL TRAIN] Epoch 250/300 | Train Loss: 1.0662 Acc: 0.7707 | Val Loss: 0.4327 Acc: 0.8945 | LR: 0.000468\n",
            "[FULL TRAIN] Epoch 251/300 | Train Loss: 1.0778 Acc: 0.7772 | Val Loss: 0.5078 Acc: 0.8911 | LR: 0.000450\n",
            "[FULL TRAIN] Epoch 252/300 | Train Loss: 1.0655 Acc: 0.7829 | Val Loss: 0.4342 Acc: 0.8921 | LR: 0.000433\n",
            "[FULL TRAIN] Epoch 253/300 | Train Loss: 1.0912 Acc: 0.7702 | Val Loss: 0.5001 Acc: 0.8930 | LR: 0.000415\n",
            "[FULL TRAIN] Epoch 254/300 | Train Loss: 1.0969 Acc: 0.7668 | Val Loss: 0.4984 Acc: 0.8890 | LR: 0.000398\n",
            "[FULL TRAIN] Epoch 255/300 | Train Loss: 1.1010 Acc: 0.7712 | Val Loss: 0.4688 Acc: 0.9013 | LR: 0.000382\n",
            "[FULL TRAIN] Epoch 256/300 | Train Loss: 1.0551 Acc: 0.7875 | Val Loss: 0.4611 Acc: 0.8943 | LR: 0.000366\n",
            "[FULL TRAIN] Epoch 257/300 | Train Loss: 1.0833 Acc: 0.7815 | Val Loss: 0.4679 Acc: 0.8925 | LR: 0.000350\n",
            "[FULL TRAIN] Epoch 258/300 | Train Loss: 1.1110 Acc: 0.7655 | Val Loss: 0.4656 Acc: 0.8926 | LR: 0.000334\n",
            "[FULL TRAIN] Epoch 259/300 | Train Loss: 1.0977 Acc: 0.7750 | Val Loss: 0.4811 Acc: 0.8894 | LR: 0.000319\n",
            "[FULL TRAIN] Epoch 260/300 | Train Loss: 1.0838 Acc: 0.7732 | Val Loss: 0.4289 Acc: 0.8952 | LR: 0.000304\n",
            "[FULL TRAIN] Epoch 261/300 | Train Loss: 1.0771 Acc: 0.7864 | Val Loss: 0.4575 Acc: 0.8937 | LR: 0.000289\n",
            "[FULL TRAIN] Epoch 262/300 | Train Loss: 1.0452 Acc: 0.7870 | Val Loss: 0.4522 Acc: 0.8966 | LR: 0.000275\n",
            "[FULL TRAIN] Epoch 263/300 | Train Loss: 1.0883 Acc: 0.7820 | Val Loss: 0.4480 Acc: 0.8946 | LR: 0.000261\n",
            "[FULL TRAIN] Epoch 264/300 | Train Loss: 1.0934 Acc: 0.7725 | Val Loss: 0.4605 Acc: 0.8893 | LR: 0.000247\n",
            "[FULL TRAIN] Epoch 265/300 | Train Loss: 1.1095 Acc: 0.7603 | Val Loss: 0.4701 Acc: 0.8941 | LR: 0.000234\n",
            "[FULL TRAIN] Epoch 266/300 | Train Loss: 1.0712 Acc: 0.7812 | Val Loss: 0.4463 Acc: 0.8965 | LR: 0.000221\n",
            "[FULL TRAIN] Epoch 267/300 | Train Loss: 1.0465 Acc: 0.7996 | Val Loss: 0.4333 Acc: 0.8958 | LR: 0.000209\n",
            "[FULL TRAIN] Epoch 268/300 | Train Loss: 1.0765 Acc: 0.7861 | Val Loss: 0.4772 Acc: 0.8948 | LR: 0.000196\n",
            "[FULL TRAIN] Epoch 269/300 | Train Loss: 1.0683 Acc: 0.7862 | Val Loss: 0.4251 Acc: 0.9010 | LR: 0.000184\n",
            "[FULL TRAIN] Epoch 270/300 | Train Loss: 1.0904 Acc: 0.7807 | Val Loss: 0.4571 Acc: 0.8977 | LR: 0.000173\n",
            "[FULL TRAIN] Epoch 271/300 | Train Loss: 1.0837 Acc: 0.7851 | Val Loss: 0.4382 Acc: 0.8973 | LR: 0.000162\n",
            "[FULL TRAIN] Epoch 272/300 | Train Loss: 1.0779 Acc: 0.7738 | Val Loss: 0.4685 Acc: 0.8898 | LR: 0.000151\n",
            "[FULL TRAIN] Epoch 273/300 | Train Loss: 1.0771 Acc: 0.7843 | Val Loss: 0.4725 Acc: 0.8955 | LR: 0.000140\n",
            "[FULL TRAIN] Epoch 274/300 | Train Loss: 1.0891 Acc: 0.7778 | Val Loss: 0.4439 Acc: 0.8961 | LR: 0.000130\n",
            "[FULL TRAIN] Epoch 275/300 | Train Loss: 1.0522 Acc: 0.7912 | Val Loss: 0.4550 Acc: 0.8957 | LR: 0.000121\n",
            "[FULL TRAIN] Early stopping at epoch 275 due to no improvement for 20 epochs after grace period.\n",
            "\n",
            "=== FINAL TEST EVALUATION ===\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): Non-finite logits encountered for a test batch. Logits sum: nan. This batch's accuracy may be affected or skipped.\n",
            "Warning (FINAL TEST): No samples successfully processed in test set for final evaluation.\n",
            "Final training phase completed.\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: ENTRY POINT - FINAL TRAINING\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\"\"\"\n",
        "DEITI — Final Training Phase\n",
        "Loads a saved architecture recipe and trains it from scratch.\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming DEITI class is defined in Cell 1, and Google Drive is mounted via Cell 2.\n",
        "\n",
        "    # Path to the saved architecture recipe (must match the base path used in the search phase).\n",
        "    # The .train_final() method will expect to load a \"_recipe.json\" file based on this.\n",
        "    model_architecture_base_path = '/content/drive/My Drive/deiti_final_architecture' # Note: No .pth or .json here\n",
        "\n",
        "    # For clarity, construct the expected recipe file path for the print statement\n",
        "    expected_recipe_file_path = model_architecture_base_path + \"_recipe.json\"\n",
        "    print(f\"Attempting to load architecture recipe from: {expected_recipe_file_path}\")\n",
        "    print(\"Ensure Google Drive is mounted and the file exists.\")\n",
        "\n",
        "    try:\n",
        "        # Initialize DEITI for the final training phase\n",
        "        final_trainer_instance = DEITI(\n",
        "            batch_size=128,         # Batch size for final training (can be adjusted)\n",
        "            pre_epochs=1,           # Placeholder, not used by train_final's core logic\n",
        "            post_epochs=1,          # Placeholder\n",
        "            batches_per_epoch=None  # Use full epochs for final training\n",
        "        )\n",
        "\n",
        "        # Call the train_final method, passing the base path\n",
        "        custom_final_lr = 8e-4 # For example, 0.00005\n",
        "        print(f\"Calling train_final with custom final_lr: {custom_final_lr}\")\n",
        "\n",
        "        final_trainer_instance.train_final(\n",
        "            epochs=300,\n",
        "            model_path=model_architecture_base_path,\n",
        "            final_lr=custom_final_lr  # <--- Set your desired final learning rate here\n",
        "        )\n",
        "        print(\"Final training phase completed.\")\n",
        "    except FileNotFoundError:\n",
        "        # The FileNotFoundError in train_final will now refer to the _recipe.json file\n",
        "        print(f\"ERROR: Architecture recipe file not found (expected around {expected_recipe_file_path}).\")\n",
        "        print(\"Please ensure the search phase ran successfully and saved the model recipe,\")\n",
        "        print(\"and that your Google Drive is correctly mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the final training phase: {e}\")\n",
        "        # Optionally, re-raise the exception to see the full traceback\n",
        "        raise\n",
        "    # No runtime.unassign() here by default, to allow inspection of final results."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}