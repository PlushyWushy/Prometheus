{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlushyWushy/Prometheus/blob/main/Prometheus_Variation_3_BEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5m-ikijRB5E",
        "outputId": "eb86e310-45bc-4c08-f374-d2bcf1b9c17a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: connect to drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVvNku1ioAcg",
        "outputId": "3a1ef315-39ba-4ed7-f999-14c8f4033ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.7.14)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wfpE9m5oBDS",
        "outputId": "dcff3711-e3c1-4434-cb17-5effa843368d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enabling TensorFloat32 matmul precision for supported GPU.\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.distributions import Categorical\n",
        "from torch.amp import autocast\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import math\n",
        "import inspect # For debugging\n",
        "import logging\n",
        "import gc # Garbage Collector interface\n",
        "import itertools\n",
        "import traceback\n",
        "import os\n",
        "\n",
        "# Silence verbose logs from the compiler\n",
        "logging.getLogger(\"torch._dynamo\").setLevel(logging.FATAL)\n",
        "logging.getLogger(\"torch._inductor\").setLevel(logging.FATAL)\n",
        "\n",
        "try:\n",
        "    import torch_geometric.nn as pyg_nn\n",
        "    from torch_geometric.data import Data\n",
        "    from torch_geometric.utils import to_undirected\n",
        "    from torch_geometric.nn.dense.linear import Linear as PyGLinear\n",
        "    PYG_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYG_AVAILABLE = False\n",
        "    print(\"PyTorch Geometric not found. GNN MetaAgent will not be available if used.\")\n",
        "\n",
        "# =======================================================================\n",
        "#  Constants & Configuration\n",
        "# =======================================================================\n",
        "PRE_EPOCHS=1; BATCHES_PER_EPOCH=None; BATCH_SIZE=128\n",
        "BASE_POST_EPOCHS = 25\n",
        "LEARNING_RATE=0.001; MAX_GRAD_NORM=5.0\n",
        "DEVICE=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if DEVICE.type == 'cuda':\n",
        "    if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
        "        print(\"Enabling TensorFloat32 matmul precision for supported GPU.\")\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# --- Action Space with Skip Connections ---\n",
        "EDIT_TYPE_ADD_CONV_BLOCK=0; EDIT_TYPE_RESIZE_LAYER=1; EDIT_TYPE_ADD_SKIP=2; EDIT_TYPE_ADD_LINEAR_BLOCK=3\n",
        "NUM_TARGET_CNN_EDIT_TYPES=4\n",
        "DISCRETE_CH_MULT_ADD=[0.5,1.0,2.0];\n",
        "DISCRETE_RESIZE_FACTORS = [0.25, 0.5, 0.75, 1.25, 1.5, 1.75]\n",
        "NUM_STAGES_TARGET_CNN=3\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "# --- MetaAgent & RL Configuration ---\n",
        "BASE_META_LR = 5e-4\n",
        "MIN_META_LR = 1e-5\n",
        "LR_ACC_BASE_THRESHOLD = 0.80\n",
        "LR_ACC_TARGET_THRESHOLD = 0.93\n",
        "\n",
        "# --- Actions for complexity reduction ---\n",
        "META_EDIT_NONE = 0\n",
        "META_EDIT_DEEPEN_GNN = 1\n",
        "META_EDIT_WIDEN_GNN_HIDDEN = 2\n",
        "META_EDIT_DEEPEN_MLP_HEAD = 3\n",
        "META_EDIT_SHRINK_GNN_HIDDEN = 4\n",
        "META_EDIT_PRUNE_GNN = 5\n",
        "META_EDIT_PRUNE_MLP_HEAD = 6\n",
        "NUM_META_SELF_EDIT_TYPES = 7\n",
        "META_SELF_EDIT_INTERVAL = 5\n",
        "\n",
        "# --- Factors and min/max constraints for two-way search ---\n",
        "META_GNN_WIDEN_FACTOR = 1.5\n",
        "META_GNN_SHRINK_FACTOR = 1.0 / META_GNN_WIDEN_FACTOR # Symmetric shrinking\n",
        "MIN_GNN_LAYERS = 1\n",
        "MIN_GNN_HIDDEN_DIM = 16\n",
        "MIN_MLP_HEAD_SEQUENTIAL_DEPTH = 1\n",
        "INITIAL_GNN_HIDDEN_DIM = 32; INITIAL_NUM_GNN_LAYERS = 2\n",
        "EDIT_TYPE_EMBED_DIM = 16\n",
        "META_AGENT_PRUNE_THRESHOLD = 15000\n",
        "\n",
        "# --- Complexity Penalty Configuration ---\n",
        "COMPLEXITY_LIMIT = 30_000_000  # Hard parameter limit\n",
        "COMPLEXITY_BREACH_PENALTY = 50.0 # Fixed penalty for breaching the hard limit\n",
        "COMPLEXITY_PENALTY_THRESHOLD = 20_000_000 # Soft threshold for scaled penalty\n",
        "COMPLEXITY_PENALTY_ALPHA = 0.2\n",
        "\n",
        "# --- Graph & State Representation ---\n",
        "OP_TYPE_IDS = {'conv2d':1,'relu':2,'maxpool2d':3, 'batchnorm2d': 4, 'batchnorm1d': 7, 'add': 8, 'input_placeholder':5, 'linear': 6, 'dropout': 9, 'none':0}\n",
        "NODE_FEATURE_DIM = 7\n",
        "NORMALIZATION_WIDTH_DIVISOR = 512.0; NORMALIZATION_IDX_DIVISOR = 50.0; NORMALIZATION_SPATIAL_DIVISOR = 32.0\n",
        "GLOBAL_SUMMARY_FEATURE_DIM = 3 + 2\n",
        "MAX_GLOBAL_HISTORY_LEN = 5\n",
        "\n",
        "# =======================================================================\n",
        "#  Custom Modules for Dynamic Graph\n",
        "# =======================================================================\n",
        "class AddWithProjection(nn.Module):\n",
        "    def __init__(self, projection_module=None):\n",
        "        super().__init__()\n",
        "        self.projection = projection_module if projection_module is not None else nn.Identity()\n",
        "    def forward(self, x_primary, x_skip):\n",
        "        x_skip_projected = self.projection(x_skip)\n",
        "        if x_primary.shape[2:] != x_skip_projected.shape[2:]:\n",
        "            x_skip_projected = F.interpolate(x_skip_projected, size=x_primary.shape[2:], mode='bilinear', align_corners=False)\n",
        "        return x_primary + x_skip_projected\n",
        "\n",
        "# =======================================================================\n",
        "#  Net2Net Utilities & Masking Helpers\n",
        "# =======================================================================\n",
        "def _valid_resize_indices(old_oc: int):\n",
        "    valid = []\n",
        "    for idx, f_resize in enumerate(DISCRETE_RESIZE_FACTORS):\n",
        "        new_oc_resize = max(1, int(round(old_oc * f_resize)))\n",
        "        if new_oc_resize != old_oc: valid.append(idx)\n",
        "    if not valid and old_oc > 0 :\n",
        "        try: valid.append(DISCRETE_RESIZE_FACTORS.index(1.0))\n",
        "        except ValueError: pass\n",
        "    if not valid: valid.append(0)\n",
        "    return valid\n",
        "def _mask_logits(logits: torch.Tensor, valid_indices: list):\n",
        "    mask_val = torch.full_like(logits, float('-inf'))\n",
        "    if valid_indices:\n",
        "        valid_indices_tensor = torch.tensor(valid_indices, device=logits.device, dtype=torch.long)\n",
        "        if valid_indices_tensor.numel() > 0:\n",
        "            valid_indices_tensor = valid_indices_tensor[valid_indices_tensor < logits.shape[-1]]\n",
        "            if valid_indices_tensor.numel() > 0:\n",
        "                 mask_val[..., valid_indices_tensor] = 0.0\n",
        "    return logits + mask_val\n",
        "\n",
        "def _invalid_meta_indices(agent):\n",
        "    invalid = []\n",
        "    agent_params = sum(p.numel() for p in agent.parameters())\n",
        "    if agent_params < META_AGENT_PRUNE_THRESHOLD:\n",
        "        invalid.extend([META_EDIT_SHRINK_GNN_HIDDEN, META_EDIT_PRUNE_GNN, META_EDIT_PRUNE_MLP_HEAD])\n",
        "\n",
        "    if agent.current_num_gnn_layers <= MIN_GNN_LAYERS:\n",
        "        invalid.append(META_EDIT_PRUNE_GNN)\n",
        "    if agent.current_gnn_hidden_dim <= MIN_GNN_HIDDEN_DIM:\n",
        "        invalid.append(META_EDIT_SHRINK_GNN_HIDDEN)\n",
        "\n",
        "    eligible_heads = agent.get_mlp_head_names()\n",
        "    all_heads_at_min_depth = all(agent.head_depth_counters.get(name, 1) <= MIN_MLP_HEAD_SEQUENTIAL_DEPTH for name in eligible_heads)\n",
        "    if all_heads_at_min_depth:\n",
        "        invalid.append(META_EDIT_PRUNE_MLP_HEAD)\n",
        "\n",
        "    all_heads_at_max_depth = all(agent.head_depth_counters.get(name, 1) >= 8 for name in eligible_heads)\n",
        "    if all_heads_at_max_depth:\n",
        "        invalid.append(META_EDIT_DEEPEN_MLP_HEAD)\n",
        "\n",
        "    return list(set(invalid))\n",
        "\n",
        "def _resize_linear_layer(old_linear, new_in_features, new_out_features, device='cpu'):\n",
        "    is_pyg_linear = PYG_AVAILABLE and isinstance(old_linear, PyGLinear)\n",
        "    old_in_features = old_linear.in_channels if is_pyg_linear else old_linear.in_features\n",
        "    old_out_features = old_linear.out_channels if is_pyg_linear else old_linear.out_features\n",
        "\n",
        "    if old_in_features == new_in_features and old_out_features == new_out_features:\n",
        "        return old_linear\n",
        "\n",
        "    new_linear_class = PyGLinear if is_pyg_linear else nn.Linear\n",
        "    new_linear = new_linear_class(new_in_features, new_out_features, bias=(old_linear.bias is not None)).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        min_in = min(old_in_features, new_in_features)\n",
        "        min_out = min(old_out_features, new_out_features)\n",
        "        new_linear.weight.data[:min_out, :min_in] = old_linear.weight.data[:min_out, :min_in].clone()\n",
        "\n",
        "        if new_out_features > old_out_features and old_out_features > 0:\n",
        "            for i in range(old_out_features, new_out_features):\n",
        "                new_linear.weight.data[i, :min_in] = old_linear.weight.data[i % old_out_features, :min_in].clone()\n",
        "\n",
        "        if new_in_features > old_in_features and old_in_features > 0:\n",
        "            scaling_factor = math.sqrt(new_in_features / old_in_features)\n",
        "            for i in range(old_in_features, new_in_features):\n",
        "                new_linear.weight.data[:min_out, i] = old_linear.weight.data[:min_out, i % old_in_features].clone() / scaling_factor\n",
        "\n",
        "        if old_linear.bias is not None and new_linear.bias is not None:\n",
        "            new_linear.bias.data[:min_out] = old_linear.bias.data[:min_out].clone()\n",
        "            if new_out_features > old_out_features and old_out_features > 0:\n",
        "                for i in range(old_out_features, new_out_features):\n",
        "                    new_linear.bias.data[i] = old_linear.bias.data[i % old_out_features].clone()\n",
        "    return new_linear\n",
        "\n",
        "def net2wider_conv_output(conv: nn.Conv2d, factor: float, device='cpu') -> nn.Conv2d:\n",
        "    old_oc = conv.out_channels; new_oc = max(1, int(round(old_oc * factor)))\n",
        "    if new_oc == old_oc : return conv\n",
        "    new_conv_module = nn.Conv2d(conv.in_channels, new_oc, conv.kernel_size, stride=conv.stride, padding=conv.padding, groups=conv.groups, bias=(conv.bias is not None)).to(device)\n",
        "    with torch.no_grad():\n",
        "        new_conv_module.weight.data.fill_(0); min_oc = min(old_oc, new_oc)\n",
        "        if old_oc > 0:\n",
        "            new_conv_module.weight.data[:min_oc] = conv.weight.data[:min_oc].clone()\n",
        "            if new_oc > old_oc:\n",
        "                r_widen = float(new_oc) / old_oc\n",
        "                for i in range(old_oc, new_oc):\n",
        "                    new_conv_module.weight.data[i] = conv.weight.data[i % old_oc].clone() / math.sqrt(r_widen)\n",
        "        if conv.bias is not None and new_conv_module.bias is not None:\n",
        "            new_conv_module.bias.data.fill_(0)\n",
        "            if old_oc > 0:\n",
        "                new_conv_module.bias.data[:min_oc] = conv.bias.data[:min_oc].clone()\n",
        "                if new_oc > old_oc:\n",
        "                    for i in range(old_oc, new_oc): new_conv_module.bias.data[i] = conv.bias.data[i % old_oc].clone()\n",
        "            elif new_oc > 0: nn.init.zeros_(new_conv_module.bias.data)\n",
        "        elif new_conv_module.bias is not None: nn.init.zeros_(new_conv_module.bias.data)\n",
        "    return new_conv_module\n",
        "def net2thinner_conv_output(conv: nn.Conv2d, factor: float, device='cpu') -> nn.Conv2d:\n",
        "    old_oc = conv.out_channels; new_oc = max(1, int(round(old_oc * factor)))\n",
        "    if new_oc == old_oc: return conv\n",
        "    new_conv_module = nn.Conv2d(conv.in_channels, new_oc, conv.kernel_size, stride=conv.stride, padding=conv.padding, groups=conv.groups, bias=(conv.bias is not None)).to(device)\n",
        "    with torch.no_grad():\n",
        "        if old_oc > 0 :\n",
        "            new_conv_module.weight.data = conv.weight.data[:new_oc].clone()\n",
        "            if conv.bias is not None and new_conv_module.bias is not None:\n",
        "                 new_conv_module.bias.data = conv.bias.data[:new_oc].clone()\n",
        "            elif new_conv_module.bias is not None: nn.init.zeros_(new_conv_module.bias.data)\n",
        "    return new_conv_module\n",
        "def resize_conv_output(conv: nn.Conv2d, factor: float, device='cpu') -> nn.Conv2d:\n",
        "    if abs(factor - 1.0) < 1e-6 : return conv\n",
        "    old_oc = conv.out_channels; new_oc = max(1, int(round(old_oc * factor)))\n",
        "    if new_oc == old_oc and old_oc > 0: return conv\n",
        "    if new_oc == old_oc and old_oc == 0 and factor != 1.0 : pass\n",
        "    elif new_oc == old_oc: return conv\n",
        "    if new_oc > old_oc: return net2wider_conv_output(conv, float(new_oc)/old_oc if old_oc > 0 else factor, device)\n",
        "    else: return net2thinner_conv_output(conv, float(new_oc)/old_oc if old_oc > 0 else factor, device)\n",
        "def resize_linear_output(linear: nn.Linear, factor: float, device='cpu') -> nn.Linear:\n",
        "    if abs(factor - 1.0) < 1e-6: return linear\n",
        "    old_of = linear.out_features; new_of = max(1, int(round(old_of * factor)))\n",
        "    if new_of == old_of: return linear\n",
        "    return _resize_linear_layer(linear, linear.in_features, new_of, device)\n",
        "\n",
        "def adapt_conv_input_channels(conv: nn.Conv2d, new_in_channels: int, device='cpu') -> nn.Conv2d:\n",
        "    if conv.in_channels == new_in_channels: return conv\n",
        "    new_in_channels = max(1, new_in_channels); old_ic = conv.in_channels\n",
        "    if conv.groups > 1:\n",
        "        if new_in_channels % conv.groups != 0:\n",
        "            print(f\"  INVALID ADAPTATION: Cannot adapt grouped Conv2d to new_in_channels={new_in_channels} with groups={conv.groups}. The edit is invalid.\")\n",
        "            return None\n",
        "    new_conv_module = nn.Conv2d(new_in_channels, conv.out_channels, conv.kernel_size, stride=conv.stride, padding=conv.padding, groups=conv.groups, bias=(conv.bias is not None)).to(device)\n",
        "    with torch.no_grad():\n",
        "        if old_ic == 0: nn.init.kaiming_normal_(new_conv_module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        else:\n",
        "            w_new_conv_adapt = torch.zeros_like(new_conv_module.weight.data)\n",
        "            oc_per_group = conv.out_channels // conv.groups; new_ic_per_group = new_in_channels // conv.groups; old_ic_per_group = old_ic // conv.groups\n",
        "            for g in range(conv.groups):\n",
        "                in_start_new, in_end_new = g * new_ic_per_group, (g + 1) * new_ic_per_group\n",
        "                in_start_old, in_end_old = g * old_ic_per_group, (g + 1) * old_ic_per_group\n",
        "                out_start, out_end = g * oc_per_group, (g + 1) * oc_per_group\n",
        "                for o_idx in range(out_start, out_end):\n",
        "                    for i_idx in range(in_start_new, in_end_new):\n",
        "                        if i_idx < in_end_old: w_new_conv_adapt[o_idx, i_idx].copy_(conv.weight.data[o_idx, i_idx])\n",
        "                        else:\n",
        "                            orig_i_idx = in_start_old + (i_idx - in_start_new) % old_ic_per_group\n",
        "                            w_new_conv_adapt[o_idx, i_idx].copy_(conv.weight.data[o_idx, orig_i_idx])\n",
        "                            w_new_conv_adapt[o_idx, i_idx] /= max(1.0, new_ic_per_group / old_ic_per_group)\n",
        "            new_conv_module.weight.data.copy_(w_new_conv_adapt)\n",
        "        if conv.bias is not None and new_conv_module.bias is not None: new_conv_module.bias.data.copy_(conv.bias.data)\n",
        "        elif new_conv_module.bias is not None: nn.init.zeros_(new_conv_module.bias.data)\n",
        "    return new_conv_module\n",
        "def adapt_linear_input_features(linear: nn.Linear, new_in_features: int, device='cpu') -> nn.Linear:\n",
        "    if linear.in_features == new_in_features: return linear\n",
        "    return _resize_linear_layer(linear, new_in_features, linear.out_features, device)\n",
        "\n",
        "def adapt_batchnorm_features(bn: nn.Module, new_num_features: int, device='cpu') -> nn.Module:\n",
        "    if bn.num_features == new_num_features: return bn\n",
        "    new_bn = type(bn)(new_num_features).to(device)\n",
        "    with torch.no_grad():\n",
        "        min_feat = min(bn.num_features, new_num_features)\n",
        "        if bn.weight is not None:\n",
        "            new_bn.weight.data[:min_feat] = bn.weight.data[:min_feat].clone()\n",
        "            if new_num_features > bn.num_features:\n",
        "                new_bn.weight.data[bn.num_features:] = bn.weight.data[-1].clone()\n",
        "        if bn.bias is not None:\n",
        "            new_bn.bias.data[:min_feat] = bn.bias.data[:min_feat].clone()\n",
        "            if new_num_features > bn.num_features:\n",
        "                new_bn.bias.data[bn.num_features:] = bn.bias.data[-1].clone()\n",
        "        if bn.running_mean is not None:\n",
        "            new_bn.running_mean[:min_feat] = bn.running_mean[:min_feat].clone()\n",
        "            if new_num_features > bn.num_features:\n",
        "                new_bn.running_mean[bn.num_features:] = bn.running_mean[-1].clone()\n",
        "        if bn.running_var is not None:\n",
        "            new_bn.running_var[:min_feat] = bn.running_var[:min_feat].clone()\n",
        "            if new_num_features > bn.num_features:\n",
        "                new_bn.running_var[bn.num_features:] = bn.running_var[-1].clone()\n",
        "    return new_bn\n",
        "def net2deeper_linear_insert_identity(head_module_owner: nn.Module, head_name: str, device='cpu'):\n",
        "    if not hasattr(head_module_owner, head_name): print(f\"Err: Attr {head_name} not found for deepening\"); return False\n",
        "    original_component = getattr(head_module_owner, head_name)\n",
        "    if isinstance(original_component, nn.Linear):\n",
        "        identity_dim = original_component.out_features\n",
        "        if identity_dim <= 0: print(f\"Cannot insert identity for dim {identity_dim} in Linear head {head_name}\"); return False\n",
        "        identity_layer = nn.Linear(identity_dim, identity_dim, bias=True).to(device)\n",
        "        with torch.no_grad(): identity_layer.weight.data.copy_(torch.eye(identity_dim,device=device)); identity_layer.bias.data.fill_(0)\n",
        "        setattr(head_module_owner,head_name,nn.Sequential(original_component,identity_layer).to(device)); print(f\"  Deepened MLP head '{head_name}' (Linear -> Sequential)\")\n",
        "        return True\n",
        "    elif isinstance(original_component, nn.Sequential):\n",
        "        if not original_component or not isinstance(original_component[-1],nn.Linear): print(f\"Cannot deepen Seq head '{head_name}', last not Linear.\"); return False\n",
        "        identity_dim = original_component[-1].in_features\n",
        "        if identity_dim <= 0: print(f\"Cannot insert identity for dim {identity_dim} in Seq head {head_name}\"); return False\n",
        "        identity_layer = nn.Linear(identity_dim, identity_dim, bias=True).to(device)\n",
        "        with torch.no_grad(): identity_layer.weight.data.copy_(torch.eye(identity_dim,device=device)); identity_layer.bias.data.fill_(0)\n",
        "        new_seq_layers = nn.ModuleList([l for l in original_component[:-1]] + [identity_layer, original_component[-1]])\n",
        "        setattr(head_module_owner,head_name,nn.Sequential(*new_seq_layers).to(device)); print(f\"  Deepened Seq head '{head_name}'.\")\n",
        "        return True\n",
        "    print(f\"Err: Head '{head_name}' type {type(original_component)} not Linear/Seq for deepening.\");\n",
        "    return False\n",
        "\n",
        "def net2thinner_linear_remove_layer(head_module_owner: nn.Module, head_name: str, device='cpu'):\n",
        "    if not hasattr(head_module_owner, head_name): print(f\"Err: Attr {head_name} not found for pruning\"); return False\n",
        "    original_component = getattr(head_module_owner, head_name)\n",
        "    if not isinstance(original_component, nn.Sequential) or len(original_component) <= 2:\n",
        "        print(f\"Cannot prune head '{head_name}': not a Sequential module with more than 2 layers.\"); return False\n",
        "\n",
        "    pruned_layers = nn.ModuleList([l for l in original_component[:-2]] + [original_component[-1]])\n",
        "\n",
        "    if len(pruned_layers) == 1:\n",
        "        setattr(head_module_owner, head_name, pruned_layers[0].to(device))\n",
        "        print(f\"  Pruned MLP head '{head_name}' (Sequential -> Linear)\")\n",
        "    else:\n",
        "        setattr(head_module_owner, head_name, nn.Sequential(*pruned_layers).to(device))\n",
        "        print(f\"  Pruned MLP head '{head_name}'.\")\n",
        "    return True\n",
        "\n",
        "def _get_gcn_conv_linear_submodule(gcn_layer):\n",
        "    if hasattr(gcn_layer, 'lin') and (isinstance(gcn_layer.lin, nn.Linear) or (PYG_AVAILABLE and isinstance(gcn_layer.lin, PyGLinear))):\n",
        "        return gcn_layer.lin\n",
        "    return None\n",
        "if PYG_AVAILABLE:\n",
        "    def resize_gcn_conv_hidden(gcn_layer: pyg_nn.GCNConv, new_hidden_dim: int, prev_layer_out_dim: int, device='cpu'):\n",
        "        old_hidden_dim = gcn_layer.out_channels\n",
        "        if new_hidden_dim == old_hidden_dim: return gcn_layer, False\n",
        "        new_gcn = pyg_nn.GCNConv(prev_layer_out_dim,new_hidden_dim,bias=(gcn_layer.bias is not None), improved=gcn_layer.improved,add_self_loops=gcn_layer.add_self_loops, normalize=gcn_layer.normalize).to(device)\n",
        "        with torch.no_grad():\n",
        "            gcn_lin_original = _get_gcn_conv_linear_submodule(gcn_layer)\n",
        "            if gcn_lin_original is not None:\n",
        "                new_gcn.lin = _resize_linear_layer(gcn_lin_original, prev_layer_out_dim, new_hidden_dim, device)\n",
        "\n",
        "            if gcn_layer.bias is not None and new_gcn.bias is not None:\n",
        "                min_out = min(old_hidden_dim, new_hidden_dim)\n",
        "                new_gcn.bias.data[:min_out] = gcn_layer.bias.data[:min_out].clone()\n",
        "                if new_hidden_dim > old_hidden_dim and old_hidden_dim > 0:\n",
        "                    for i in range(old_hidden_dim, new_hidden_dim):\n",
        "                        new_gcn.bias.data[i] = gcn_layer.bias.data[i % old_hidden_dim].clone()\n",
        "        return new_gcn,True\n",
        "\n",
        "    def adapt_gcn_conv_input_dim(gcn_layer: pyg_nn.GCNConv, new_input_dim: int, device='cpu'):\n",
        "        old_input_dim = gcn_layer.in_channels\n",
        "        if new_input_dim == old_input_dim: return gcn_layer, False\n",
        "        new_gcn = pyg_nn.GCNConv(new_input_dim,gcn_layer.out_channels,bias=(gcn_layer.bias is not None), improved=gcn_layer.improved,add_self_loops=gcn_layer.add_self_loops,normalize=gcn_layer.normalize).to(device)\n",
        "        with torch.no_grad():\n",
        "            gcn_lin_original_adapt = _get_gcn_conv_linear_submodule(gcn_layer)\n",
        "            if gcn_lin_original_adapt is not None:\n",
        "                new_gcn.lin = _resize_linear_layer(gcn_lin_original_adapt, new_input_dim, gcn_layer.out_channels, device)\n",
        "\n",
        "            if gcn_layer.bias is not None and new_gcn.bias is not None:\n",
        "                new_gcn.bias.data.copy_(gcn_layer.bias.data)\n",
        "        return new_gcn, True\n",
        "\n",
        "    def create_identity_gcn_layer(dim: int, device='cpu', **gcn_kwargs):\n",
        "        identity_gcn = pyg_nn.GCNConv(dim,dim,bias=gcn_kwargs.get('bias',True), normalize=gcn_kwargs.get('normalize',True), add_self_loops=gcn_kwargs.get('add_self_loops',True), improved=gcn_kwargs.get('improved',False)).to(device)\n",
        "        with torch.no_grad():\n",
        "            gcn_lin_identity = _get_gcn_conv_linear_submodule(identity_gcn)\n",
        "            if gcn_lin_identity is not None:\n",
        "                if gcn_lin_identity.weight.shape[0] == gcn_lin_identity.weight.shape[1]:\n",
        "                    gcn_lin_identity.weight.data.copy_(torch.eye(dim,device=device))\n",
        "                else:\n",
        "                    nn.init.kaiming_uniform_(gcn_lin_identity.weight, a=math.sqrt(5))\n",
        "                if hasattr(gcn_lin_identity, 'bias') and gcn_lin_identity.bias is not None:\n",
        "                    gcn_lin_identity.bias.data.fill_(0.0)\n",
        "            if identity_gcn.bias is not None:\n",
        "                identity_gcn.bias.data.fill_(0.0)\n",
        "        return identity_gcn\n",
        "\n",
        "# =======================================================================\n",
        "#  Dynamic Models (TargetCNN)\n",
        "# =======================================================================\n",
        "class DynamicStageModule(nn.Module):\n",
        "    def __init__(self, stage_idx_dyn, initial_in_channels_dyn, initial_spatial_size_dyn, max_ops_dyn=None):\n",
        "        super().__init__()\n",
        "        self.stage_idx = stage_idx_dyn\n",
        "        self.initial_in_channels = initial_in_channels_dyn\n",
        "        self.initial_spatial_size = initial_spatial_size_dyn\n",
        "        self.max_ops = max_ops_dyn\n",
        "        self.ops = nn.ModuleList()\n",
        "        self.op_descriptions = []\n",
        "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "\n",
        "    def add_op(self, op_module_dyn, op_description_dyn, insert_at=None):\n",
        "        if self.max_ops is not None and len(self.ops) >= self.max_ops:\n",
        "            return False\n",
        "        if insert_at is None:\n",
        "            self.ops.append(op_module_dyn)\n",
        "            self.op_descriptions.append(op_description_dyn)\n",
        "        else:\n",
        "            self.ops.insert(insert_at, op_module_dyn)\n",
        "            self.op_descriptions.insert(insert_at, op_description_dyn)\n",
        "        return True\n",
        "\n",
        "    def get_op_output_properties(self, op_idx):\n",
        "        if op_idx == -1:\n",
        "            return self.initial_in_channels, self.initial_spatial_size\n",
        "        if 0 <= op_idx < len(self.op_descriptions):\n",
        "            desc = self.op_descriptions[op_idx]\n",
        "            return desc.get('out_channels', 0), desc.get('out_spatial', 0)\n",
        "        raise IndexError(f\"Operator index {op_idx} out of range for stage {self.stage_idx} with {len(self.ops)} ops.\")\n",
        "\n",
        "    def get_current_out_properties(self):\n",
        "        if not self.op_descriptions:\n",
        "            return self.initial_in_channels, self.initial_spatial_size\n",
        "        return self.get_op_output_properties(len(self.ops) - 1)\n",
        "\n",
        "    def forward(self, x_dyn):\n",
        "        outputs_history_dyn = {-1: x_dyn}\n",
        "        is_first_conv_in_model = (self.stage_idx == 0)\n",
        "\n",
        "        for i_dyn, op_desc_item_dyn in enumerate(self.op_descriptions):\n",
        "            op_module_fwd_dyn = self.ops[i_dyn]\n",
        "            input_indices_dyn = op_desc_item_dyn.get('input_indices', [-1])\n",
        "            if not isinstance(input_indices_dyn, list): input_indices_dyn = [input_indices_dyn]\n",
        "\n",
        "            current_op_inputs_dyn = []\n",
        "            for source_op_local_idx_dyn in input_indices_dyn:\n",
        "                if not (-1 <= source_op_local_idx_dyn < i_dyn):\n",
        "                     source_op_local_idx_dyn = (i_dyn - 1) if i_dyn > 0 else -1\n",
        "\n",
        "                if source_op_local_idx_dyn in outputs_history_dyn:\n",
        "                    current_op_inputs_dyn.append(outputs_history_dyn[source_op_local_idx_dyn])\n",
        "                else:\n",
        "                    default_input_key_dyn = (i_dyn-1) if i_dyn > 0 else -1\n",
        "                    current_op_inputs_dyn.append(outputs_history_dyn.get(default_input_key_dyn, x_dyn))\n",
        "            try:\n",
        "                if not current_op_inputs_dyn: op_output_dyn = op_module_fwd_dyn(x_dyn)\n",
        "                elif len(current_op_inputs_dyn) == 1: op_output_dyn = op_module_fwd_dyn(current_op_inputs_dyn[0])\n",
        "                else: op_output_dyn = op_module_fwd_dyn(*current_op_inputs_dyn)\n",
        "            except Exception as e_dyn_fwd:\n",
        "                print(f\"CRITICAL Error in DynamicStageModule op {i_dyn}, type {op_desc_item_dyn.get('type','Unknown')}, stage {self.stage_idx}: {e_dyn_fwd}\"); raise e_dyn_fwd\n",
        "\n",
        "            if self.training and False:\n",
        "                if i_dyn > 1 and 'batchnorm' in self.op_descriptions[i_dyn-1]['type'] and 'conv2d' in self.op_descriptions[i_dyn-2]['type']:\n",
        "                    if not is_first_conv_in_model:\n",
        "                        op_output_dyn = self.dropout(op_output_dyn)\n",
        "                    is_first_conv_in_model = False\n",
        "\n",
        "            outputs_history_dyn[i_dyn] = op_output_dyn\n",
        "        return outputs_history_dyn[len(self.ops)-1] if self.ops else x_dyn\n",
        "\n",
        "class TargetCNN(nn.Module):\n",
        "    def __init__(self, num_classes_cnn=10, num_stages_cnn=NUM_STAGES_TARGET_CNN, init_model_ch_cnn=64, input_spatial_size=32):\n",
        "        super().__init__()\n",
        "        self.num_stages = num_stages_cnn\n",
        "        self.stages = nn.ModuleList()\n",
        "        current_channels_cnn = 3\n",
        "        current_spatial_size = input_spatial_size\n",
        "        self.input_placeholder_desc = {'type': 'input_placeholder', 'out_channels': current_channels_cnn, 'out_spatial': input_spatial_size, 'input_indices': []}\n",
        "        stage_base_channels_cnn = [init_model_ch_cnn, init_model_ch_cnn * 2, init_model_ch_cnn * 4]\n",
        "        for i_cnn_stage in range(num_stages_cnn):\n",
        "            stage_module_cnn = DynamicStageModule(i_cnn_stage, current_channels_cnn, current_spatial_size, max_ops_dyn=None)\n",
        "            target_stage_out_channels_cnn = stage_base_channels_cnn[i_cnn_stage] if i_cnn_stage < len(stage_base_channels_cnn) else stage_base_channels_cnn[-1]\n",
        "            conv1 = nn.Conv2d(current_channels_cnn, target_stage_out_channels_cnn, 3, 1, 1, bias=False).to(DEVICE)\n",
        "            bn1 = nn.BatchNorm2d(target_stage_out_channels_cnn).to(DEVICE)\n",
        "            relu1 = nn.ReLU(inplace=False).to(DEVICE)\n",
        "            stage_module_cnn.add_op(conv1, {'type': 'conv2d', 'out_channels': target_stage_out_channels_cnn, 'out_spatial': current_spatial_size, 'input_indices': [-1]})\n",
        "            stage_module_cnn.add_op(bn1, {'type': 'batchnorm2d', 'out_channels': target_stage_out_channels_cnn, 'out_spatial': current_spatial_size, 'input_indices': [0]})\n",
        "            stage_module_cnn.add_op(relu1, {'type': 'relu', 'out_channels': target_stage_out_channels_cnn, 'out_spatial': current_spatial_size, 'input_indices': [1]})\n",
        "            current_channels_after_block_cnn, current_spatial_after_block = stage_module_cnn.get_current_out_properties()\n",
        "            if i_cnn_stage < num_stages_cnn - 1 :\n",
        "                pool_cnn = nn.MaxPool2d(2, 2).to(DEVICE)\n",
        "                current_spatial_size //= 2\n",
        "                stage_module_cnn.add_op(pool_cnn, {'type': 'maxpool2d', 'out_channels': current_channels_after_block_cnn, 'out_spatial': current_spatial_size, 'input_indices': [2]})\n",
        "            self.stages.append(stage_module_cnn)\n",
        "            current_channels_cnn, current_spatial_size = stage_module_cnn.get_current_out_properties()\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        fc_in_features_cnn, _ = self.get_last_stage_out_properties()\n",
        "        self.classifier = nn.ModuleList([nn.Linear(max(1, fc_in_features_cnn), num_classes_cnn).to(DEVICE)])\n",
        "        self.classifier_op_descriptions = [{'type': 'linear', 'out_features': num_classes_cnn, 'input_indices': [-1]}]\n",
        "    def forward(self,x_cnn_fwd):\n",
        "        for stage_cnn_fwd in self.stages: x_cnn_fwd=stage_cnn_fwd(x_cnn_fwd)\n",
        "        x_cnn_fwd = self.flatten(self.adaptive_pool(x_cnn_fwd))\n",
        "        for op_module in self.classifier:\n",
        "            x_cnn_fwd = op_module(x_cnn_fwd)\n",
        "        return x_cnn_fwd\n",
        "    def get_last_stage_out_properties(self):\n",
        "        if not self.stages:\n",
        "            return self.input_placeholder_desc['out_channels'], self.input_placeholder_desc['out_spatial']\n",
        "        return self.stages[-1].get_current_out_properties()\n",
        "    def get_classifier_in_features(self):\n",
        "        if not self.classifier: return 0\n",
        "        return self.classifier[0].in_features\n",
        "    def get_classifier_out_features(self):\n",
        "        if not self.classifier: return 0\n",
        "        return self.classifier[-1].out_features\n",
        "\n",
        "# =======================================================================\n",
        "#  GNN MetaAgent\n",
        "# =======================================================================\n",
        "if PYG_AVAILABLE:\n",
        "    class MetaAgentGNN(nn.Module):\n",
        "        def __init__(self, node_feature_dim_agent=NODE_FEATURE_DIM, initial_gnn_hidden_dim_agent=INITIAL_GNN_HIDDEN_DIM, initial_num_gnn_layers_agent=INITIAL_NUM_GNN_LAYERS,\n",
        "                     global_history_dim_flat_agent=GLOBAL_SUMMARY_FEATURE_DIM * MAX_GLOBAL_HISTORY_LEN, device_to_use_agent=DEVICE, **kwargs_agent):\n",
        "            super().__init__()\n",
        "            self.device = device_to_use_agent\n",
        "            self.node_feature_dim = node_feature_dim_agent\n",
        "            self.current_gnn_hidden_dim = initial_gnn_hidden_dim_agent\n",
        "            self.current_num_gnn_layers = initial_num_gnn_layers_agent\n",
        "            self.action_space_sizes = {k: v for k, v in kwargs_agent.items()}\n",
        "            self.head_depth_counters = {}\n",
        "            self.edit_type_embedding = nn.Embedding(self.action_space_sizes['num_edit_types'], EDIT_TYPE_EMBED_DIM).to(self.device)\n",
        "            self.policy_head_names = ['head_target_loc_stage', 'head_target_conv_ch_mult', 'head_target_resize_factor']\n",
        "            self.heads_input_dim_global_current = -1\n",
        "            self.gnn_layers = nn.ModuleList()\n",
        "            self._build_gnn_layers()\n",
        "            self._update_mlp_heads()\n",
        "\n",
        "        def _get_current_gnn_output_dim(self):\n",
        "            if self.current_num_gnn_layers == 0:\n",
        "                return self.node_feature_dim\n",
        "            return self.current_gnn_hidden_dim\n",
        "\n",
        "        def _adapt_head_input(self, head, new_input_dim):\n",
        "            if isinstance(head, (nn.Linear, PyGLinear)):\n",
        "                is_pyg = isinstance(head, PyGLinear)\n",
        "                old_in = head.in_channels if is_pyg else head.in_features\n",
        "                old_out = head.out_channels if is_pyg else head.out_features\n",
        "                if old_in != new_input_dim:\n",
        "                    return _resize_linear_layer(head, new_input_dim, old_out, self.device)\n",
        "            elif isinstance(head, nn.Sequential):\n",
        "                first_linear = head[0]\n",
        "                if isinstance(first_linear, (nn.Linear, PyGLinear)):\n",
        "                    is_pyg = isinstance(first_linear, PyGLinear)\n",
        "                    old_in = first_linear.in_channels if is_pyg else first_linear.in_features\n",
        "                    old_out = first_linear.out_channels if is_pyg else first_linear.out_features\n",
        "                    if old_in != new_input_dim:\n",
        "                        head[0] = _resize_linear_layer(first_linear, new_input_dim, old_out, self.device)\n",
        "            return head\n",
        "\n",
        "        def _update_mlp_heads(self):\n",
        "            gnn_output_dim = self._get_current_gnn_output_dim()\n",
        "            global_history_dim_flat = GLOBAL_SUMMARY_FEATURE_DIM * MAX_GLOBAL_HISTORY_LEN\n",
        "            new_base_input_dim = gnn_output_dim + global_history_dim_flat\n",
        "\n",
        "            if new_base_input_dim == self.heads_input_dim_global_current:\n",
        "                return\n",
        "\n",
        "            head_configs = {\n",
        "                'head_target_edit_type': (new_base_input_dim, self.action_space_sizes['num_edit_types']),\n",
        "                'head_meta_self_edit_type': (new_base_input_dim, NUM_META_SELF_EDIT_TYPES),\n",
        "                'head_value': (new_base_input_dim, 1),\n",
        "                'head_target_loc_stage': (new_base_input_dim + EDIT_TYPE_EMBED_DIM, self.action_space_sizes['num_stages_target']),\n",
        "                'head_target_conv_ch_mult': (new_base_input_dim + EDIT_TYPE_EMBED_DIM, len(DISCRETE_CH_MULT_ADD)),\n",
        "                'head_target_resize_factor': (new_base_input_dim + EDIT_TYPE_EMBED_DIM, len(DISCRETE_RESIZE_FACTORS)),\n",
        "                'head_resize_op_selector_scorer': (gnn_output_dim, 1),\n",
        "                'head_skip_source_scorer': (gnn_output_dim, 1),\n",
        "                'head_skip_destination_scorer': (gnn_output_dim, 1),\n",
        "            }\n",
        "\n",
        "            for name, (in_dim, out_dim) in head_configs.items():\n",
        "                if hasattr(self, name):\n",
        "                    setattr(self, name, self._adapt_head_input(getattr(self, name), in_dim))\n",
        "                else:\n",
        "                    setattr(self, name, nn.Linear(in_dim, out_dim).to(self.device))\n",
        "                    if name in self.policy_head_names:\n",
        "                        self.head_depth_counters[name] = 1\n",
        "\n",
        "            self.heads_input_dim_global_current = new_base_input_dim\n",
        "\n",
        "        def _build_gnn_layers(self):\n",
        "            current_in_dim = self.node_feature_dim\n",
        "            if self.current_num_gnn_layers > 0:\n",
        "                for _ in range(self.current_num_gnn_layers):\n",
        "                    out_dim = self.current_gnn_hidden_dim\n",
        "                    self.gnn_layers.append(pyg_nn.GCNConv(current_in_dim, out_dim, bias=True, normalize=True, add_self_loops=True).to(self.device))\n",
        "                    current_in_dim = out_dim\n",
        "\n",
        "        def get_mlp_head_names(self):\n",
        "             return self.policy_head_names\n",
        "\n",
        "        def _process_graph_and_state(self, graph_data, global_states_history_flat):\n",
        "            node_features, edge_index = graph_data.x, graph_data.edge_index\n",
        "            embeddings = node_features\n",
        "            if self.current_num_gnn_layers > 0 and graph_data.num_nodes > 0:\n",
        "                for gnn_layer in self.gnn_layers:\n",
        "                    embeddings = F.relu(gnn_layer(embeddings, edge_index))\n",
        "            elif graph_data.num_nodes == 0:\n",
        "                embeddings = torch.empty(0, self._get_current_gnn_output_dim(), device=self.device)\n",
        "            batch_vector = graph_data.batch\n",
        "            if batch_vector is None and embeddings.numel() > 0:\n",
        "                batch_vector = torch.zeros(embeddings.size(0), dtype=torch.long, device=self.device)\n",
        "            graph_embedding = pyg_nn.global_mean_pool(embeddings, batch_vector) if graph_data.num_nodes > 0 else torch.zeros(1, self._get_current_gnn_output_dim(), device=self.device)\n",
        "            if global_states_history_flat.ndim == 1: global_states_history_flat = global_states_history_flat.unsqueeze(0)\n",
        "            if graph_embedding.ndim == 1: graph_embedding = graph_embedding.unsqueeze(0)\n",
        "            combined_features = torch.cat((graph_embedding, global_states_history_flat), dim=1)\n",
        "            return combined_features, embeddings\n",
        "\n",
        "        def forward(self, graph_data, global_states_history_flat):\n",
        "            combined_features, node_embeddings = self._process_graph_and_state(graph_data, global_states_history_flat)\n",
        "            l_te = self.head_target_edit_type(combined_features)\n",
        "            l_mse = self.head_meta_self_edit_type(combined_features)\n",
        "            value_pred = self.head_value(combined_features)\n",
        "            return l_te, l_mse, value_pred, node_embeddings, combined_features\n",
        "\n",
        "        def get_conditional_logits(self, base_state_embedding, chosen_edit_type_tensor):\n",
        "            type_emb = self.edit_type_embedding(chosen_edit_type_tensor)\n",
        "            conditional_state = torch.cat([base_state_embedding, type_emb], dim=1)\n",
        "            logits = {}\n",
        "            edit_type = chosen_edit_type_tensor.item()\n",
        "            if edit_type == EDIT_TYPE_ADD_CONV_BLOCK:\n",
        "                logits['stage'] = self.head_target_loc_stage(conditional_state)\n",
        "                logits['ch_mult'] = self.head_target_conv_ch_mult(conditional_state)\n",
        "            elif edit_type == EDIT_TYPE_RESIZE_LAYER:\n",
        "                logits['stage'] = self.head_target_loc_stage(conditional_state)\n",
        "                logits['resize_factor'] = self.head_target_resize_factor(conditional_state)\n",
        "            elif edit_type == EDIT_TYPE_ADD_SKIP:\n",
        "                logits['stage'] = self.head_target_loc_stage(conditional_state)\n",
        "            return logits\n",
        "\n",
        "        def deepen_gnn(self, device='cpu'):\n",
        "            print(f\"  Deepening MetaAgentGNN: GNN Layers {self.current_num_gnn_layers} -> {self.current_num_gnn_layers + 1}\")\n",
        "            new_layer_in_dim = self._get_current_gnn_output_dim()\n",
        "            if self.current_num_gnn_layers == 0 :\n",
        "                new_gcn_layer = pyg_nn.GCNConv(self.node_feature_dim, self.current_gnn_hidden_dim, bias=True, normalize=True, add_self_loops=True).to(device)\n",
        "            else:\n",
        "                new_gcn_layer = create_identity_gcn_layer(self.current_gnn_hidden_dim, device=device)\n",
        "            self.gnn_layers.append(new_gcn_layer)\n",
        "            self.current_num_gnn_layers += 1\n",
        "            self._update_mlp_heads()\n",
        "            return True\n",
        "\n",
        "        def widen_gnn_hidden_dim(self, factor=META_GNN_WIDEN_FACTOR, device='cpu'):\n",
        "            old_dim = self.current_gnn_hidden_dim\n",
        "            new_dim = max(MIN_GNN_HIDDEN_DIM, int(round(old_dim * factor)))\n",
        "            if new_dim == old_dim or self.current_num_gnn_layers == 0: return False\n",
        "\n",
        "            print(f\"  Widening MetaAgentGNN: GNN Hidden Dim {old_dim} -> {new_dim}\")\n",
        "            new_gnn_list = nn.ModuleList()\n",
        "            current_in_dim = self.node_feature_dim\n",
        "            for i in range(self.current_num_gnn_layers):\n",
        "                original_gcn = self.gnn_layers[i]\n",
        "                final_gcn, _ = resize_gcn_conv_hidden(original_gcn, new_dim, current_in_dim, device)\n",
        "                new_gnn_list.append(final_gcn)\n",
        "                current_in_dim = new_dim\n",
        "            self.gnn_layers = new_gnn_list\n",
        "            self.current_gnn_hidden_dim = new_dim\n",
        "            self._update_mlp_heads()\n",
        "            return True\n",
        "\n",
        "        def deepen_one_mlp_head(self, head_attr_name, device='cpu'):\n",
        "            original_comp = getattr(self, head_attr_name, None)\n",
        "            if original_comp is None: return False\n",
        "            current_depth = self.head_depth_counters.get(head_attr_name, 1)\n",
        "            if current_depth >= 8: return False\n",
        "            changed = net2deeper_linear_insert_identity(self, head_attr_name, device=device)\n",
        "            if changed:\n",
        "                new_comp = getattr(self, head_attr_name)\n",
        "                self.head_depth_counters[head_attr_name] = len(new_comp) if isinstance(new_comp, nn.Sequential) else 1\n",
        "            return changed\n",
        "\n",
        "        def shrink_gnn_hidden_dim(self, factor=META_GNN_SHRINK_FACTOR, device='cpu'):\n",
        "            return self.widen_gnn_hidden_dim(factor=factor, device=device)\n",
        "\n",
        "        def prune_gnn_layer(self, device='cpu'):\n",
        "            if self.current_num_gnn_layers <= MIN_GNN_LAYERS: return False\n",
        "            print(f\"  Pruning MetaAgentGNN: GNN Layers {self.current_num_gnn_layers} -> {self.current_num_gnn_layers - 1}\")\n",
        "            self.gnn_layers.pop(-1)\n",
        "            self.current_num_gnn_layers -= 1\n",
        "            self._update_mlp_heads()\n",
        "            return True\n",
        "\n",
        "        def prune_one_mlp_head(self, head_attr_name, device='cpu'):\n",
        "            original_comp = getattr(self, head_attr_name, None)\n",
        "            if original_comp is None: return False\n",
        "            current_depth = self.head_depth_counters.get(head_attr_name, 1)\n",
        "            if current_depth <= MIN_MLP_HEAD_SEQUENTIAL_DEPTH: return False\n",
        "            changed = net2thinner_linear_remove_layer(self, head_attr_name, device=device)\n",
        "            if changed:\n",
        "                new_comp = getattr(self, head_attr_name)\n",
        "                self.head_depth_counters[head_attr_name] = len(new_comp) if isinstance(new_comp, nn.Sequential) else 1\n",
        "            return changed\n",
        "\n",
        "# =======================================================================\n",
        "#  Prometheus System\n",
        "# =======================================================================\n",
        "class DEITI:\n",
        "    def __init__(self):\n",
        "        if not PYG_AVAILABLE: raise ImportError(\"PyTorch Geometric required for Prometheus.\")\n",
        "        self.device=DEVICE\n",
        "        self.target_cnn = TargetCNN().to(DEVICE)\n",
        "        self.meta_agent = MetaAgentGNN(\n",
        "            device_to_use_agent=self.device, num_edit_types=NUM_TARGET_CNN_EDIT_TYPES, num_stages_target=NUM_STAGES_TARGET_CNN,\n",
        "            num_ch_mults=len(DISCRETE_CH_MULT_ADD), num_resize_factors=len(DISCRETE_RESIZE_FACTORS),\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        self.criterion_target=nn.CrossEntropyLoss()\n",
        "        self.global_states_history_buffer=[]\n",
        "        self.amp_scaler = torch.amp.GradScaler(enabled=(self.device.type=='cuda'))\n",
        "        self._init_dataloaders()\n",
        "        self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "        self.sched_target = CosineAnnealingLR(self.opt_target, T_max=BASE_POST_EPOCHS)\n",
        "        self.opt_meta = optim.Adam(self.meta_agent.parameters(), lr=BASE_META_LR)\n",
        "\n",
        "        self.frozen_bns = []\n",
        "        self.warmup_state = {'active': False, 'original_lr': LEARNING_RATE, 'param_ratio': 1.0}\n",
        "\n",
        "        self.best_global_accuracy = -1.0\n",
        "        self.best_global_model = None\n",
        "\n",
        "        self.iterations_without_improvement = 0\n",
        "        self.consecutive_dummy_pass_failures = 0\n",
        "\n",
        "    def _init_dataloaders(self):\n",
        "        cifar10_mean, cifar10_std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "            transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "            transforms.RandomErasing(p=0.5, scale=(0.02, 0.2)),\n",
        "        ])\n",
        "\n",
        "        val_transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "        ])\n",
        "        try:\n",
        "            tr_ds = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=train_transforms)\n",
        "            val_ds = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=val_transforms)\n",
        "        except Exception as e:\n",
        "            print(f\"CIFAR-10 download failed: {e}. Using FakeData.\")\n",
        "            fake_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "            ])\n",
        "            tr_ds = torchvision.datasets.FakeData(size=BATCH_SIZE*50, image_size=(3,32,32), num_classes=10, transform=fake_transform)\n",
        "            val_ds = torchvision.datasets.FakeData(size=BATCH_SIZE*20, image_size=(3,32,32), num_classes=10, transform=fake_transform)\n",
        "\n",
        "        num_workers = 4 if self.device.type == 'cuda' else 0\n",
        "        use_persistent_workers = num_workers > 0\n",
        "        print(f\"Using {num_workers} workers for data loading (persistent: {use_persistent_workers}).\")\n",
        "        self.train_loader = DataLoader(tr_ds, BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=self.device.type=='cuda', drop_last=True, persistent_workers=use_persistent_workers)\n",
        "        self.val_loader = DataLoader(val_ds, BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=self.device.type=='cuda', drop_last=True, persistent_workers=use_persistent_workers)\n",
        "\n",
        "    def _create_target_cnn_graph_data(self):\n",
        "        nodes, src, tgt, op_map, gid = {}, [], [], {}, 0\n",
        "        _, initial_spatial = self.target_cnn.input_placeholder_desc['out_channels'], self.target_cnn.input_placeholder_desc['out_spatial']\n",
        "        current_spatial_map = {-1: initial_spatial}\n",
        "        for si, sm in enumerate(self.target_cnn.stages):\n",
        "            stage_initial_spatial = current_spatial_map.get(si-1, initial_spatial)\n",
        "            op_output_props = {-1: (sm.initial_in_channels, stage_initial_spatial)}\n",
        "            for oi, od in enumerate(sm.op_descriptions):\n",
        "                op_map[(si, oi)] = gid\n",
        "                op_type_str = od.get('type', 'none')\n",
        "                input_indices = od.get('input_indices', [-1])\n",
        "                input_indices = input_indices if isinstance(input_indices, list) else [input_indices]\n",
        "                prev_op_idx = input_indices[0]\n",
        "                _, expected_in_spatial = op_output_props.get(prev_op_idx, (sm.initial_in_channels, stage_initial_spatial))\n",
        "                is_conv = 1.0 if op_type_str == 'conv2d' else 0.0\n",
        "                out_spatial = expected_in_spatial\n",
        "                stride_val = 1\n",
        "                if op_type_str == 'maxpool2d':\n",
        "                    stride_val = sm.ops[oi].stride if hasattr(sm.ops[oi], 'stride') else 1\n",
        "                    out_spatial //= stride_val\n",
        "                elif op_type_str == 'conv2d':\n",
        "                    current_op = sm.ops[oi]\n",
        "                    stride_val = current_op.stride[0] if isinstance(current_op.stride, tuple) else current_op.stride\n",
        "                    out_spatial //= stride_val\n",
        "                op_output_props[oi] = (od.get('out_channels', 0), out_spatial)\n",
        "                od['out_spatial'] = out_spatial\n",
        "                nf = [\n",
        "                    float(OP_TYPE_IDS.get(op_type_str, 0)),\n",
        "                    float(od.get('out_channels', 0)) / NORMALIZATION_WIDTH_DIVISOR,\n",
        "                    float(si) / max(1, NUM_STAGES_TARGET_CNN - 1),\n",
        "                    float(oi) / max(1, len(sm.ops) - 1),\n",
        "                    is_conv,\n",
        "                    float(stride_val -1),\n",
        "                    float(out_spatial) / NORMALIZATION_SPATIAL_DIVISOR,\n",
        "                ]\n",
        "                nodes[gid] = nf\n",
        "                gid += 1\n",
        "            if sm.op_descriptions:\n",
        "                 current_spatial_map[si] = op_output_props[len(sm.ops)-1][1]\n",
        "            else:\n",
        "                 current_spatial_map[si] = stage_initial_spatial\n",
        "        s_out_gid_map = {s: op_map.get((s, len(sm_loop.ops) - 1), -1) for s, sm_loop in enumerate(self.target_cnn.stages) if sm_loop.ops}\n",
        "        last_conv_gid = s_out_gid_map.get(len(self.target_cnn.stages) - 1, -1)\n",
        "        for oi, od in enumerate(self.target_cnn.classifier_op_descriptions):\n",
        "            op_map[('classifier', oi)] = gid\n",
        "            op_type_str = od.get('type', 'none')\n",
        "            nf = [\n",
        "                float(OP_TYPE_IDS.get(op_type_str, 0)),\n",
        "                float(od.get('out_features', 0)) / NORMALIZATION_WIDTH_DIVISOR,\n",
        "                1.0, float(oi) / max(1, len(self.target_cnn.classifier) - 1),\n",
        "                0.0, 0.0, 0.0\n",
        "            ]\n",
        "            nodes[gid] = nf\n",
        "            gid += 1\n",
        "        for si, sm in enumerate(self.target_cnn.stages):\n",
        "            for oi, od in enumerate(sm.op_descriptions):\n",
        "                cur_gid = op_map.get((si, oi), -1)\n",
        "                if cur_gid == -1: continue\n",
        "                in_ids = od.get('input_indices', [-1] if oi == 0 else [oi - 1])\n",
        "                in_ids = in_ids if isinstance(in_ids, list) else [in_ids]\n",
        "                for lsid in in_ids:\n",
        "                    if lsid == -1: sgid = s_out_gid_map.get(si - 1, -1) if si > 0 else -1\n",
        "                    else: sgid = op_map.get((si, lsid), -1)\n",
        "                    if sgid != -1: src.append(sgid); tgt.append(cur_gid)\n",
        "        prev_gid = last_conv_gid\n",
        "        for oi, od in enumerate(self.target_cnn.classifier_op_descriptions):\n",
        "            cur_gid = op_map.get(('classifier', oi), -1)\n",
        "            if cur_gid != -1 and prev_gid != -1:\n",
        "                src.append(prev_gid)\n",
        "                tgt.append(cur_gid)\n",
        "            prev_gid = cur_gid\n",
        "        x = torch.tensor([nodes[i] for i in range(gid)] if gid > 0 else [[0.] * NODE_FEATURE_DIM], dtype=torch.float32, device=self.device)\n",
        "        eidx = torch.tensor([src, tgt], dtype=torch.long, device=self.device) if src else torch.empty((2, 0), dtype=torch.long, device=self.device)\n",
        "        return Data(x=x, edge_index=eidx, batch=torch.zeros(x.size(0), dtype=torch.long, device=self.device) if x.size(0) > 0 else None), op_map, s_out_gid_map\n",
        "\n",
        "    def _ensure_target_cnn_consistency(self):\n",
        "        current_channels, current_spatial = self.target_cnn.input_placeholder_desc['out_channels'], self.target_cnn.input_placeholder_desc['out_spatial']\n",
        "        for stage_module in self.target_cnn.stages:\n",
        "            stage_module.initial_in_channels = current_channels\n",
        "            stage_module.initial_spatial_size = current_spatial\n",
        "            op_output_props = {-1: (current_channels, current_spatial)}\n",
        "            for i, op in enumerate(stage_module.ops):\n",
        "                desc = stage_module.op_descriptions[i]\n",
        "                input_indices = desc.get('input_indices', [-1]); input_indices = input_indices if isinstance(input_indices, list) else [input_indices]\n",
        "                prev_op_idx = input_indices[0] if input_indices else -1\n",
        "                expected_in_channels, expected_in_spatial = op_output_props.get(prev_op_idx, (current_channels, current_spatial))\n",
        "                new_op = None\n",
        "                if isinstance(op, nn.Conv2d):\n",
        "                    if op.in_channels != expected_in_channels: new_op = adapt_conv_input_channels(op, expected_in_channels, self.device)\n",
        "                    if new_op is None and op.in_channels != expected_in_channels: return False\n",
        "                    desc['out_channels'] = op.out_channels if new_op is None else new_op.out_channels\n",
        "                    stride_val = op.stride[0] if isinstance(op.stride, tuple) else op.stride\n",
        "                    desc['out_spatial'] = expected_in_spatial // stride_val\n",
        "                elif isinstance(op, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "                    if op.num_features != expected_in_channels: new_op = adapt_batchnorm_features(op, expected_in_channels, self.device)\n",
        "                    desc['out_channels'] = expected_in_channels\n",
        "                    desc['out_spatial'] = expected_in_spatial\n",
        "                elif isinstance(op, nn.ReLU):\n",
        "                    desc['out_channels'] = expected_in_channels; desc['out_spatial'] = expected_in_spatial\n",
        "                elif isinstance(op, nn.MaxPool2d):\n",
        "                    stride_val = op.stride if isinstance(op.stride, int) else op.stride[0]\n",
        "                    desc['out_channels'] = expected_in_channels; desc['out_spatial'] = expected_in_spatial // stride_val\n",
        "                elif isinstance(op, AddWithProjection):\n",
        "                    primary_ch, primary_sp = op_output_props[input_indices[0]]; skip_ch, _ = op_output_props[input_indices[1]]\n",
        "                    if primary_ch != skip_ch: op.projection = nn.Sequential(nn.Conv2d(skip_ch, primary_ch, kernel_size=1, bias=False), nn.BatchNorm2d(primary_ch)).to(self.device)\n",
        "                    else: op.projection = nn.Identity()\n",
        "                    desc['out_channels'], desc['out_spatial'] = primary_ch, primary_sp\n",
        "                if new_op is not None: stage_module.ops[i] = new_op\n",
        "                op_output_props[i] = (desc.get('out_channels'), desc.get('out_spatial'))\n",
        "            current_channels, current_spatial = stage_module.get_current_out_properties()\n",
        "        last_conv_channels, _ = self.target_cnn.get_last_stage_out_properties()\n",
        "        current_features = max(1, last_conv_channels)\n",
        "        for i, op in enumerate(self.target_cnn.classifier):\n",
        "            desc = self.target_cnn.classifier_op_descriptions[i]\n",
        "            new_op = None\n",
        "            if isinstance(op, nn.Linear):\n",
        "                if op.in_features != current_features: new_op = adapt_linear_input_features(op, current_features, self.device)\n",
        "                current_features = op.out_features if new_op is None else new_op.out_features\n",
        "                desc['out_features'] = current_features\n",
        "            elif isinstance(op, nn.BatchNorm1d):\n",
        "                if op.num_features != current_features: new_op = adapt_batchnorm_features(op, current_features, self.device)\n",
        "            elif isinstance(op, nn.ReLU):\n",
        "                pass\n",
        "            if new_op is not None: self.target_cnn.classifier[i] = new_op\n",
        "        return True\n",
        "\n",
        "    def _apply_target_cnn_edit(self, actions):\n",
        "        edit_type = actions['target_edit_type'].item(); changed = False\n",
        "        newly_added_bns = []\n",
        "\n",
        "        if edit_type == EDIT_TYPE_ADD_LINEAR_BLOCK:\n",
        "            prev_out_features = -1\n",
        "            for op in reversed(self.target_cnn.classifier[:-1]):\n",
        "                if hasattr(op, 'out_features'):\n",
        "                    prev_out_features = op.out_features\n",
        "                    break\n",
        "            if prev_out_features == -1:\n",
        "                prev_out_features = self.target_cnn.get_classifier_in_features()\n",
        "\n",
        "            new_linear = nn.Linear(prev_out_features, prev_out_features).to(self.device)\n",
        "            nn.init.eye_(new_linear.weight)\n",
        "            if new_linear.bias is not None: nn.init.zeros_(new_linear.bias)\n",
        "\n",
        "            bn = nn.BatchNorm1d(prev_out_features, momentum=0.1).to(self.device)\n",
        "            with torch.no_grad(): bn.weight.data.fill_(1.0); bn.bias.data.zero_()\n",
        "            bn.eval(); newly_added_bns.append(bn)\n",
        "\n",
        "            new_relu = nn.ReLU(inplace=False).to(self.device)\n",
        "            insert_idx = len(self.target_cnn.classifier) - 1\n",
        "            self.target_cnn.classifier.insert(insert_idx, new_linear); self.target_cnn.classifier.insert(insert_idx + 1, bn); self.target_cnn.classifier.insert(insert_idx + 2, new_relu)\n",
        "            self.target_cnn.classifier_op_descriptions.insert(insert_idx, {'type': 'linear', 'out_features': new_linear.out_features})\n",
        "            self.target_cnn.classifier_op_descriptions.insert(insert_idx + 1, {'type': 'batchnorm1d', 'out_features': new_linear.out_features})\n",
        "            self.target_cnn.classifier_op_descriptions.insert(insert_idx + 2, {'type': 'relu', 'out_features': new_linear.out_features})\n",
        "            changed = True\n",
        "\n",
        "        elif edit_type == EDIT_TYPE_ADD_CONV_BLOCK:\n",
        "            stage_idx = actions['target_loc_stage'].item()\n",
        "            if not (0 <= stage_idx < len(self.target_cnn.stages)): return False, []\n",
        "            stage = self.target_cnn.stages[stage_idx]\n",
        "            in_ch, in_sp = stage.get_current_out_properties(); in_ch = max(1, in_ch)\n",
        "\n",
        "            k = 3; s = 1\n",
        "            identity_conv = nn.Conv2d(in_ch, in_ch, k, stride=s, padding=(k-1)//2, bias=False).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                identity_conv.weight.data.zero_()\n",
        "                center = k // 2\n",
        "                for i in range(in_ch): identity_conv.weight.data[i, i, center, center] = 1.0\n",
        "\n",
        "            bn = nn.BatchNorm2d(in_ch, momentum=0.1).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                bn.weight.data.fill_(1.0)\n",
        "                bn.bias.data.zero_()\n",
        "            bn.eval()\n",
        "            newly_added_bns.append(bn)\n",
        "            relu = nn.ReLU(inplace=False).to(self.device)\n",
        "\n",
        "            insert_idx = len(stage.ops)\n",
        "            in_indices = [-1] if insert_idx == 0 else [insert_idx-1]\n",
        "            stage.add_op(identity_conv, {'type':'conv2d', 'out_channels': in_ch, 'out_spatial': in_sp, 'input_indices':in_indices}, insert_at=insert_idx)\n",
        "            stage.add_op(bn, {'type':'batchnorm2d', 'out_channels': in_ch, 'out_spatial': in_sp, 'input_indices':[insert_idx]}, insert_at=insert_idx+1)\n",
        "            stage.add_op(relu, {'type':'relu', 'out_channels': in_ch, 'out_spatial': in_sp, 'input_indices':[insert_idx+1]}, insert_at=insert_idx+2)\n",
        "\n",
        "            m = DISCRETE_CH_MULT_ADD[actions['target_conv_ch_mult_idx'].item()]\n",
        "            target_out_ch = max(1, int(round(in_ch * m)))\n",
        "\n",
        "            if in_ch != target_out_ch:\n",
        "                factor = float(target_out_ch) / in_ch\n",
        "                widened_conv = resize_conv_output(stage.ops[insert_idx], factor, self.device)\n",
        "                widened_bn = adapt_batchnorm_features(stage.ops[insert_idx+1], widened_conv.out_channels, self.device)\n",
        "\n",
        "                stage.ops[insert_idx] = widened_conv\n",
        "                stage.ops[insert_idx+1] = widened_bn\n",
        "\n",
        "                stage.op_descriptions[insert_idx]['out_channels'] = widened_conv.out_channels\n",
        "                stage.op_descriptions[insert_idx+1]['out_channels'] = widened_conv.out_channels\n",
        "                stage.op_descriptions[insert_idx+2]['out_channels'] = widened_conv.out_channels\n",
        "\n",
        "            changed = True\n",
        "\n",
        "        elif edit_type == EDIT_TYPE_RESIZE_LAYER:\n",
        "            stage_idx = actions['target_loc_stage'].item()\n",
        "            if not (0 <= stage_idx < len(self.target_cnn.stages)): return False, []\n",
        "            stage = self.target_cnn.stages[stage_idx]\n",
        "            op_idx = actions.get('target_actual_op_idx_in_stage', -1)\n",
        "            if op_idx != -1 and 0 <= op_idx < len(stage.ops):\n",
        "                op_mod = stage.ops[op_idx]\n",
        "                factor = DISCRETE_RESIZE_FACTORS[actions['target_resize_factor_idx'].item()]\n",
        "                if isinstance(op_mod, nn.Conv2d):\n",
        "                    new_op = resize_conv_output(op_mod, factor, self.device)\n",
        "                    if new_op is not op_mod: stage.ops[op_idx] = new_op; stage.op_descriptions[op_idx]['out_channels'] = new_op.out_channels; changed = True\n",
        "                elif isinstance(op_mod, nn.Linear):\n",
        "                    new_op = resize_linear_output(op_mod, factor, self.device)\n",
        "                    if new_op is not op_mod: stage.ops[op_idx] = new_op; stage.op_descriptions[op_idx]['out_features'] = new_op.out_features; changed = True\n",
        "        elif edit_type == EDIT_TYPE_ADD_SKIP:\n",
        "            stage_idx = actions['target_loc_stage'].item()\n",
        "            if not (0 <= stage_idx < len(self.target_cnn.stages)): return False, []\n",
        "            stage = self.target_cnn.stages[stage_idx]\n",
        "            source_op_idx = actions.get('source_op_idx', -1); dest_op_idx = actions.get('dest_op_idx', -1)\n",
        "            if source_op_idx != -1 and dest_op_idx != -1:\n",
        "                add_op = AddWithProjection().to(self.device)\n",
        "                _, dest_sp = stage.get_op_output_properties(dest_op_idx)\n",
        "                dest_ch, _ = stage.get_op_output_properties(dest_op_idx)\n",
        "                add_desc = {'type': 'add', 'out_channels': dest_ch, 'out_spatial': dest_sp, 'input_indices': [dest_op_idx, source_op_idx]}\n",
        "                insert_at_idx = dest_op_idx + 1\n",
        "                stage.add_op(add_op, add_desc, insert_at=insert_at_idx)\n",
        "                for i in range(insert_at_idx, len(stage.ops)):\n",
        "                    if stage.op_descriptions[i].get('input_indices') == [dest_op_idx]:\n",
        "                        stage.op_descriptions[i]['input_indices'] = [insert_at_idx]; break\n",
        "                changed = True\n",
        "\n",
        "        if changed:\n",
        "            consistency_ok = self._ensure_target_cnn_consistency()\n",
        "            return consistency_ok, newly_added_bns\n",
        "\n",
        "        return False, []\n",
        "\n",
        "    def _apply_meta_self_edit(self, action):\n",
        "        edit_type = action.item()\n",
        "        changed = False\n",
        "        if edit_type == META_EDIT_DEEPEN_GNN:\n",
        "            changed = self.meta_agent.deepen_gnn(device=self.device)\n",
        "        elif edit_type == META_EDIT_WIDEN_GNN_HIDDEN:\n",
        "            changed = self.meta_agent.widen_gnn_hidden_dim(factor=META_GNN_WIDEN_FACTOR, device=self.device)\n",
        "        elif edit_type == META_EDIT_DEEPEN_MLP_HEAD:\n",
        "            head_names = self.meta_agent.get_mlp_head_names()\n",
        "            if head_names:\n",
        "                changed = self.meta_agent.deepen_one_mlp_head(np.random.choice(head_names), device=self.device)\n",
        "        elif edit_type == META_EDIT_SHRINK_GNN_HIDDEN:\n",
        "            changed = self.meta_agent.shrink_gnn_hidden_dim(factor=META_GNN_SHRINK_FACTOR, device=self.device)\n",
        "        elif edit_type == META_EDIT_PRUNE_GNN:\n",
        "            changed = self.meta_agent.prune_gnn_layer(device=self.device)\n",
        "        elif edit_type == META_EDIT_PRUNE_MLP_HEAD:\n",
        "            head_names = self.meta_agent.get_mlp_head_names()\n",
        "            prunable_heads = [h for h in head_names if self.meta_agent.head_depth_counters.get(h, 1) > MIN_MLP_HEAD_SEQUENTIAL_DEPTH]\n",
        "            if prunable_heads:\n",
        "                changed = self.meta_agent.prune_one_mlp_head(np.random.choice(prunable_heads), device=self.device)\n",
        "\n",
        "        if changed:\n",
        "            print(f\"MetaAgentGNN arch changed. New Params: {sum(p.numel() for p in self.meta_agent.parameters())}. Re-init optimizer.\")\n",
        "            current_lr = self.opt_meta.param_groups[0]['lr']\n",
        "            del self.opt_meta\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            self.opt_meta = optim.Adam(self.meta_agent.parameters(), lr=current_lr)\n",
        "        return changed\n",
        "\n",
        "    def _sanitize_bn_stats(self):\n",
        "        for m in self.target_cnn.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                m.running_mean.nan_to_num_(nan=0.0, posinf=1e4, neginf=-1e4)\n",
        "                m.running_var.nan_to_num_(nan=1.0, posinf=1e4, neginf=1e-4)\n",
        "                m.running_var.clamp_(min=1e-5)\n",
        "\n",
        "    def _train_target_one_epoch(self, loader, optimizer, scheduler, name=\"Tr\", current_epoch=0):\n",
        "        self.target_cnn.train()\n",
        "\n",
        "        warmup_total_epochs = 5\n",
        "        if self.warmup_state['active']:\n",
        "            if current_epoch < warmup_total_epochs:\n",
        "                lr_scale = (current_epoch + 1) / warmup_total_epochs\n",
        "                lr_scale /= math.sqrt(self.warmup_state['param_ratio'])\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['lr'] = self.warmup_state['original_lr'] * lr_scale\n",
        "\n",
        "            if current_epoch >= warmup_total_epochs:\n",
        "                print(f\"  Warmup complete. Restoring LR to {self.warmup_state['original_lr']:.2e}.\")\n",
        "                for g in optimizer.param_groups: g['lr'] = self.warmup_state['original_lr']\n",
        "                self.warmup_state['active'] = False\n",
        "\n",
        "        if self.frozen_bns and current_epoch >= warmup_total_epochs:\n",
        "            print(f\"  Unfreezing {len(self.frozen_bns)} new BatchNorm layers.\")\n",
        "            for bn in self.frozen_bns:\n",
        "                bn.train()\n",
        "            self.frozen_bns = []\n",
        "\n",
        "        loss_sum, n_batches, correct, total = 0, 0, 0, 0\n",
        "        data_iterator = itertools.islice(loader, BATCHES_PER_EPOCH) if BATCHES_PER_EPOCH is not None else loader\n",
        "        for x,y in data_iterator:\n",
        "            if x.size(0) <= 1: continue\n",
        "            x,y=x.to(self.device),y.to(self.device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(self.device.type,enabled=(self.device.type=='cuda')):\n",
        "                logits=self.target_cnn(x); loss = self.criterion_target(logits, y)\n",
        "\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                continue\n",
        "\n",
        "            self.amp_scaler.scale(loss).backward()\n",
        "            self.amp_scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(self.target_cnn.parameters(), MAX_GRAD_NORM)\n",
        "            self.amp_scaler.step(optimizer)\n",
        "            self.amp_scaler.update()\n",
        "\n",
        "            loss_sum+=loss.item(); _,pred=logits.max(1); total+=y.size(0); correct+=pred.eq(y).sum().item(); n_batches+=1\n",
        "\n",
        "        if scheduler and not self.warmup_state['active']:\n",
        "            scheduler.step()\n",
        "\n",
        "        self._sanitize_bn_stats()\n",
        "\n",
        "        return loss_sum/max(1,n_batches), correct/max(1,total)\n",
        "\n",
        "    def _validate_target(self, loader):\n",
        "        self.target_cnn.eval()\n",
        "        loss_sum, correct, total, n_batches = 0,0,0,0\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        data_iterator = itertools.islice(loader, BATCHES_PER_EPOCH) if BATCHES_PER_EPOCH is not None else loader\n",
        "        with torch.no_grad():\n",
        "            for x,y in data_iterator:\n",
        "                if x.size(0) <= 1: continue\n",
        "                x,y=x.to(self.device),y.to(self.device)\n",
        "                logits=self.target_cnn(x); loss=criterion(logits,y)\n",
        "                if torch.isnan(loss) or torch.isinf(loss): continue\n",
        "                loss_sum+=loss.item(); _,pred=logits.max(1); total+=y.size(0); correct+=pred.eq(y).sum().item(); n_batches+=1\n",
        "        return loss_sum/max(1,n_batches), correct/max(1,total)\n",
        "\n",
        "    def _validate_edit_with_dummy_pass(self):\n",
        "        self.target_cnn.eval()\n",
        "        try:\n",
        "            dummy_x, _ = next(iter(self.val_loader))\n",
        "            dummy_x = dummy_x.to(self.device)\n",
        "\n",
        "            with torch.no_grad(), autocast(self.device.type, enabled=(self.device.type=='cuda')):\n",
        "                output = self.target_cnn(dummy_x)\n",
        "\n",
        "            if torch.isnan(output).any() or torch.isinf(output).any():\n",
        "                print(\"  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\")\n",
        "                return False\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"  !!! Dummy pass failed with exception: {e}. Edit is invalid. !!!\")\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "        finally:\n",
        "            self.target_cnn.train()\n",
        "\n",
        "    def train_loop(self, iterations=100):\n",
        "        for itr in range(iterations):\n",
        "            print(f\"\\n===== Iteration {itr+1}/{iterations} =====\")\n",
        "            print(f\"Current MetaAgentGNN: GNN Layers={self.meta_agent.current_num_gnn_layers}, GNN Hidden={self.meta_agent.current_gnn_hidden_dim}, Params: {sum(p.numel() for p in self.meta_agent.parameters()):,}\")\n",
        "            print(f\"Current Global Best Accuracy: {self.best_global_accuracy:.4f}\")\n",
        "\n",
        "            if not hasattr(self, 'opt_target') or self.opt_target is None:\n",
        "                self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "            for pre_ep in range(PRE_EPOCHS):\n",
        "                self._train_target_one_epoch(self.train_loader, self.opt_target, None, f\"PreE{pre_ep+1}\", current_epoch=pre_ep)\n",
        "            val_loss_b, acc_b = self._validate_target(self.val_loader)\n",
        "            print(f\"PreVal: ValidL={val_loss_b:.4f}, ValidA={acc_b:.4f}\")\n",
        "\n",
        "            pre_edit_model_backup = copy.deepcopy(self.target_cnn)\n",
        "            old_params = sum(p.numel() for p in self.target_cnn.parameters() if p.requires_grad)\n",
        "\n",
        "            t_changed = False\n",
        "            complexity_breach = False\n",
        "            final_reward = 0.0\n",
        "\n",
        "            if itr == 0:\n",
        "                print(\"\\n--- Iteration 1: No-edit, performing full initial training. ---\")\n",
        "                post_epochs = 50\n",
        "                param_ratio = 1.0\n",
        "                lp_t, en_t, val_p = 0.0, 0.0, torch.tensor(0.0)\n",
        "\n",
        "            else:\n",
        "                graph, op_map, s_out_gid_map = self._create_target_cnn_graph_data()\n",
        "                target_params_M = sum(p.numel() for p in self.target_cnn.parameters() if p.requires_grad)/1e6\n",
        "                norm_m_gl=self.meta_agent.current_num_gnn_layers / (MIN_GNN_LAYERS + 5)\n",
        "                norm_m_gh=self.meta_agent.current_gnn_hidden_dim / (MIN_GNN_HIDDEN_DIM + 256)\n",
        "                g_state=[val_loss_b,acc_b,target_params_M,norm_m_gl,norm_m_gh]; self.global_states_history_buffer.append(g_state)\n",
        "                if len(self.global_states_history_buffer)>MAX_GLOBAL_HISTORY_LEN: self.global_states_history_buffer.pop(0)\n",
        "                hist = list(self.global_states_history_buffer);\n",
        "                while len(hist)<MAX_GLOBAL_HISTORY_LEN: hist.insert(0,[0.]*GLOBAL_SUMMARY_FEATURE_DIM)\n",
        "                g_hist_flat=torch.tensor(hist,dtype=torch.float32,device=self.device).view(1,-1)\n",
        "                actions = {}; lp_t, en_t = 0.0, 0.0\n",
        "                l_te, l_mse, val_p, fne, base_state_embed = self.meta_agent(graph, g_hist_flat)\n",
        "                dist_te = Categorical(logits=l_te); s_te = dist_te.sample()\n",
        "                actions['target_edit_type'] = s_te; lp_t += dist_te.log_prob(s_te); en_t += dist_te.entropy().mean()\n",
        "                edit_type = s_te.item()\n",
        "                conditional_logits = self.meta_agent.get_conditional_logits(base_state_embed, s_te)\n",
        "                if edit_type != EDIT_TYPE_ADD_LINEAR_BLOCK:\n",
        "                    l_ts = conditional_logits['stage']; dist_ts = Categorical(logits=l_ts); s_ts = dist_ts.sample()\n",
        "                    actions['target_loc_stage'] = s_ts; lp_t += dist_ts.log_prob(s_ts); en_t += dist_ts.entropy().mean()\n",
        "                if edit_type == EDIT_TYPE_ADD_CONV_BLOCK:\n",
        "                    l_tc = conditional_logits['ch_mult']; dist_tc = Categorical(logits=l_tc); s_tc = dist_tc.sample()\n",
        "                    actions['target_conv_ch_mult_idx'] = s_tc; lp_t += dist_tc.log_prob(s_tc); en_t += dist_tc.entropy().mean()\n",
        "                elif edit_type == EDIT_TYPE_RESIZE_LAYER:\n",
        "                    stage=self.target_cnn.stages[s_ts.item()]; candidates=[]\n",
        "                    for oi,op in enumerate(stage.ops):\n",
        "                        if isinstance(op, (nn.Conv2d, nn.Linear)):\n",
        "                            valid_idx = _valid_resize_indices(op.out_channels if isinstance(op, nn.Conv2d) else op.out_features)\n",
        "                            if valid_idx:\n",
        "                                gid=op_map.get((s_ts.item(),oi),-1)\n",
        "                                if gid!=-1 and gid<fne.size(0): candidates.append((oi,fne[gid],valid_idx))\n",
        "                    a_op_idx_rsz = -1; l_tr = conditional_logits['resize_factor']; dist_rf = Categorical(logits=l_tr)\n",
        "                    if candidates:\n",
        "                        scores = self.meta_agent.head_resize_op_selector_scorer(torch.stack([e for _,e,_ in candidates])).squeeze(-1)\n",
        "                        if scores.numel() > 0:\n",
        "                            dist_co = Categorical(logits=scores); s_kth = dist_co.sample(); a_op_idx_rsz = candidates[s_kth.item()][0]\n",
        "                            lp_t += dist_co.log_prob(s_kth); en_t += dist_co.entropy().mean()\n",
        "                            valid_rf_idx = candidates[s_kth.item()][2]\n",
        "                            masked_l_tr = _mask_logits(l_tr.squeeze(0), valid_rf_idx)\n",
        "                            dist_rf = Categorical(logits=masked_l_tr if not torch.all(torch.isinf(masked_l_tr)) else l_tr)\n",
        "                    s_rf_idx = dist_rf.sample(); lp_t += dist_rf.log_prob(s_rf_idx); en_t += dist_rf.entropy().mean()\n",
        "                    actions['target_resize_factor_idx'] = s_rf_idx; actions['target_actual_op_idx_in_stage'] = a_op_idx_rsz\n",
        "                elif edit_type == EDIT_TYPE_ADD_SKIP:\n",
        "                    stage = self.target_cnn.stages[s_ts.item()]; valid_pairs = []\n",
        "                    for i in range(-1, len(stage.ops)):\n",
        "                        for j in range(i + 1, len(stage.ops)):\n",
        "                            _, src_sp = stage.get_op_output_properties(i); _, dest_sp = stage.get_op_output_properties(j)\n",
        "                            if src_sp == dest_sp: valid_pairs.append((i, j))\n",
        "                    if valid_pairs:\n",
        "                        source_gids = [s_out_gid_map.get(s_ts.item() - 1, -1) if p[0] == -1 else op_map[(s_ts.item(), p[0])] for p in valid_pairs]\n",
        "                        dest_gids = [op_map[(s_ts.item(), p[1])] for p in valid_pairs]\n",
        "                        source_scores = self.meta_agent.head_skip_source_scorer(fne[source_gids]).squeeze()\n",
        "                        dest_scores = self.meta_agent.head_skip_destination_scorer(fne[dest_gids]).squeeze()\n",
        "                        if source_scores.dim() == 0: source_scores = source_scores.unsqueeze(0); dest_scores = dest_scores.unsqueeze(0)\n",
        "                        pair_scores = source_scores + dest_scores; dist_pair = Categorical(logits=pair_scores); chosen_pair_idx = dist_pair.sample()\n",
        "                        actions['source_op_idx'], actions['dest_op_idx'] = valid_pairs[chosen_pair_idx.item()]\n",
        "                        lp_t += dist_pair.log_prob(chosen_pair_idx); en_t += dist_pair.entropy()\n",
        "                    else: actions['source_op_idx'] = -1; actions['dest_op_idx'] = -1\n",
        "\n",
        "                log_parts = [f\"TargEdit:Typ={edit_type}\"]\n",
        "                if 'target_loc_stage' in actions: log_parts.append(f\"Stg={actions['target_loc_stage'].item()}\")\n",
        "                if edit_type==EDIT_TYPE_ADD_CONV_BLOCK: log_parts.append(f\"CHM={DISCRETE_CH_MULT_ADD[actions['target_conv_ch_mult_idx'].item()]}\")\n",
        "                elif edit_type==EDIT_TYPE_RESIZE_LAYER: log_parts.append(f\"Op={actions.get('target_actual_op_idx_in_stage',-1)},RszF={DISCRETE_RESIZE_FACTORS[actions.get('target_resize_factor_idx', 0).item()]}\")\n",
        "                elif edit_type==EDIT_TYPE_ADD_SKIP: log_parts.append(f\"Src={actions.get('source_op_idx', -1)}->Dest={actions.get('dest_op_idx', -1)}\")\n",
        "                print(\" \".join(log_parts))\n",
        "\n",
        "                param_ratio = 1.0\n",
        "                edit_applied_successfully, new_bns = self._apply_target_cnn_edit(actions)\n",
        "                t_changed = edit_applied_successfully\n",
        "\n",
        "                if edit_applied_successfully:\n",
        "                    # NEW: Check for complexity limit breach *before* dummy pass\n",
        "                    new_params = sum(p.numel() for p in self.target_cnn.parameters() if p.requires_grad)\n",
        "                    if new_params > COMPLEXITY_LIMIT:\n",
        "                        print(f\"!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has {new_params:,} params (> {COMPLEXITY_LIMIT:,}). Reverting edit. !!!\")\n",
        "                        self.target_cnn = pre_edit_model_backup\n",
        "                        t_changed = False\n",
        "                        complexity_breach = True # Set flag for punishment\n",
        "                    else:\n",
        "                        is_edit_valid = self._validate_edit_with_dummy_pass()\n",
        "                        if not is_edit_valid:\n",
        "                            print(\"  Edit rolled back due to dummy pass failure.\")\n",
        "                            self.target_cnn = pre_edit_model_backup\n",
        "                            t_changed = False\n",
        "                            self.consecutive_dummy_pass_failures += 1\n",
        "                            if self.consecutive_dummy_pass_failures >= 3:\n",
        "                                print(\"\\n!!! INSTABILITY DETECTED: 3 consecutive edits failed dummy pass. !!!\")\n",
        "                                print(\"  Attempting to prune (simplify) Meta-Agent...\")\n",
        "                                pruning_actions = [META_EDIT_SHRINK_GNN_HIDDEN, META_EDIT_PRUNE_GNN, META_EDIT_PRUNE_MLP_HEAD]\n",
        "                                valid_pruning_actions = [a for a in pruning_actions if a not in _invalid_meta_indices(self.meta_agent)]\n",
        "                                if valid_pruning_actions:\n",
        "                                    chosen_self_edit = np.random.choice(valid_pruning_actions)\n",
        "                                    self._apply_meta_self_edit(torch.tensor(chosen_self_edit, device=self.device))\n",
        "                                else:\n",
        "                                    print(\"  Meta-Agent is at minimum complexity or below prune threshold. No pruning possible.\")\n",
        "                                self.consecutive_dummy_pass_failures = 0\n",
        "                        else:\n",
        "                            self.consecutive_dummy_pass_failures = 0\n",
        "\n",
        "                if t_changed:\n",
        "                    param_ratio = new_params / max(1, old_params)\n",
        "                    print(f\"  TargetCNN arch changed. New Params: {new_params:,} (Ratio: {param_ratio:.2f})\")\n",
        "                    print(\"  Creating fresh optimizer for new architecture.\")\n",
        "                    if hasattr(self, 'opt_target'): del self.opt_target\n",
        "                    if hasattr(self, 'sched_target'): del self.sched_target\n",
        "                    gc.collect(); torch.cuda.empty_cache()\n",
        "                    self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "                    self.amp_scaler = torch.amp.GradScaler(enabled=(self.device.type=='cuda'))\n",
        "                    if param_ratio > 1.2:\n",
        "                        print(f\"  Large parameter jump. Activating LR warmup.\")\n",
        "                        self.warmup_state = {'active': True, 'original_lr': LEARNING_RATE, 'param_ratio': param_ratio}\n",
        "                    else:\n",
        "                        self.warmup_state['active'] = False\n",
        "                    self.frozen_bns = new_bns\n",
        "                else:\n",
        "                    if not complexity_breach: # Don't print this if it was a complexity breach, we already did\n",
        "                        print(\"  Edit was invalid or a no-op. Restoring pre-edit model.\")\n",
        "                    self.target_cnn = pre_edit_model_backup\n",
        "                    if hasattr(self, 'opt_target'): del self.opt_target\n",
        "                    if hasattr(self, 'sched_target'): del self.sched_target\n",
        "                    gc.collect(); torch.cuda.empty_cache()\n",
        "                    self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "                    self.amp_scaler = torch.amp.GradScaler(enabled=(self.device.type=='cuda'))\n",
        "\n",
        "                post_epochs = int(BASE_POST_EPOCHS * max(1.0, param_ratio))\n",
        "                post_epochs = min(post_epochs, 100)\n",
        "\n",
        "            print(f\"  Training for {post_epochs} post-edit epochs.\")\n",
        "            self.sched_target = CosineAnnealingLR(self.opt_target, T_max=post_epochs)\n",
        "\n",
        "            for post_ep in range(post_epochs):\n",
        "                train_loss, train_acc = self._train_target_one_epoch(self.train_loader,self.opt_target, self.sched_target, f\"PostE{post_ep+1}\", current_epoch=post_ep)\n",
        "                if (post_ep + 1) % 5 == 0 or post_ep == post_epochs - 1:\n",
        "                    print(f\"  PostE{post_ep+1}: TrainL={train_loss:.4f}, TrainA={train_acc:.4f}\")\n",
        "            val_loss_a, acc_a = self._validate_target(self.val_loader)\n",
        "            print(f\"PostVal: ValidL={val_loss_a:.4f}, ValidA={acc_a:.4f}\")\n",
        "\n",
        "            if itr > 0:\n",
        "                reward = 100 * (acc_a - acc_b)\n",
        "                penalty = 0.0\n",
        "                if complexity_breach:\n",
        "                    penalty = COMPLEXITY_BREACH_PENALTY\n",
        "                    print(f\"  Applying harsh penalty ({penalty:.2f}) for breaching complexity limit.\")\n",
        "                else:\n",
        "                    current_params = sum(p.numel() for p in self.target_cnn.parameters())\n",
        "                    if current_params > COMPLEXITY_PENALTY_THRESHOLD:\n",
        "                        excess_params_M = (current_params - COMPLEXITY_PENALTY_THRESHOLD) / 1e6\n",
        "                        penalty = (excess_params_M ** 2) * COMPLEXITY_PENALTY_ALPHA\n",
        "\n",
        "                final_reward = reward - penalty\n",
        "                print(f\"Reward: {reward:.4f} | Penalty: {penalty:.2f} | Final Reward: {final_reward:.4f}\")\n",
        "\n",
        "            new_best_found = False\n",
        "            if acc_a > self.best_global_accuracy and final_reward >= -10:\n",
        "                print(f\"  *** New Best Global Accuracy! {self.best_global_accuracy:.4f} -> {acc_a:.4f} ***\")\n",
        "                self.best_global_accuracy = acc_a\n",
        "                if self.best_global_model is not None:\n",
        "                    del self.best_global_model\n",
        "                self.best_global_model = copy.deepcopy(self.target_cnn)\n",
        "                self.iterations_without_improvement = 0\n",
        "                new_best_found = True\n",
        "            elif acc_a > self.best_global_accuracy and final_reward < -10:\n",
        "                print(f\"  New best accuracy ({acc_a:.4f}) achieved, but REJECTED due to large negative reward ({final_reward:.2f}). Not saving as global best.\")\n",
        "                self.iterations_without_improvement += 1\n",
        "            else:\n",
        "                self.iterations_without_improvement += 1\n",
        "                print(f\"  No improvement for {self.iterations_without_improvement} iterations.\")\n",
        "\n",
        "            if itr > 0:\n",
        "                if final_reward < -10 and t_changed:\n",
        "                    print(f\"  !!! PUNISHMENT REVERSION: Final reward ({final_reward:.2f}) is below -10. Reverting current model to pre-edit architecture. !!!\")\n",
        "                    del self.target_cnn; gc.collect()\n",
        "                    self.target_cnn = copy.deepcopy(pre_edit_model_backup)\n",
        "                    if hasattr(self, 'opt_target'): del self.opt_target\n",
        "                    if hasattr(self, 'sched_target'): del self.sched_target\n",
        "                    gc.collect(); torch.cuda.empty_cache()\n",
        "                    self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "                    self.amp_scaler = torch.amp.GradScaler(enabled=(self.device.type == 'cuda'))\n",
        "\n",
        "                advantage = final_reward - val_p.detach().squeeze().item()\n",
        "                actor_loss = -(lp_t)*advantage\n",
        "                critic_loss = F.mse_loss(val_p.squeeze(), torch.tensor(final_reward, device=self.device, dtype=torch.float32))\n",
        "                meta_loss=actor_loss.mean()+ 0.5 * critic_loss - 0.0005*(en_t)\n",
        "                if not (torch.isnan(meta_loss) or torch.isinf(meta_loss)):\n",
        "                    progress = (max(0, acc_a - LR_ACC_BASE_THRESHOLD)) / max(1e-6, LR_ACC_TARGET_THRESHOLD - LR_ACC_BASE_THRESHOLD)\n",
        "                    progress = min(1.0, progress)\n",
        "                    new_meta_lr = BASE_META_LR - progress * (BASE_META_LR - MIN_META_LR)\n",
        "                    for param_group in self.opt_meta.param_groups: param_group['lr'] = new_meta_lr\n",
        "                    self.opt_meta.zero_grad(); meta_loss.backward(); clip_grad_norm_(self.meta_agent.parameters(),MAX_GRAD_NORM); self.opt_meta.step()\n",
        "                    print(f\"MetaL:{meta_loss.item():.4f}(A:{actor_loss.mean().item():.4f},C:{critic_loss.item():.4f},E:{en_t.item():.4f}) | MetaLR: {new_meta_lr:.2e}\")\n",
        "                else:\n",
        "                    print(\"MetaLoss NaN/Inf! Skipping meta-update.\"); gc.collect()\n",
        "\n",
        "            if self.iterations_without_improvement >= 5:\n",
        "                print(\"\\n!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\")\n",
        "                print(f\"  Reverting Target CNN to best known model (Acc: {self.best_global_accuracy:.4f}).\")\n",
        "                if self.best_global_model is not None:\n",
        "                    self.target_cnn = copy.deepcopy(self.best_global_model)\n",
        "                    if hasattr(self, 'opt_target'): del self.opt_target\n",
        "                    if hasattr(self, 'sched_target'): del self.sched_target\n",
        "                    gc.collect()\n",
        "                    self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "                print(\"  Attempting to upgrade (grow) Meta-Agent...\")\n",
        "                growth_actions = [META_EDIT_DEEPEN_GNN, META_EDIT_WIDEN_GNN_HIDDEN, META_EDIT_DEEPEN_MLP_HEAD]\n",
        "                valid_growth_actions = [a for a in growth_actions if a not in _invalid_meta_indices(self.meta_agent)]\n",
        "                if valid_growth_actions:\n",
        "                    chosen_self_edit = np.random.choice(valid_growth_actions)\n",
        "                    self._apply_meta_self_edit(torch.tensor(chosen_self_edit, device=self.device))\n",
        "                else:\n",
        "                    print(\"  Meta-Agent cannot grow further. No self-edit possible.\")\n",
        "\n",
        "                self.iterations_without_improvement = 0\n",
        "                print(\"  Stagnation counter reset. Continuing search with upgraded agent.\\n\")\n",
        "\n",
        "            revert_threshold = 0.04\n",
        "            if self.best_global_model is not None and acc_a < (self.best_global_accuracy - revert_threshold) and not new_best_found:\n",
        "                print(f\"  !!! Accuracy dropped by >{revert_threshold:.0%}. Reverting to the global best model (Acc: {self.best_global_accuracy:.4f}). !!!\")\n",
        "                del self.target_cnn; gc.collect()\n",
        "                self.target_cnn = copy.deepcopy(self.best_global_model)\n",
        "                if hasattr(self, 'opt_target'): del self.opt_target\n",
        "                if hasattr(self, 'sched_target'): del self.sched_target\n",
        "                gc.collect()\n",
        "                self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "                self.sched_target = CosineAnnealingLR(self.opt_target, T_max=BASE_POST_EPOCHS)\n",
        "                self.amp_scaler = torch.amp.GradScaler(enabled=(self.device.type=='cuda'))\n",
        "\n",
        "            del pre_edit_model_backup\n",
        "            gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY3Awdagg28s",
        "outputId": "8650c6b8-9467-4102-99c5-3dc5d80a0877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model checkpoints will be saved to: /content/drive/My Drive/DEITI_Checkpoints\n",
            "Using 4 workers for data loading (persistent: True).\n",
            "\n",
            "\n",
            "==================== STAGE 1: BROAD SEARCH ====================\n",
            "Init TargetCNN P: 373,834\n",
            "Init MetaAgentGNN: L=2,H=32,P=3,059\n",
            "\n",
            "--- Initial Validation ---\n",
            "InitVal: L=2.3010,A=0.1330\n",
            "\n",
            "===== Iteration 1/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.1330\n",
            "PreVal: ValidL=1.4743, ValidA=0.4527\n",
            "\n",
            "--- Iteration 1: No-edit, performing full initial training. ---\n",
            "  Training for 50 post-edit epochs.\n",
            "  PostE5: TrainL=1.4052, TrainA=0.5019\n",
            "  PostE10: TrainL=1.2580, TrainA=0.5590\n",
            "  PostE15: TrainL=1.1637, TrainA=0.5896\n",
            "  PostE20: TrainL=1.0877, TrainA=0.6209\n",
            "  PostE25: TrainL=1.0385, TrainA=0.6386\n",
            "  PostE30: TrainL=0.9963, TrainA=0.6530\n",
            "  PostE35: TrainL=0.9565, TrainA=0.6708\n",
            "  PostE40: TrainL=0.9331, TrainA=0.6790\n",
            "  PostE45: TrainL=0.9245, TrainA=0.6828\n",
            "  PostE50: TrainL=0.9117, TrainA=0.6840\n",
            "PostVal: ValidL=0.5333, ValidA=0.8170\n",
            "  *** New Best Global Accuracy! 0.1330 -> 0.8170 ***\n",
            "\n",
            "===== Iteration 2/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.8170\n",
            "PreVal: ValidL=0.5331, ValidA=0.8181\n",
            "TargEdit:Typ=2 Stg=0 Src=0->Dest=1\n",
            "  TargetCNN arch changed. New Params: 373,834 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=1.0743, TrainA=0.6266\n",
            "  PostE10: TrainL=1.0262, TrainA=0.6456\n",
            "  PostE15: TrainL=0.9750, TrainA=0.6618\n",
            "  PostE20: TrainL=0.9294, TrainA=0.6793\n",
            "  PostE25: TrainL=0.9128, TrainA=0.6872\n",
            "PostVal: ValidL=0.5298, ValidA=0.8174\n",
            "Reward: -0.0701 | Penalty: 0.00 | Final Reward: -0.0701\n",
            "  *** New Best Global Accuracy! 0.8170 -> 0.8174 ***\n",
            "MetaL:-0.0128(A:-0.0107,C:0.0000,E:4.1585) | MetaLR: 4.34e-04\n",
            "\n",
            "===== Iteration 3/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.8174\n",
            "PreVal: ValidL=0.5294, ValidA=0.8175\n",
            "TargEdit:Typ=2 Stg=2 Src=0->Dest=1\n",
            "  TargetCNN arch changed. New Params: 373,834 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.9736, TrainA=0.6593\n",
            "  PostE10: TrainL=0.9238, TrainA=0.6787\n",
            "  PostE15: TrainL=0.8880, TrainA=0.6944\n",
            "  PostE20: TrainL=0.8375, TrainA=0.7125\n",
            "  PostE25: TrainL=0.8193, TrainA=0.7171\n",
            "PostVal: ValidL=0.4552, ValidA=0.8457\n",
            "Reward: 2.8145 | Penalty: 0.00 | Final Reward: 2.8145\n",
            "  *** New Best Global Accuracy! 0.8174 -> 0.8457 ***\n",
            "MetaL:19.1799(A:14.8029,C:8.7582,E:4.1560) | MetaLR: 3.28e-04\n",
            "\n",
            "===== Iteration 4/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.8457\n",
            "PreVal: ValidL=0.4553, ValidA=0.8459\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 440,138 (Ratio: 1.18)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 29 post-edit epochs.\n",
            "  PostE5: TrainL=0.9354, TrainA=0.6733\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.8835, TrainA=0.6924\n",
            "  PostE15: TrainL=0.8299, TrainA=0.7112\n",
            "  PostE20: TrainL=0.7822, TrainA=0.7270\n",
            "  PostE25: TrainL=0.7480, TrainA=0.7404\n",
            "  PostE29: TrainL=0.7347, TrainA=0.7436\n",
            "PostVal: ValidL=0.3901, ValidA=0.8700\n",
            "Reward: 2.4139 | Penalty: 0.00 | Final Reward: 2.4139\n",
            "  *** New Best Global Accuracy! 0.8457 -> 0.8700 ***\n",
            "MetaL:7.9565(A:4.6484,C:6.6175,E:1.3633) | MetaLR: 2.36e-04\n",
            "\n",
            "===== Iteration 5/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.8700\n",
            "PreVal: ValidL=0.3908, ValidA=0.8687\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 587,850 (Ratio: 1.34)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 33 post-edit epochs.\n",
            "  PostE5: TrainL=0.8765, TrainA=0.6945\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.8377, TrainA=0.7093\n",
            "  PostE15: TrainL=0.7843, TrainA=0.7256\n",
            "  PostE20: TrainL=0.7340, TrainA=0.7421\n",
            "  PostE25: TrainL=0.6927, TrainA=0.7596\n",
            "  PostE30: TrainL=0.6470, TrainA=0.7723\n",
            "  PostE33: TrainL=0.6353, TrainA=0.7786\n",
            "PostVal: ValidL=0.3255, ValidA=0.8887\n",
            "Reward: 2.0032 | Penalty: 0.00 | Final Reward: 2.0032\n",
            "  *** New Best Global Accuracy! 0.8700 -> 0.8887 ***\n",
            "MetaL:10.3729(A:7.8929,C:4.9635,E:3.4856) | MetaLR: 1.66e-04\n",
            "\n",
            "===== Iteration 6/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.8887\n",
            "PreVal: ValidL=0.3244, ValidA=0.8883\n",
            "TargEdit:Typ=2 Stg=0 Src=0->Dest=3\n",
            "  TargetCNN arch changed. New Params: 587,722 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.8156, TrainA=0.7152\n",
            "  PostE10: TrainL=0.7393, TrainA=0.7432\n",
            "  PostE15: TrainL=0.6705, TrainA=0.7655\n",
            "  PostE20: TrainL=0.6197, TrainA=0.7835\n",
            "  PostE25: TrainL=0.6001, TrainA=0.7904\n",
            "PostVal: ValidL=0.3137, ValidA=0.8914\n",
            "Reward: 0.3105 | Penalty: 0.00 | Final Reward: 0.3105\n",
            "  *** New Best Global Accuracy! 0.8887 -> 0.8914 ***\n",
            "MetaL:2.3578(A:2.2613,C:0.1981,E:5.1055) | MetaLR: 1.55e-04\n",
            "\n",
            "===== Iteration 7/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.8914\n",
            "PreVal: ValidL=0.3133, ValidA=0.8922\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 654,026 (Ratio: 1.11)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.7312, TrainA=0.7458\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.6785, TrainA=0.7624\n",
            "  PostE15: TrainL=0.6219, TrainA=0.7817\n",
            "  PostE20: TrainL=0.5660, TrainA=0.8003\n",
            "  PostE25: TrainL=0.5456, TrainA=0.8089\n",
            "  PostE27: TrainL=0.5400, TrainA=0.8111\n",
            "PostVal: ValidL=0.2944, ValidA=0.9018\n",
            "Reward: 0.9615 | Penalty: 0.00 | Final Reward: 0.9615\n",
            "  *** New Best Global Accuracy! 0.8914 -> 0.9018 ***\n",
            "MetaL:2.7782(A:2.1474,C:1.2629,E:1.3531) | MetaLR: 1.16e-04\n",
            "\n",
            "===== Iteration 8/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9018\n",
            "PreVal: ValidL=0.2935, ValidA=0.8999\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 720,330 (Ratio: 1.10)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.6576, TrainA=0.7706\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.6172, TrainA=0.7836\n",
            "  PostE15: TrainL=0.5734, TrainA=0.7993\n",
            "  PostE20: TrainL=0.5253, TrainA=0.8157\n",
            "  PostE25: TrainL=0.5060, TrainA=0.8239\n",
            "  PostE27: TrainL=0.4982, TrainA=0.8269\n",
            "PostVal: ValidL=0.2764, ValidA=0.9061\n",
            "Reward: 0.6210 | Penalty: 0.00 | Final Reward: 0.6210\n",
            "  *** New Best Global Accuracy! 0.9018 -> 0.9061 ***\n",
            "MetaL:1.8914(A:1.5595,C:0.6652,E:1.3520) | MetaLR: 9.99e-05\n",
            "\n",
            "===== Iteration 9/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9061\n",
            "PreVal: ValidL=0.2763, ValidA=0.9060\n",
            "TargEdit:Typ=1 Stg=0 Op=6,RszF=1.25\n",
            "  TargetCNN arch changed. New Params: 775,626 (Ratio: 1.08)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 26 post-edit epochs.\n",
            "  PostE5: TrainL=0.6126, TrainA=0.7847\n",
            "  PostE10: TrainL=0.5711, TrainA=0.8001\n",
            "  PostE15: TrainL=0.5349, TrainA=0.8123\n",
            "  PostE20: TrainL=0.4876, TrainA=0.8285\n",
            "  PostE25: TrainL=0.4699, TrainA=0.8366\n",
            "  PostE26: TrainL=0.4670, TrainA=0.8369\n",
            "PostVal: ValidL=0.2753, ValidA=0.9081\n",
            "Reward: 0.2003 | Penalty: 0.00 | Final Reward: 0.2003\n",
            "  *** New Best Global Accuracy! 0.9061 -> 0.9081 ***\n",
            "MetaL:2.3053(A:2.2154,C:0.1846,E:4.8619) | MetaLR: 9.27e-05\n",
            "\n",
            "===== Iteration 10/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9081\n",
            "PreVal: ValidL=0.2728, ValidA=0.9072\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 841,930 (Ratio: 1.09)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.5885, TrainA=0.7949\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.5515, TrainA=0.8073\n",
            "  PostE15: TrainL=0.5159, TrainA=0.8206\n",
            "  PostE20: TrainL=0.4674, TrainA=0.8343\n",
            "  PostE25: TrainL=0.4370, TrainA=0.8465\n",
            "  PostE27: TrainL=0.4448, TrainA=0.8435\n",
            "PostVal: ValidL=0.2655, ValidA=0.9127\n",
            "Reward: 0.5509 | Penalty: 0.00 | Final Reward: 0.5509\n",
            "  *** New Best Global Accuracy! 0.9081 -> 0.9127 ***\n",
            "MetaL:1.8449(A:1.5266,C:0.6380,E:1.3481) | MetaLR: 7.54e-05\n",
            "\n",
            "===== Iteration 11/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9127\n",
            "PreVal: ValidL=0.2677, ValidA=0.9120\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 1,432,266 (Ratio: 1.70)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 42 post-edit epochs.\n",
            "  PostE5: TrainL=0.5487, TrainA=0.8081\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.5447, TrainA=0.8094\n",
            "  PostE15: TrainL=0.5181, TrainA=0.8184\n",
            "  PostE20: TrainL=0.4780, TrainA=0.8319\n",
            "  PostE25: TrainL=0.4405, TrainA=0.8464\n",
            "  PostE30: TrainL=0.4037, TrainA=0.8586\n",
            "  PostE35: TrainL=0.3745, TrainA=0.8686\n",
            "  PostE40: TrainL=0.3497, TrainA=0.8770\n",
            "  PostE42: TrainL=0.3449, TrainA=0.8793\n",
            "PostVal: ValidL=0.2439, ValidA=0.9219\n",
            "Reward: 0.9916 | Penalty: 0.00 | Final Reward: 0.9916\n",
            "  *** New Best Global Accuracy! 0.9127 -> 0.9219 ***\n",
            "MetaL:4.6920(A:3.9066,C:1.5744,E:3.4915) | MetaLR: 4.06e-05\n",
            "\n",
            "===== Iteration 12/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9219\n",
            "PreVal: ValidL=0.2436, ValidA=0.9233\n",
            "TargEdit:Typ=1 Stg=2 Op=0,RszF=0.75\n",
            "  TargetCNN arch changed. New Params: 1,268,298 (Ratio: 0.89)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.4922, TrainA=0.8278\n",
            "  PostE10: TrainL=0.4524, TrainA=0.8430\n",
            "  PostE15: TrainL=0.4066, TrainA=0.8587\n",
            "  PostE20: TrainL=0.3698, TrainA=0.8709\n",
            "  PostE25: TrainL=0.3461, TrainA=0.8778\n",
            "PostVal: ValidL=0.2597, ValidA=0.9165\n",
            "Reward: -0.6811 | Penalty: 0.00 | Final Reward: -0.6811\n",
            "  No improvement for 1 iterations.\n",
            "MetaL:-1.7262(A:-1.8151,C:0.1819,E:4.1665) | MetaLR: 6.10e-05\n",
            "\n",
            "===== Iteration 13/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9219\n",
            "PreVal: ValidL=0.2534, ValidA=0.9184\n",
            "TargEdit:Typ=2 Stg=0 Src=2->Dest=3\n",
            "  TargetCNN arch changed. New Params: 1,157,706 (Ratio: 0.91)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.7117, TrainA=0.7515\n",
            "  PostE10: TrainL=0.6432, TrainA=0.7757\n",
            "  PostE15: TrainL=0.5875, TrainA=0.7932\n",
            "  PostE20: TrainL=0.5373, TrainA=0.8108\n",
            "  PostE25: TrainL=0.5260, TrainA=0.8132\n",
            "PostVal: ValidL=0.2835, ValidA=0.9042\n",
            "Reward: -1.4123 | Penalty: 0.00 | Final Reward: -1.4123\n",
            "  No improvement for 2 iterations.\n",
            "MetaL:-5.7185(A:-6.3403,C:1.2493,E:5.6792) | MetaLR: 1.07e-04\n",
            "\n",
            "===== Iteration 14/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9219\n",
            "PreVal: ValidL=0.2810, ValidA=0.9040\n",
            "TargEdit:Typ=2 Stg=0 Src=4->Dest=7\n",
            "  TargetCNN arch changed. New Params: 1,168,266 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.6072, TrainA=0.7855\n",
            "  PostE10: TrainL=0.5698, TrainA=0.8004\n",
            "  PostE15: TrainL=0.5218, TrainA=0.8167\n",
            "  PostE20: TrainL=0.4833, TrainA=0.8294\n",
            "  PostE25: TrainL=0.4737, TrainA=0.8344\n",
            "PostVal: ValidL=0.2609, ValidA=0.9105\n",
            "Reward: 0.6410 | Penalty: 0.00 | Final Reward: 0.6410\n",
            "  No improvement for 3 iterations.\n",
            "MetaL:6.2442(A:5.7520,C:0.9903,E:5.9216) | MetaLR: 8.37e-05\n",
            "\n",
            "===== Iteration 15/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9219\n",
            "PreVal: ValidL=0.2590, ValidA=0.9114\n",
            "TargEdit:Typ=1 Stg=0 Op=7,RszF=0.75\n",
            "  TargetCNN arch changed. New Params: 1,142,586 (Ratio: 0.98)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.5601, TrainA=0.8040\n",
            "  PostE10: TrainL=0.5343, TrainA=0.8141\n",
            "  PostE15: TrainL=0.4894, TrainA=0.8277\n",
            "  PostE20: TrainL=0.4537, TrainA=0.8419\n",
            "  PostE25: TrainL=0.4390, TrainA=0.8464\n",
            "PostVal: ValidL=0.2569, ValidA=0.9152\n",
            "Reward: 0.3806 | Penalty: 0.00 | Final Reward: 0.3806\n",
            "  No improvement for 4 iterations.\n",
            "MetaL:4.2921(A:3.9901,C:0.6089,E:4.8533) | MetaLR: 6.59e-05\n",
            "\n",
            "===== Iteration 16/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9219\n",
            "PreVal: ValidL=0.2596, ValidA=0.9164\n",
            "TargEdit:Typ=1 Stg=1 Op=4,RszF=0.25\n",
            "  TargetCNN arch changed. New Params: 589,242 (Ratio: 0.52)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.6726, TrainA=0.7646\n",
            "  PostE10: TrainL=0.6254, TrainA=0.7798\n",
            "  PostE15: TrainL=0.5743, TrainA=0.7997\n",
            "  PostE20: TrainL=0.5427, TrainA=0.8087\n",
            "  PostE25: TrainL=0.5260, TrainA=0.8141\n",
            "PostVal: ValidL=0.3038, ValidA=0.8987\n",
            "Reward: -1.7628 | Penalty: 0.00 | Final Reward: -1.7628\n",
            "  No improvement for 5 iterations.\n",
            "MetaL:-5.2999(A:-6.2076,C:1.8204,E:4.8498) | MetaLR: 1.28e-04\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9219).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Widening MetaAgentGNN: GNN Hidden Dim 32 -> 48\n",
            "MetaAgentGNN arch changed. New Params: 4915. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 17/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9219\n",
            "PreVal: ValidL=0.3106, ValidA=0.8987\n",
            "TargEdit:Typ=2 Stg=1 Src=3->Dest=6\n",
            "  TargetCNN arch changed. New Params: 1,465,546 (Ratio: 1.02)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.4974, TrainA=0.8262\n",
            "  PostE10: TrainL=0.4449, TrainA=0.8431\n",
            "  PostE15: TrainL=0.3919, TrainA=0.8634\n",
            "  PostE20: TrainL=0.3497, TrainA=0.8762\n",
            "  PostE25: TrainL=0.3347, TrainA=0.8833\n",
            "PostVal: ValidL=0.2410, ValidA=0.9233\n",
            "Reward: 2.4539 | Penalty: 0.00 | Final Reward: 2.4539\n",
            "  *** New Best Global Accuracy! 0.9219 -> 0.9233 ***\n",
            "MetaL:16.7541(A:13.4538,C:6.6055,E:4.8798) | MetaLR: 3.53e-05\n",
            "\n",
            "===== Iteration 18/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9233\n",
            "PreVal: ValidL=0.2380, ValidA=0.9228\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 1,696,266 (Ratio: 1.16)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 28 post-edit epochs.\n",
            "  PostE5: TrainL=0.4701, TrainA=0.8357\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.4287, TrainA=0.8513\n",
            "  PostE15: TrainL=0.3768, TrainA=0.8695\n",
            "  PostE20: TrainL=0.3399, TrainA=0.8810\n",
            "  PostE25: TrainL=0.3082, TrainA=0.8922\n",
            "  PostE28: TrainL=0.3014, TrainA=0.8939\n",
            "PostVal: ValidL=0.2401, ValidA=0.9257\n",
            "Reward: 0.2905 | Penalty: 0.00 | Final Reward: 0.2905\n",
            "  *** New Best Global Accuracy! 0.9233 -> 0.9257 ***\n",
            "MetaL:1.2883(A:1.1954,C:0.1892,E:3.4374) | MetaLR: 2.63e-05\n",
            "\n",
            "===== Iteration 19/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9257\n",
            "PreVal: ValidL=0.2382, ValidA=0.9260\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 1,762,570 (Ratio: 1.04)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.4233, TrainA=0.8528\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.3844, TrainA=0.8651\n",
            "  PostE15: TrainL=0.3416, TrainA=0.8823\n",
            "  PostE20: TrainL=0.3014, TrainA=0.8948\n",
            "  PostE25: TrainL=0.2839, TrainA=0.9016\n",
            "PostVal: ValidL=0.2419, ValidA=0.9272\n",
            "Reward: 0.1202 | Penalty: 0.00 | Final Reward: 0.1202\n",
            "  *** New Best Global Accuracy! 0.9257 -> 0.9272 ***\n",
            "MetaL:0.5853(A:0.5386,C:0.0948,E:1.3540) | MetaLR: 2.06e-05\n",
            "\n",
            "===== Iteration 20/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9272\n",
            "PreVal: ValidL=0.2398, ValidA=0.9272\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 1,993,290 (Ratio: 1.13)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 28 post-edit epochs.\n",
            "  PostE5: TrainL=0.4101, TrainA=0.8575\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.3757, TrainA=0.8684\n",
            "  PostE15: TrainL=0.3274, TrainA=0.8833\n",
            "  PostE20: TrainL=0.2952, TrainA=0.8967\n",
            "  PostE25: TrainL=0.2668, TrainA=0.9060\n",
            "  PostE28: TrainL=0.2571, TrainA=0.9098\n",
            "PostVal: ValidL=0.2327, ValidA=0.9315\n",
            "Reward: 0.4307 | Penalty: 0.00 | Final Reward: 0.4307\n",
            "  *** New Best Global Accuracy! 0.9272 -> 0.9315 ***\n",
            "MetaL:1.9649(A:1.7629,C:0.4075,E:3.4357) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 21/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9315\n",
            "PreVal: ValidL=0.2336, ValidA=0.9322\n",
            "TargEdit:Typ=0 Stg=2 CHM=0.5\n",
            "  TargetCNN arch changed. New Params: 2,255,690 (Ratio: 1.13)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 28 post-edit epochs.\n",
            "  PostE5: TrainL=0.4246, TrainA=0.8534\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.3688, TrainA=0.8719\n",
            "  PostE15: TrainL=0.3193, TrainA=0.8886\n",
            "  PostE20: TrainL=0.2904, TrainA=0.8980\n",
            "  PostE25: TrainL=0.2575, TrainA=0.9099\n",
            "  PostE28: TrainL=0.2522, TrainA=0.9144\n",
            "PostVal: ValidL=0.2113, ValidA=0.9351\n",
            "Reward: 0.2905 | Penalty: 0.00 | Final Reward: 0.2905\n",
            "  *** New Best Global Accuracy! 0.9315 -> 0.9351 ***\n",
            "MetaL:2.4355(A:2.2915,C:0.2914,E:3.4348) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 22/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9351\n",
            "PreVal: ValidL=0.2119, ValidA=0.9343\n",
            "TargEdit:Typ=1 Stg=2 Op=0,RszF=0.75\n",
            "  TargetCNN arch changed. New Params: 2,034,378 (Ratio: 0.90)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.3915, TrainA=0.8644\n",
            "  PostE10: TrainL=0.3399, TrainA=0.8809\n",
            "  PostE15: TrainL=0.2992, TrainA=0.8950\n",
            "  PostE20: TrainL=0.2619, TrainA=0.9080\n",
            "  PostE25: TrainL=0.2433, TrainA=0.9149\n",
            "PostVal: ValidL=0.2184, ValidA=0.9354\n",
            "Reward: 0.1102 | Penalty: 0.00 | Final Reward: 0.1102\n",
            "  *** New Best Global Accuracy! 0.9351 -> 0.9354 ***\n",
            "MetaL:2.0078(A:1.9332,C:0.1539,E:4.7404) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 23/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9354\n",
            "PreVal: ValidL=0.2198, ValidA=0.9343\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 2,265,098 (Ratio: 1.11)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.3678, TrainA=0.8701\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.3228, TrainA=0.8878\n",
            "  PostE15: TrainL=0.2925, TrainA=0.8979\n",
            "  PostE20: TrainL=0.2454, TrainA=0.9141\n",
            "  PostE25: TrainL=0.2240, TrainA=0.9219\n",
            "  PostE27: TrainL=0.2289, TrainA=0.9198\n",
            "PostVal: ValidL=0.2148, ValidA=0.9384\n",
            "Reward: 0.4107 | Penalty: 0.00 | Final Reward: 0.4107\n",
            "  *** New Best Global Accuracy! 0.9354 -> 0.9384 ***\n",
            "MetaL:2.1756(A:1.9260,C:0.5026,E:3.4186) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 24/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9384\n",
            "PreVal: ValidL=0.2149, ValidA=0.9376\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 2,331,402 (Ratio: 1.03)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.3361, TrainA=0.8825\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2976, TrainA=0.8959\n",
            "  PostE15: TrainL=0.2617, TrainA=0.9093\n",
            "  PostE20: TrainL=0.2272, TrainA=0.9213\n",
            "  PostE25: TrainL=0.2156, TrainA=0.9248\n",
            "PostVal: ValidL=0.2088, ValidA=0.9414\n",
            "Reward: 0.3806 | Penalty: 0.00 | Final Reward: 0.3806\n",
            "  *** New Best Global Accuracy! 0.9384 -> 0.9414 ***\n",
            "MetaL:1.5508(A:1.3031,C:0.4968,E:1.3356) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 25/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9414\n",
            "PreVal: ValidL=0.2105, ValidA=0.9406\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 2,397,706 (Ratio: 1.03)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.3187, TrainA=0.8899\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2833, TrainA=0.9033\n",
            "  PostE15: TrainL=0.2538, TrainA=0.9132\n",
            "  PostE20: TrainL=0.2157, TrainA=0.9252\n",
            "  PostE25: TrainL=0.2048, TrainA=0.9291\n",
            "PostVal: ValidL=0.2116, ValidA=0.9383\n",
            "Reward: -0.2304 | Penalty: 0.00 | Final Reward: -0.2304\n",
            "  No improvement for 1 iterations.\n",
            "MetaL:0.1519(A:0.1495,C:0.0061,E:1.3195) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 26/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9414\n",
            "PreVal: ValidL=0.2084, ValidA=0.9395\n",
            "TargEdit:Typ=2 Stg=0 Src=4->Dest=10\n",
            "  TargetCNN arch changed. New Params: 2,408,266 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.3111, TrainA=0.8919\n",
            "  PostE10: TrainL=0.2856, TrainA=0.8999\n",
            "  PostE15: TrainL=0.2443, TrainA=0.9155\n",
            "  PostE20: TrainL=0.2167, TrainA=0.9249\n",
            "  PostE25: TrainL=0.2051, TrainA=0.9295\n",
            "PostVal: ValidL=0.2048, ValidA=0.9431\n",
            "Reward: 0.3606 | Penalty: 0.00 | Final Reward: 0.3606\n",
            "  *** New Best Global Accuracy! 0.9414 -> 0.9431 ***\n",
            "MetaL:5.1514(A:4.9126,C:0.4849,E:7.2650) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 27/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9431\n",
            "PreVal: ValidL=0.2042, ValidA=0.9429\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 3,054,026 (Ratio: 1.27)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 31 post-edit epochs.\n",
            "  PostE5: TrainL=0.3100, TrainA=0.8934\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.3089, TrainA=0.8943\n",
            "  PostE15: TrainL=0.2800, TrainA=0.9028\n",
            "  PostE20: TrainL=0.2488, TrainA=0.9128\n",
            "  PostE25: TrainL=0.2256, TrainA=0.9217\n",
            "  PostE30: TrainL=0.1943, TrainA=0.9326\n",
            "  PostE31: TrainL=0.1903, TrainA=0.9335\n",
            "PostVal: ValidL=0.2117, ValidA=0.9426\n",
            "Reward: -0.0300 | Penalty: 0.00 | Final Reward: -0.0300\n",
            "  No improvement for 1 iterations.\n",
            "MetaL:0.8927(A:0.8414,C:0.1061,E:3.3972) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 28/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9431\n",
            "PreVal: ValidL=0.2119, ValidA=0.9439\n",
            "TargEdit:Typ=2 Stg=0 Src=3->Dest=10\n",
            "  TargetCNN arch changed. New Params: 3,064,266 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.3070, TrainA=0.8944\n",
            "  PostE10: TrainL=0.2740, TrainA=0.9044\n",
            "  PostE15: TrainL=0.2381, TrainA=0.9170\n",
            "  PostE20: TrainL=0.2110, TrainA=0.9271\n",
            "  PostE25: TrainL=0.1996, TrainA=0.9303\n",
            "PostVal: ValidL=0.2107, ValidA=0.9411\n",
            "Reward: -0.2804 | Penalty: 0.00 | Final Reward: -0.2804\n",
            "  No improvement for 2 iterations.\n",
            "MetaL:1.0137(A:1.0085,C:0.0182,E:7.6961) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 29/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9431\n",
            "PreVal: ValidL=0.2100, ValidA=0.9407\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 4,687,306 (Ratio: 1.53)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 38 post-edit epochs.\n",
            "  PostE5: TrainL=0.3201, TrainA=0.8888\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.3079, TrainA=0.8912\n",
            "  PostE15: TrainL=0.2854, TrainA=0.9017\n",
            "  PostE20: TrainL=0.2627, TrainA=0.9093\n",
            "  PostE25: TrainL=0.2376, TrainA=0.9169\n",
            "  PostE30: TrainL=0.2103, TrainA=0.9267\n",
            "  PostE35: TrainL=0.1838, TrainA=0.9357\n",
            "  PostE38: TrainL=0.1806, TrainA=0.9374\n",
            "PostVal: ValidL=0.2065, ValidA=0.9466\n",
            "Reward: 0.5909 | Penalty: 0.00 | Final Reward: 0.5909\n",
            "  *** New Best Global Accuracy! 0.9431 -> 0.9466 ***\n",
            "MetaL:3.5831(A:3.0195,C:1.1306,E:3.3762) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 30/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9466\n",
            "PreVal: ValidL=0.2065, ValidA=0.9459\n",
            "TargEdit:Typ=0 Stg=2 CHM=0.5\n",
            "  TargetCNN arch changed. New Params: 4,744,778 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.3258, TrainA=0.8864\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2702, TrainA=0.9065\n",
            "  PostE15: TrainL=0.2353, TrainA=0.9189\n",
            "  PostE20: TrainL=0.1983, TrainA=0.9305\n",
            "  PostE25: TrainL=0.1899, TrainA=0.9358\n",
            "PostVal: ValidL=0.1886, ValidA=0.9476\n",
            "Reward: 0.1703 | Penalty: 0.00 | Final Reward: 0.1703\n",
            "  *** New Best Global Accuracy! 0.9466 -> 0.9476 ***\n",
            "MetaL:4.0990(A:3.7330,C:0.7353,E:3.3991) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 31/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9476\n",
            "PreVal: ValidL=0.1910, ValidA=0.9473\n",
            "TargEdit:Typ=2 Stg=0 Src=14->Dest=22\n",
            "  TargetCNN arch changed. New Params: 4,796,618 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.2828, TrainA=0.9032\n",
            "  PostE10: TrainL=0.2521, TrainA=0.9133\n",
            "  PostE15: TrainL=0.2143, TrainA=0.9267\n",
            "  PostE20: TrainL=0.1838, TrainA=0.9366\n",
            "  PostE25: TrainL=0.1721, TrainA=0.9407\n",
            "PostVal: ValidL=0.1857, ValidA=0.9488\n",
            "Reward: 0.1502 | Penalty: 0.00 | Final Reward: 0.1502\n",
            "  *** New Best Global Accuracy! 0.9476 -> 0.9488 ***\n",
            "MetaL:7.6175(A:7.1545,C:0.9338,E:7.7216) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 32/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9488\n",
            "PreVal: ValidL=0.1840, ValidA=0.9488\n",
            "TargEdit:Typ=0 Stg=1 CHM=0.5\n",
            "  TargetCNN arch changed. New Params: 5,534,410 (Ratio: 1.15)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 28 post-edit epochs.\n",
            "  PostE5: TrainL=0.2979, TrainA=0.8951\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2657, TrainA=0.9067\n",
            "  PostE15: TrainL=0.2253, TrainA=0.9221\n",
            "  PostE20: TrainL=0.1921, TrainA=0.9327\n",
            "  PostE25: TrainL=0.1692, TrainA=0.9417\n",
            "  PostE28: TrainL=0.1708, TrainA=0.9408\n",
            "PostVal: ValidL=0.1869, ValidA=0.9505\n",
            "Reward: 0.1703 | Penalty: 0.00 | Final Reward: 0.1703\n",
            "  *** New Best Global Accuracy! 0.9488 -> 0.9505 ***\n",
            "MetaL:5.1434(A:4.4826,C:1.3251,E:3.3604) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 33/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9505\n",
            "PreVal: ValidL=0.1853, ValidA=0.9502\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 7,157,450 (Ratio: 1.29)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 32 post-edit epochs.\n",
            "  PostE5: TrainL=0.2930, TrainA=0.8990\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2858, TrainA=0.9014\n",
            "  PostE15: TrainL=0.2553, TrainA=0.9124\n",
            "  PostE20: TrainL=0.2269, TrainA=0.9210\n",
            "  PostE25: TrainL=0.1913, TrainA=0.9345\n",
            "  PostE30: TrainL=0.1723, TrainA=0.9402\n",
            "  PostE32: TrainL=0.1616, TrainA=0.9439\n",
            "PostVal: ValidL=0.1833, ValidA=0.9529\n",
            "Reward: 0.2704 | Penalty: 0.00 | Final Reward: 0.2704\n",
            "  *** New Best Global Accuracy! 0.9505 -> 0.9529 ***\n",
            "MetaL:4.1019(A:3.2583,C:1.6906,E:3.2846) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 34/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9529\n",
            "PreVal: ValidL=0.1916, ValidA=0.9514\n",
            "TargEdit:Typ=2 Stg=0 Src=3->Dest=13\n",
            "  TargetCNN arch changed. New Params: 6,931,850 (Ratio: 0.97)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.2720, TrainA=0.9053\n",
            "  PostE10: TrainL=0.2344, TrainA=0.9185\n",
            "  PostE15: TrainL=0.2042, TrainA=0.9277\n",
            "  PostE20: TrainL=0.1672, TrainA=0.9417\n",
            "  PostE25: TrainL=0.1557, TrainA=0.9457\n",
            "PostVal: ValidL=0.1797, ValidA=0.9521\n",
            "Reward: 0.0701 | Penalty: 0.00 | Final Reward: 0.0701\n",
            "  No improvement for 1 iterations.\n",
            "MetaL:10.3979(A:9.6023,C:1.5990,E:7.7251) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 35/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9529\n",
            "PreVal: ValidL=0.1825, ValidA=0.9527\n",
            "TargEdit:Typ=1 Stg=1 Op=8,RszF=1.5\n",
            "  TargetCNN arch changed. New Params: 8,112,010 (Ratio: 1.17)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 29 post-edit epochs.\n",
            "  PostE5: TrainL=0.2436, TrainA=0.9145\n",
            "  PostE10: TrainL=0.2238, TrainA=0.9226\n",
            "  PostE15: TrainL=0.1933, TrainA=0.9323\n",
            "  PostE20: TrainL=0.1640, TrainA=0.9424\n",
            "  PostE25: TrainL=0.1507, TrainA=0.9475\n",
            "  PostE29: TrainL=0.1439, TrainA=0.9506\n",
            "PostVal: ValidL=0.1825, ValidA=0.9548\n",
            "Reward: 0.2103 | Penalty: 0.00 | Final Reward: 0.2103\n",
            "  *** New Best Global Accuracy! 0.9529 -> 0.9548 ***\n",
            "MetaL:11.7347(A:10.4574,C:2.5597,E:5.1211) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 36/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9548\n",
            "PreVal: ValidL=0.1838, ValidA=0.9537\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 13,717,386 (Ratio: 1.69)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 42 post-edit epochs.\n",
            "  PostE5: TrainL=0.2521, TrainA=0.9141\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2543, TrainA=0.9118\n",
            "  PostE15: TrainL=0.2346, TrainA=0.9184\n",
            "  PostE20: TrainL=0.2209, TrainA=0.9247\n",
            "  PostE25: TrainL=0.1925, TrainA=0.9335\n",
            "  PostE30: TrainL=0.1690, TrainA=0.9423\n",
            "  PostE35: TrainL=0.1534, TrainA=0.9471\n",
            "  PostE40: TrainL=0.1370, TrainA=0.9521\n",
            "  PostE42: TrainL=0.1334, TrainA=0.9539\n",
            "PostVal: ValidL=0.1761, ValidA=0.9567\n",
            "Reward: 0.3005 | Penalty: 0.00 | Final Reward: 0.3005\n",
            "  *** New Best Global Accuracy! 0.9548 -> 0.9567 ***\n",
            "MetaL:6.3101(A:4.4313,C:3.7608,E:3.2077) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 37/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9567\n",
            "PreVal: ValidL=0.1797, ValidA=0.9575\n",
            "TargEdit:Typ=2 Stg=1 Src=9->Dest=18\n",
            "  TargetCNN arch changed. New Params: 14,505,866 (Ratio: 1.06)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 26 post-edit epochs.\n",
            "  PostE5: TrainL=0.2196, TrainA=0.9247\n",
            "  PostE10: TrainL=0.1969, TrainA=0.9322\n",
            "  PostE15: TrainL=0.1700, TrainA=0.9402\n",
            "  PostE20: TrainL=0.1346, TrainA=0.9541\n",
            "  PostE25: TrainL=0.1308, TrainA=0.9560\n",
            "  PostE26: TrainL=0.1316, TrainA=0.9555\n",
            "PostVal: ValidL=0.1714, ValidA=0.9584\n",
            "Reward: 0.0901 | Penalty: 0.00 | Final Reward: 0.0901\n",
            "  *** New Best Global Accuracy! 0.9567 -> 0.9584 ***\n",
            "MetaL:19.2412(A:16.6679,C:5.1535,E:7.0412) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 38/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9584\n",
            "PreVal: ValidL=0.1763, ValidA=0.9574\n",
            "TargEdit:Typ=2 Stg=2 Src=8->Dest=9\n",
            "  TargetCNN arch changed. New Params: 14,505,866 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.2107, TrainA=0.9284\n",
            "  PostE10: TrainL=0.1883, TrainA=0.9353\n",
            "  PostE15: TrainL=0.1572, TrainA=0.9462\n",
            "  PostE20: TrainL=0.1332, TrainA=0.9527\n",
            "  PostE25: TrainL=0.1274, TrainA=0.9571\n",
            "PostVal: ValidL=0.1775, ValidA=0.9569\n",
            "Reward: -0.0501 | Penalty: 0.00 | Final Reward: -0.0501\n",
            "  No improvement for 1 iterations.\n",
            "MetaL:26.2515(A:22.4520,C:7.6049,E:5.8459) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 39/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9584\n",
            "PreVal: ValidL=0.1739, ValidA=0.9570\n",
            "TargEdit:Typ=2 Stg=1 Src=0->Dest=2\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1873, TrainA=0.9349\n",
            "  PostE10: TrainL=0.1701, TrainA=0.9413\n",
            "  PostE15: TrainL=0.1482, TrainA=0.9497\n",
            "  PostE20: TrainL=0.1270, TrainA=0.9562\n",
            "  PostE25: TrainL=0.1190, TrainA=0.9588\n",
            "PostVal: ValidL=0.1664, ValidA=0.9573\n",
            "Reward: 0.0300 | Penalty: 0.00 | Final Reward: 0.0300\n",
            "  No improvement for 2 iterations.\n",
            "MetaL:30.6602(A:24.5544,C:12.2186,E:7.0389) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 40/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9584\n",
            "PreVal: ValidL=0.1662, ValidA=0.9571\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 35,153,802 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1818, TrainA=0.9375\n",
            "  PostE10: TrainL=0.1661, TrainA=0.9434\n",
            "  PostE15: TrainL=0.1397, TrainA=0.9529\n",
            "  PostE20: TrainL=0.1242, TrainA=0.9571\n",
            "  PostE25: TrainL=0.1108, TrainA=0.9623\n",
            "PostVal: ValidL=0.1681, ValidA=0.9605\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: 0.3405 | Penalty: 50.00 | Final Reward: -49.6595\n",
            "  New best accuracy (0.9605) achieved, but REJECTED due to large negative reward (-49.66). Not saving as global best.\n",
            "MetaL:994.8757(A:-74.0099,C:2137.7737,E:2.3879) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 41/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9584\n",
            "PreVal: ValidL=0.1686, ValidA=0.9607\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 15,151,626 (Ratio: 1.04)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 26 post-edit epochs.\n",
            "  PostE5: TrainL=0.1923, TrainA=0.9342\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1729, TrainA=0.9399\n",
            "  PostE15: TrainL=0.1447, TrainA=0.9499\n",
            "  PostE20: TrainL=0.1251, TrainA=0.9569\n",
            "  PostE25: TrainL=0.1130, TrainA=0.9606\n",
            "  PostE26: TrainL=0.1126, TrainA=0.9612\n",
            "PostVal: ValidL=0.1727, ValidA=0.9591\n",
            "Reward: -0.1603 | Penalty: 0.00 | Final Reward: -0.1603\n",
            "  *** New Best Global Accuracy! 0.9584 -> 0.9591 ***\n",
            "MetaL:8.8237(A:4.0854,C:9.4791,E:2.2832) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 42/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9591\n",
            "PreVal: ValidL=0.1711, ValidA=0.9591\n",
            "TargEdit:Typ=0 Stg=2 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 15,241,994 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1895, TrainA=0.9350\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1650, TrainA=0.9434\n",
            "  PostE15: TrainL=0.1423, TrainA=0.9524\n",
            "  PostE20: TrainL=0.1145, TrainA=0.9599\n",
            "  PostE25: TrainL=0.1111, TrainA=0.9613\n",
            "PostVal: ValidL=0.1632, ValidA=0.9592\n",
            "Reward: 0.0100 | Penalty: 0.00 | Final Reward: 0.0100\n",
            "  *** New Best Global Accuracy! 0.9591 -> 0.9592 ***\n",
            "MetaL:15.1425(A:9.7504,C:10.7864,E:2.2943) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 43/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9592\n",
            "PreVal: ValidL=0.1633, ValidA=0.9602\n",
            "TargEdit:Typ=0 Stg=2 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 15,570,186 (Ratio: 1.02)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1947, TrainA=0.9328\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1658, TrainA=0.9431\n",
            "  PostE15: TrainL=0.1385, TrainA=0.9528\n",
            "  PostE20: TrainL=0.1159, TrainA=0.9602\n",
            "  PostE25: TrainL=0.1053, TrainA=0.9639\n",
            "PostVal: ValidL=0.1549, ValidA=0.9603\n",
            "Reward: 0.0100 | Penalty: 0.00 | Final Reward: 0.0100\n",
            "  *** New Best Global Accuracy! 0.9592 -> 0.9603 ***\n",
            "MetaL:15.5146(A:9.9313,C:11.1688,E:2.2562) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 44/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9603\n",
            "PreVal: ValidL=0.1560, ValidA=0.9605\n",
            "TargEdit:Typ=0 Stg=2 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 16,160,522 (Ratio: 1.04)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1852, TrainA=0.9376\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1624, TrainA=0.9437\n",
            "  PostE15: TrainL=0.1321, TrainA=0.9534\n",
            "  PostE20: TrainL=0.1093, TrainA=0.9619\n",
            "  PostE25: TrainL=0.1085, TrainA=0.9633\n",
            "PostVal: ValidL=0.1544, ValidA=0.9630\n",
            "Reward: 0.2504 | Penalty: 0.00 | Final Reward: 0.2504\n",
            "  *** New Best Global Accuracy! 0.9603 -> 0.9630 ***\n",
            "MetaL:22.2162(A:15.4276,C:13.5794,E:2.2770) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 45/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9630\n",
            "PreVal: ValidL=0.1551, ValidA=0.9617\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 36,808,458 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1669, TrainA=0.9430\n",
            "  PostE10: TrainL=0.1463, TrainA=0.9500\n",
            "  PostE15: TrainL=0.1285, TrainA=0.9561\n",
            "  PostE20: TrainL=0.1053, TrainA=0.9637\n",
            "  PostE25: TrainL=0.1022, TrainA=0.9655\n",
            "PostVal: ValidL=0.1677, ValidA=0.9578\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: -0.3906 | Penalty: 50.00 | Final Reward: -50.3906\n",
            "  No improvement for 1 iterations.\n",
            "MetaL:1038.5514(A:-60.2411,C:2197.5872,E:2.2327) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 46/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9630\n",
            "PreVal: ValidL=0.1690, ValidA=0.9577\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 36,808,458 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1611, TrainA=0.9443\n",
            "  PostE10: TrainL=0.1435, TrainA=0.9506\n",
            "  PostE15: TrainL=0.1206, TrainA=0.9580\n",
            "  PostE20: TrainL=0.1027, TrainA=0.9647\n",
            "  PostE25: TrainL=0.0990, TrainA=0.9664\n",
            "PostVal: ValidL=0.1569, ValidA=0.9622\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: 0.4507 | Penalty: 50.00 | Final Reward: -49.5493\n",
            "  No improvement for 2 iterations.\n",
            "MetaL:998.5472(A:-58.2255,C:2113.5476,E:2.1930) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 47/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9630\n",
            "PreVal: ValidL=0.1569, ValidA=0.9623\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 17,082,762 (Ratio: 1.06)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 26 post-edit epochs.\n",
            "  PostE5: TrainL=0.1702, TrainA=0.9418\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1523, TrainA=0.9485\n",
            "  PostE15: TrainL=0.1236, TrainA=0.9571\n",
            "  PostE20: TrainL=0.1056, TrainA=0.9639\n",
            "  PostE25: TrainL=0.0965, TrainA=0.9662\n",
            "  PostE26: TrainL=0.0951, TrainA=0.9667\n",
            "PostVal: ValidL=0.1569, ValidA=0.9602\n",
            "Reward: -0.2103 | Penalty: 0.00 | Final Reward: -0.2103\n",
            "  No improvement for 3 iterations.\n",
            "MetaL:14.8381(A:8.9916,C:11.6950,E:2.1825) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 48/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9630\n",
            "PreVal: ValidL=0.1558, ValidA=0.9593\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 37,730,698 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1583, TrainA=0.9462\n",
            "  PostE10: TrainL=0.1360, TrainA=0.9527\n",
            "  PostE15: TrainL=0.1166, TrainA=0.9597\n",
            "  PostE20: TrainL=0.0932, TrainA=0.9681\n",
            "  PostE25: TrainL=0.0929, TrainA=0.9679\n",
            "PostVal: ValidL=0.1597, ValidA=0.9620\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: 0.2704 | Penalty: 50.00 | Final Reward: -49.7296\n",
            "  No improvement for 4 iterations.\n",
            "MetaL:1003.6873(A:-55.9136,C:2119.2041,E:2.1687) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 49/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9630\n",
            "PreVal: ValidL=0.1621, ValidA=0.9614\n",
            "TargEdit:Typ=0 Stg=1 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 26,521,994 (Ratio: 1.55)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 38 post-edit epochs.\n",
            "  PostE5: TrainL=0.1747, TrainA=0.9396\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1768, TrainA=0.9395\n",
            "  PostE15: TrainL=0.1588, TrainA=0.9454\n",
            "  PostE20: TrainL=0.1418, TrainA=0.9507\n",
            "  PostE25: TrainL=0.1261, TrainA=0.9573\n",
            "  PostE30: TrainL=0.1084, TrainA=0.9629\n",
            "  PostE35: TrainL=0.0956, TrainA=0.9675\n",
            "  PostE38: TrainL=0.0911, TrainA=0.9685\n",
            "PostVal: ValidL=0.1546, ValidA=0.9628\n",
            "Reward: 0.1402 | Penalty: 8.51 | Final Reward: -8.3671\n",
            "  No improvement for 5 iterations.\n",
            "MetaL:-1.2575(A:-11.8907,C:21.2685,E:2.1070) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9630).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepening MetaAgentGNN: GNN Layers 2 -> 3\n",
            "MetaAgentGNN arch changed. New Params: 7267. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 50/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9630\n",
            "PreVal: ValidL=0.2060, ValidA=0.9459\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 18,373,642 (Ratio: 1.14)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 28 post-edit epochs.\n",
            "  PostE5: TrainL=0.1887, TrainA=0.9355\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1648, TrainA=0.9435\n",
            "  PostE15: TrainL=0.1356, TrainA=0.9540\n",
            "  PostE20: TrainL=0.1182, TrainA=0.9589\n",
            "  PostE25: TrainL=0.1024, TrainA=0.9644\n",
            "  PostE28: TrainL=0.1041, TrainA=0.9637\n",
            "PostVal: ValidL=0.1618, ValidA=0.9602\n",
            "Reward: 1.4323 | Penalty: 0.00 | Final Reward: 1.4323\n",
            "  No improvement for 1 iterations.\n",
            "MetaL:20.1104(A:6.4902,C:27.2425,E:2.1003) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 51/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9630\n",
            "PreVal: ValidL=0.1621, ValidA=0.9598\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 39,021,578 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1686, TrainA=0.9421\n",
            "  PostE10: TrainL=0.1454, TrainA=0.9509\n",
            "  PostE15: TrainL=0.1227, TrainA=0.9582\n",
            "  PostE20: TrainL=0.1056, TrainA=0.9636\n",
            "  PostE25: TrainL=0.0966, TrainA=0.9670\n",
            "PostVal: ValidL=0.1639, ValidA=0.9592\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: -0.0601 | Penalty: 50.00 | Final Reward: -50.0601\n",
            "  No improvement for 2 iterations.\n",
            "MetaL:1011.3019(A:-53.7448,C:2130.0955,E:2.1611) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 52/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9630\n",
            "PreVal: ValidL=0.1655, ValidA=0.9582\n",
            "TargEdit:Typ=0 Stg=1 CHM=0.5\n",
            "  TargetCNN arch changed. New Params: 22,208,522 (Ratio: 1.21)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 30 post-edit epochs.\n",
            "  PostE5: TrainL=0.1921, TrainA=0.9335\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2219, TrainA=0.9338\n",
            "  PostE15: TrainL=0.1633, TrainA=0.9427\n",
            "  PostE20: TrainL=0.1388, TrainA=0.9528\n",
            "  PostE25: TrainL=0.1175, TrainA=0.9599\n",
            "  PostE30: TrainL=0.1020, TrainA=0.9644\n",
            "PostVal: ValidL=0.1573, ValidA=0.9614\n",
            "Reward: 0.3205 | Penalty: 0.98 | Final Reward: -0.6550\n",
            "  No improvement for 3 iterations.\n",
            "MetaL:18.3560(A:12.8976,C:10.9188,E:1.9981) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 53/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9630\n",
            "PreVal: ValidL=0.1572, ValidA=0.9621\n",
            "TargEdit:Typ=0 Stg=1 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 24,568,842 (Ratio: 1.11)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.1854, TrainA=0.9362\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1648, TrainA=0.9441\n",
            "  PostE15: TrainL=0.1333, TrainA=0.9547\n",
            "  PostE20: TrainL=0.1070, TrainA=0.9628\n",
            "  PostE25: TrainL=0.0958, TrainA=0.9670\n",
            "  PostE27: TrainL=0.0989, TrainA=0.9655\n",
            "PostVal: ValidL=0.1486, ValidA=0.9640\n",
            "Reward: 0.1903 | Penalty: 4.17 | Final Reward: -3.9846\n",
            "  *** New Best Global Accuracy! 0.9630 -> 0.9640 ***\n",
            "MetaL:1.3517(A:1.1861,C:0.3334,E:2.2074) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 54/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1486, ValidA=0.9642\n",
            "TargEdit:Typ=0 Stg=2 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 25,815,050 (Ratio: 1.05)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 26 post-edit epochs.\n",
            "  PostE5: TrainL=0.1804, TrainA=0.9386\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1527, TrainA=0.9476\n",
            "  PostE15: TrainL=0.1285, TrainA=0.9562\n",
            "  PostE20: TrainL=0.1064, TrainA=0.9640\n",
            "  PostE25: TrainL=0.0960, TrainA=0.9663\n",
            "  PostE26: TrainL=0.0972, TrainA=0.9664\n",
            "PostVal: ValidL=0.1524, ValidA=0.9621\n",
            "Reward: -0.2103 | Penalty: 6.76 | Final Reward: -6.9733\n",
            "  No improvement for 1 iterations.\n",
            "MetaL:-4.4420(A:-6.2528,C:3.6235,E:1.9234) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 55/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1519, ValidA=0.9619\n",
            "TargEdit:Typ=0 Stg=1 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 28,175,370 (Ratio: 1.09)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.1805, TrainA=0.9394\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1530, TrainA=0.9469\n",
            "  PostE15: TrainL=0.1318, TrainA=0.9555\n",
            "  PostE20: TrainL=0.1083, TrainA=0.9628\n",
            "  PostE25: TrainL=0.0989, TrainA=0.9660\n",
            "  PostE27: TrainL=0.0975, TrainA=0.9670\n",
            "PostVal: ValidL=0.1466, ValidA=0.9620\n",
            "Reward: 0.0100 | Penalty: 13.37 | Final Reward: -13.3573\n",
            "  No improvement for 2 iterations.\n",
            "  !!! PUNISHMENT REVERSION: Final reward (-13.36) is below -10. Reverting current model to pre-edit architecture. !!!\n",
            "MetaL:6.2099(A:-22.9196,C:58.2609,E:1.9203) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 56/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1995, ValidA=0.9492\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 33,927,690 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1725, TrainA=0.9421\n",
            "  PostE10: TrainL=0.1399, TrainA=0.9521\n",
            "  PostE15: TrainL=0.1186, TrainA=0.9591\n",
            "  PostE20: TrainL=0.0965, TrainA=0.9669\n",
            "  PostE25: TrainL=0.0920, TrainA=0.9686\n",
            "PostVal: ValidL=0.1493, ValidA=0.9624\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: 1.3221 | Penalty: 50.00 | Final Reward: -48.6779\n",
            "  No improvement for 3 iterations.\n",
            "MetaL:870.2968(A:-39.2879,C:1819.1710,E:1.6610) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 57/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1486, ValidA=0.9628\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 31,420,426 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1523, TrainA=0.9484\n",
            "  PostE10: TrainL=0.1338, TrainA=0.9538\n",
            "  PostE15: TrainL=0.1104, TrainA=0.9615\n",
            "  PostE20: TrainL=0.0947, TrainA=0.9676\n",
            "  PostE25: TrainL=0.0885, TrainA=0.9697\n",
            "PostVal: ValidL=0.1428, ValidA=0.9640\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: 0.1202 | Penalty: 50.00 | Final Reward: -49.8798\n",
            "  No improvement for 4 iterations.\n",
            "MetaL:924.0921(A:-38.3158,C:1924.8175,E:1.5525) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 58/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1445, ValidA=0.9637\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 33,927,690 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1438, TrainA=0.9515\n",
            "  PostE10: TrainL=0.1317, TrainA=0.9571\n",
            "  PostE15: TrainL=0.1076, TrainA=0.9627\n",
            "  PostE20: TrainL=0.0937, TrainA=0.9676\n",
            "  PostE25: TrainL=0.0858, TrainA=0.9697\n",
            "PostVal: ValidL=0.1511, ValidA=0.9626\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: -0.1102 | Penalty: 50.00 | Final Reward: -50.1102\n",
            "  No improvement for 5 iterations.\n",
            "MetaL:930.0399(A:-46.6520,C:1953.3851,E:1.4982) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9640).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepening MetaAgentGNN: GNN Layers 3 -> 4\n",
            "MetaAgentGNN arch changed. New Params: 9619. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 59/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=48, Params: 9,619\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1879, ValidA=0.9527\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 28,256,522 (Ratio: 1.15)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 28 post-edit epochs.\n",
            "  PostE5: TrainL=0.1834, TrainA=0.9374\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1614, TrainA=0.9450\n",
            "  PostE15: TrainL=0.1319, TrainA=0.9543\n",
            "  PostE20: TrainL=0.1113, TrainA=0.9621\n",
            "  PostE25: TrainL=0.0959, TrainA=0.9667\n",
            "  PostE28: TrainL=0.0926, TrainA=0.9682\n",
            "PostVal: ValidL=0.1549, ValidA=0.9621\n",
            "Reward: 0.9415 | Penalty: 13.63 | Final Reward: -12.6925\n",
            "  No improvement for 1 iterations.\n",
            "  !!! PUNISHMENT REVERSION: Final reward (-12.69) is below -10. Reverting current model to pre-edit architecture. !!!\n",
            "MetaL:0.7018(A:-23.1731,C:47.7513,E:1.4721) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 60/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=48, Params: 9,619\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1834, ValidA=0.9485\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 32,681,482 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1683, TrainA=0.9416\n",
            "  PostE10: TrainL=0.1435, TrainA=0.9513\n",
            "  PostE15: TrainL=0.1232, TrainA=0.9583\n",
            "  PostE20: TrainL=0.1043, TrainA=0.9641\n",
            "  PostE25: TrainL=0.0923, TrainA=0.9685\n",
            "PostVal: ValidL=0.1541, ValidA=0.9604\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: 1.1919 | Penalty: 50.00 | Final Reward: -48.8081\n",
            "  No improvement for 2 iterations.\n",
            "MetaL:877.3643(A:-52.0829,C:1858.8961,E:1.5333) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 61/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=48, Params: 9,619\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1535, ValidA=0.9604\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 30,174,218 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1510, TrainA=0.9479\n",
            "  PostE10: TrainL=0.1334, TrainA=0.9533\n",
            "  PostE15: TrainL=0.1175, TrainA=0.9593\n",
            "  PostE20: TrainL=0.0966, TrainA=0.9675\n",
            "  PostE25: TrainL=0.0896, TrainA=0.9689\n",
            "PostVal: ValidL=0.1488, ValidA=0.9638\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: 0.3405 | Penalty: 50.00 | Final Reward: -49.6595\n",
            "  No improvement for 3 iterations.\n",
            "MetaL:937.7697(A:-35.1175,C:1945.7756,E:1.5223) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 62/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=48, Params: 9,619\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1517, ValidA=0.9630\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 32,681,482 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1468, TrainA=0.9496\n",
            "  PostE10: TrainL=0.1259, TrainA=0.9565\n",
            "  PostE15: TrainL=0.1088, TrainA=0.9624\n",
            "  PostE20: TrainL=0.0982, TrainA=0.9665\n",
            "  PostE25: TrainL=0.0865, TrainA=0.9700\n",
            "PostVal: ValidL=0.1480, ValidA=0.9641\n",
            "  Applying harsh penalty (50.00) for breaching complexity limit.\n",
            "Reward: 0.1102 | Penalty: 50.00 | Final Reward: -49.8898\n",
            "  New best accuracy (0.9641) achieved, but REJECTED due to large negative reward (-49.89). Not saving as global best.\n",
            "MetaL:929.6351(A:-52.6695,C:1964.6107,E:1.5761) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 63/120 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=48, Params: 9,619\n",
            "Current Global Best Accuracy: 0.9640\n",
            "PreVal: ValidL=0.1472, ValidA=0.9648\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "!!! COMPLEXITY LIMIT EXCEEDED: Proposed model has 32,681,482 params (> 30,000,000). Reverting edit. !!!\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1449, TrainA=0.9504\n",
            "  PostE10: TrainL=0.1245, TrainA=0.9575\n",
            "  PostE15: TrainL=0.1035, TrainA=0.9638\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# ==============================================================================\n",
        "#  ENTRY POINT 1: STAGE 1 - BROAD SEARCH AND SAVING (Saves both models)\n",
        "# ==============================================================================\n",
        "#\n",
        "if __name__ == '__main__':\n",
        "    if not PYG_AVAILABLE:\n",
        "        print(\"Exiting: PyTorch Geometric is required for this script.\")\n",
        "        exit()\n",
        "\n",
        "    torch.manual_seed(1)\n",
        "    np.random.seed(1)\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- Mount Google Drive for saving checkpoints ---\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        SAVE_DIR = \"/content/drive/My Drive/DEITI_Checkpoints\"\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab. Models will be saved locally to './checkpoints'.\")\n",
        "        SAVE_DIR = \"./checkpoints\"\n",
        "\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "    print(f\"Model checkpoints will be saved to: {SAVE_DIR}\")\n",
        "\n",
        "    deiti_system = DEITI()\n",
        "\n",
        "    # --- STAGE 1: Broad Search ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" STAGE 1: BROAD SEARCH \" + \"=\"*20)\n",
        "    try:\n",
        "        print(f\"Init TargetCNN P: {sum(p.numel() for p in deiti_system.target_cnn.parameters() if p.requires_grad):,}\")\n",
        "        print(f\"Init MetaAgentGNN: L={deiti_system.meta_agent.current_num_gnn_layers},H={deiti_system.meta_agent.current_gnn_hidden_dim},P={sum(p.numel() for p in deiti_system.meta_agent.parameters()):,}\")\n",
        "\n",
        "        print(\"\\n--- Initial Validation ---\")\n",
        "        initial_loss, initial_acc = deiti_system._validate_target(deiti_system.val_loader)\n",
        "        print(f\"InitVal: L={initial_loss:.4f},A={initial_acc:.4f}\")\n",
        "\n",
        "        deiti_system.best_global_accuracy = initial_acc\n",
        "        deiti_system.best_global_model = copy.deepcopy(deiti_system.target_cnn)\n",
        "\n",
        "        deiti_system.train_loop(iterations=120)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nFATAL RUNTIME ERROR in STAGE 1: {e}\")\n",
        "        traceback.print_exc()\n",
        "        print(\"Stopping execution.\")\n",
        "        raise e\n",
        "\n",
        "    # --- Save the Stage 1 checkpoint ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" SAVING STAGE 1 CHECKPOINT \" + \"=\"*20)\n",
        "    target_cnn_path = os.path.join(SAVE_DIR, \"target_cnn_model.pth\")\n",
        "    meta_agent_path = os.path.join(SAVE_DIR, \"meta_agent_model.pth\") # New path for meta-agent\n",
        "    acc_path = os.path.join(SAVE_DIR, \"best_accuracy.txt\")\n",
        "\n",
        "    try:\n",
        "        if deiti_system.best_global_model:\n",
        "            print(f\"Saving best TargetCNN from search with accuracy: {deiti_system.best_global_accuracy:.4f}\")\n",
        "            torch.save(deiti_system.best_global_model, target_cnn_path)\n",
        "            print(f\"Successfully saved Stage 1 best TargetCNN object to: {target_cnn_path}\")\n",
        "\n",
        "            # Save the final meta-agent from the end of the search\n",
        "            print(\"Saving final Meta-Agent state...\")\n",
        "            torch.save(deiti_system.meta_agent, meta_agent_path)\n",
        "            print(f\"Successfully saved Meta-Agent object to: {meta_agent_path}\")\n",
        "\n",
        "            # Save the best validation accuracy from the search loop\n",
        "            with open(acc_path, \"w\") as f:\n",
        "                f.write(str(deiti_system.best_global_accuracy))\n",
        "            print(f\"Saved best accuracy ({deiti_system.best_global_accuracy:.4f}) to: {acc_path}\")\n",
        "        else:\n",
        "            print(\"No best model was tracked during Stage 1. Cannot save checkpoint.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! FAILED to save checkpoint: {e} !!!\")\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dGf_cqedmfZP",
        "outputId": "00247c39-454b-467c-c492-bc696436e65f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "\n",
            "==================== LOADING PREVIOUS BEST MODELS ====================\n",
            "Successfully loaded TargetCNN from: /content/drive/My Drive/DEITI_Checkpoints/target_cnn_model.pth\n",
            "Successfully loaded Meta-Agent from: /content/drive/My Drive/DEITI_Checkpoints/meta_agent_model.pth\n",
            "Loaded previous best accuracy: 0.0000\n",
            "\n",
            "\n",
            "==================== STAGE 2: FOCUSED SEARCH ====================\n",
            "Using 4 workers for data loading (persistent: True).\n",
            "Resetting global best trackers for this search session.\n",
            "Starting Stage 2 with TargetCNN params: 373,834, Acc: 0.0000\n",
            "Continuing with Meta-Agent params: 6,073,894\n",
            "\n",
            "===== Iteration 1/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,894\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "\n",
            "--- Iteration 1: No-edit, performing full initial training. ---\n",
            "  Training for 50 post-edit epochs.\n",
            "  PostE5: TrainL=0.0000, TrainA=0.0000\n",
            "  PostE10: TrainL=0.0000, TrainA=0.0000\n",
            "  PostE15: TrainL=0.0000, TrainA=0.0000\n",
            "  PostE20: TrainL=0.0000, TrainA=0.0000\n",
            "  PostE25: TrainL=0.0000, TrainA=0.0000\n",
            "  PostE30: TrainL=0.0000, TrainA=0.0000\n",
            "  PostE35: TrainL=0.0000, TrainA=0.0000\n",
            "  PostE40: TrainL=0.0000, TrainA=0.0000\n",
            "  PostE45: TrainL=0.0000, TrainA=0.0000\n",
            "  PostE50: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "  No improvement for 1 iterations.\n",
            "\n",
            "===== Iteration 2/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,894\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 410,826 (Ratio: 1.10)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 1 post-edit epochs.\n",
            "  PostE1: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "  No improvement for 2 iterations.\n",
            "MetaL:15586907136.0000(A:0.0000,C:31173814272.0000,E:0.0000) | MetaLR: 5.00e-04\n",
            "\n",
            "===== Iteration 3/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,894\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 447,818 (Ratio: 1.09)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 1 post-edit epochs.\n",
            "  PostE1: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "  No improvement for 3 iterations.\n",
            "MetaL:25140631552.0000(A:0.0000,C:50281263104.0000,E:0.0000) | MetaLR: 5.00e-04\n",
            "\n",
            "===== Iteration 4/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,894\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 484,810 (Ratio: 1.08)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 1 post-edit epochs.\n",
            "  PostE1: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "  No improvement for 4 iterations.\n",
            "MetaL:12946610176.0000(A:0.0000,C:25893220352.0000,E:0.0000) | MetaLR: 5.00e-04\n",
            "\n",
            "===== Iteration 5/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,894\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 521,802 (Ratio: 1.08)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 1 post-edit epochs.\n",
            "  PostE1: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "  No improvement for 5 iterations.\n",
            "MetaL:69433672.0000(A:0.0000,C:138867344.0000,E:0.0000) | MetaLR: 5.00e-04\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.0000).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepened Seq head 'head_target_resize_factor'.\n",
            "MetaAgentGNN arch changed. New Params: 6073936. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 6/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,936\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 410,826 (Ratio: 1.10)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 1 post-edit epochs.\n",
            "  PostE1: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "  No improvement for 1 iterations.\n",
            "MetaL:1022660736.0000(A:0.0000,C:2045321472.0000,E:0.0000) | MetaLR: 5.00e-04\n",
            "\n",
            "===== Iteration 7/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,936\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 447,818 (Ratio: 1.09)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 1 post-edit epochs.\n",
            "  PostE1: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "  No improvement for 2 iterations.\n",
            "MetaL:58854121472.0000(A:0.0000,C:117708242944.0000,E:0.0000) | MetaLR: 5.00e-04\n",
            "\n",
            "===== Iteration 8/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,936\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 484,810 (Ratio: 1.08)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 1 post-edit epochs.\n",
            "  PostE1: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "  No improvement for 3 iterations.\n",
            "MetaL:42344775680.0000(A:0.0000,C:84689551360.0000,E:0.0000) | MetaLR: 5.00e-04\n",
            "\n",
            "===== Iteration 9/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,936\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 521,802 (Ratio: 1.08)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 1 post-edit epochs.\n",
            "  PostE1: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "  No improvement for 4 iterations.\n",
            "MetaL:8184002048.0000(A:0.0000,C:16368004096.0000,E:0.0000) | MetaLR: 5.00e-04\n",
            "\n",
            "===== Iteration 10/60 =====\n",
            "Current MetaAgentGNN: GNN Layers=10, GNN Hidden=819, Params: 6,073,936\n",
            "Current Global Best Accuracy: 0.0000\n",
            "PreVal: ValidL=0.0000, ValidA=0.0000\n",
            "TargEdit:Typ=0 Stg=0 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 558,794 (Ratio: 1.07)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 1 post-edit epochs.\n",
            "  PostE1: TrainL=0.0000, TrainA=0.0000\n",
            "PostVal: ValidL=0.0000, ValidA=0.0000\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "  No improvement for 5 iterations.\n",
            "MetaL:3670994432.0000(A:0.0000,C:7341988864.0000,E:0.0000) | MetaLR: 5.00e-04\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.0000).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Widening MetaAgentGNN: GNN Hidden Dim 819 -> 1228\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-740092303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# Run the training loop for Stage 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mdeiti_system_stage2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n\\nFATAL RUNTIME ERROR in STAGE 2: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-366476376.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self, iterations)\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvalid_growth_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                     \u001b[0mchosen_self_edit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_growth_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_meta_self_edit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_self_edit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Meta-Agent cannot grow further. No self-edit possible.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-366476376.py\u001b[0m in \u001b[0;36m_apply_meta_self_edit\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mchanged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepen_gnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0medit_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMETA_EDIT_WIDEN_GNN_HIDDEN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mchanged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiden_gnn_hidden_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMETA_GNN_WIDEN_FACTOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0medit_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMETA_EDIT_DEEPEN_MLP_HEAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mhead_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mlp_head_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-366476376.py\u001b[0m in \u001b[0;36mwiden_gnn_hidden_dim\u001b[0;34m(self, factor, device)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_gnn_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_gnn_hidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_mlp_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-366476376.py\u001b[0m in \u001b[0;36m_update_mlp_heads\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_head_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-366476376.py\u001b[0m in \u001b[0;36m_adapt_head_input\u001b[0;34m(self, head, new_input_dim)\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0mold_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_pyg\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfirst_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mold_in\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnew_input_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m                         \u001b[0mhead\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_resize_linear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_input_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-366476376.py\u001b[0m in \u001b[0;36m_resize_linear_layer\u001b[0;34m(old_linear, new_in_features, new_out_features, device)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mscaling_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_in_features\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mold_in_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_in_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_in_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mnew_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mold_in_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscaling_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mold_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#\n",
        "# ==============================================================================\n",
        "#  ENTRY POINT 2: STAGE 2 - LOADING AND FOCUSED, REPEATABLE SEARCH\n",
        "# ==============================================================================\n",
        "#\n",
        "if __name__ == '__main__':\n",
        "    if not PYG_AVAILABLE:\n",
        "        print(\"Exiting: PyTorch Geometric is required for this script.\")\n",
        "        exit()\n",
        "\n",
        "    torch.manual_seed(4) # Use a different seed for Stage 2 if desired\n",
        "    np.random.seed(4)\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- Mount Google Drive to load checkpoints ---\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        SAVE_DIR = \"/content/drive/My Drive/DEITI_Checkpoints\"\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab. Loading models from local './checkpoints'.\")\n",
        "        SAVE_DIR = \"./checkpoints\"\n",
        "\n",
        "    if not os.path.exists(SAVE_DIR):\n",
        "        print(f\"ERROR: Save directory '{SAVE_DIR}' does not exist. Cannot load model.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Define checkpoint paths ---\n",
        "    target_cnn_path = os.path.join(SAVE_DIR, \"target_cnn_model.pth\")\n",
        "    meta_agent_path = os.path.join(SAVE_DIR, \"meta_agent_model.pth\")\n",
        "    acc_path = os.path.join(SAVE_DIR, \"best_accuracy.txt\")\n",
        "\n",
        "    # --- Load the saved models and accuracy from previous run ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" LOADING PREVIOUS BEST MODELS \" + \"=\"*20)\n",
        "    best_target_cnn_previous = None\n",
        "    loaded_meta_agent = None\n",
        "    previous_best_acc = 0.0\n",
        "\n",
        "    try:\n",
        "        # Load the TargetCNN\n",
        "        best_target_cnn_previous = torch.load(target_cnn_path, map_location=DEVICE, weights_only=False)\n",
        "        best_target_cnn_previous.to(DEVICE)\n",
        "        print(f\"Successfully loaded TargetCNN from: {target_cnn_path}\")\n",
        "\n",
        "        # Load the Meta-Agent\n",
        "        loaded_meta_agent = torch.load(meta_agent_path, map_location=DEVICE, weights_only=False)\n",
        "        loaded_meta_agent.to(DEVICE)\n",
        "        print(f\"Successfully loaded Meta-Agent from: {meta_agent_path}\")\n",
        "\n",
        "        # Load the accuracy\n",
        "        with open(acc_path, \"r\") as f:\n",
        "            previous_best_acc = float(f.read())\n",
        "        print(f\"Loaded previous best accuracy: {previous_best_acc:.4f}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"!!! CHECKPOINT FILE NOT FOUND: {e}. Cannot proceed. !!!\")\n",
        "        exit()\n",
        "    except Exception as e:\n",
        "        print(f\"!!! FAILED to load checkpoint: {e} !!!\")\n",
        "        traceback.print_exc()\n",
        "        exit()\n",
        "\n",
        "    # --- STAGE 2: Focused Search ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" STAGE 2: FOCUSED SEARCH \" + \"=\"*20)\n",
        "\n",
        "    # Initialize a new DEITI system\n",
        "    deiti_system_stage2 = DEITI()\n",
        "\n",
        "    # Replace the default models with the loaded ones\n",
        "    del deiti_system_stage2.target_cnn\n",
        "    del deiti_system_stage2.meta_agent\n",
        "    gc.collect()\n",
        "    deiti_system_stage2.target_cnn = best_target_cnn_previous\n",
        "    deiti_system_stage2.meta_agent = loaded_meta_agent\n",
        "\n",
        "    # Reset optimizer, as it's tied to the new model parameters\n",
        "    deiti_system_stage2.opt_target = None\n",
        "    deiti_system_stage2.opt_meta = optim.Adam(deiti_system_stage2.meta_agent.parameters(), lr=BASE_META_LR) # Re-create optimizer for loaded agent\n",
        "\n",
        "    print(\"Resetting global best trackers for this search session.\")\n",
        "    deiti_system_stage2.best_global_accuracy = previous_best_acc\n",
        "    if deiti_system_stage2.best_global_model is not None:\n",
        "        del deiti_system_stage2.best_global_model\n",
        "    deiti_system_stage2.best_global_model = copy.deepcopy(deiti_system_stage2.target_cnn)\n",
        "\n",
        "    print(f\"Starting Stage 2 with TargetCNN params: {sum(p.numel() for p in deiti_system_stage2.target_cnn.parameters()):,}, Acc: {previous_best_acc:.4f}\")\n",
        "    print(f\"Continuing with Meta-Agent params: {sum(p.numel() for p in deiti_system_stage2.meta_agent.parameters()):,}\")\n",
        "\n",
        "    # Run the training loop for Stage 2\n",
        "    try:\n",
        "        deiti_system_stage2.train_loop(iterations=60)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nFATAL RUNTIME ERROR in STAGE 2: {e}\")\n",
        "        traceback.print_exc()\n",
        "        print(\"Stopping execution.\")\n",
        "        raise e\n",
        "\n",
        "    # --- Save the final best model and accuracy from Stage 2 ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" SAVING FINAL CHECKPOINT FOR THIS STAGE \" + \"=\"*20)\n",
        "\n",
        "    try:\n",
        "        if deiti_system_stage2.best_global_model:\n",
        "            print(f\"Saving final best TargetCNN with accuracy: {deiti_system_stage2.best_global_accuracy:.4f}\")\n",
        "            torch.save(deiti_system_stage2.best_global_model, target_cnn_path)\n",
        "            print(f\"Successfully saved final TargetCNN object to: {target_cnn_path}\")\n",
        "\n",
        "            # Save the final meta-agent state from this run\n",
        "            print(\"Saving final Meta-Agent state...\")\n",
        "            torch.save(deiti_system_stage2.meta_agent, meta_agent_path)\n",
        "            print(f\"Successfully saved Meta-Agent object to: {meta_agent_path}\")\n",
        "\n",
        "            # Overwrite the accuracy file with the new best accuracy for the next run\n",
        "            with open(acc_path, \"w\") as f:\n",
        "                f.write(str(deiti_system_stage2.best_global_accuracy))\n",
        "            print(f\"Updated accuracy file for next run: {acc_path}\")\n",
        "        else:\n",
        "            print(\"No better model was found in this run. Checkpoint remains unchanged.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! FAILED to save final checkpoint: {e} !!!\")\n",
        "        traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP4uh1n8qVaFSkFzwB7RaGT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}