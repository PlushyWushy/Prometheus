{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlushyWushy/Prometheus/blob/main/SVHN_final_Prometheus_Variation_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5m-ikijRB5E",
        "outputId": "b0e74744-ee9f-4092-f440-b530cf0c9c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: connect to drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVvNku1ioAcg",
        "outputId": "83c5aaba-3ae8-482a-81f1-e56926dec460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.6.15)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wfpE9m5oBDS",
        "outputId": "9dfe2031-d61c-4ee6-ee94-bc7d91616528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling TensorFloat32 matmul precision for supported GPU.\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.distributions import Categorical\n",
        "from torch.amp import autocast\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import math\n",
        "import inspect # For debugging\n",
        "import logging\n",
        "import gc # Garbage Collector interface\n",
        "import itertools\n",
        "import traceback\n",
        "import os\n",
        "\n",
        "# Silence verbose logs from the compiler\n",
        "logging.getLogger(\"torch._dynamo\").setLevel(logging.FATAL)\n",
        "logging.getLogger(\"torch._inductor\").setLevel(logging.FATAL)\n",
        "\n",
        "try:\n",
        "    import torch_geometric.nn as pyg_nn\n",
        "    from torch_geometric.data import Data\n",
        "    from torch_geometric.utils import to_undirected\n",
        "    from torch_geometric.nn.dense.linear import Linear as PyGLinear\n",
        "    PYG_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYG_AVAILABLE = False\n",
        "    print(\"PyTorch Geometric not found. GNN MetaAgent will not be available if used.\")\n",
        "\n",
        "# =======================================================================\n",
        "#  Constants & Configuration\n",
        "# =======================================================================\n",
        "# --- FIX 1: Set BATCHES_PER_EPOCH to None to use the full dataset ---\n",
        "PRE_EPOCHS=1; BATCHES_PER_EPOCH=None; BATCH_SIZE=128\n",
        "BASE_POST_EPOCHS = 25\n",
        "LEARNING_RATE=0.001; MAX_GRAD_NORM=5.0\n",
        "DEVICE=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if DEVICE.type == 'cuda':\n",
        "    if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
        "        print(\"Enabling TensorFloat32 matmul precision for supported GPU.\")\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# --- Action Space with Skip Connections ---\n",
        "EDIT_TYPE_ADD_CONV_BLOCK=0; EDIT_TYPE_RESIZE_LAYER=1; EDIT_TYPE_ADD_SKIP=2; EDIT_TYPE_ADD_LINEAR_BLOCK=3\n",
        "NUM_TARGET_CNN_EDIT_TYPES=4\n",
        "DISCRETE_CH_MULT_ADD=[0.5,1.0,2.0];\n",
        "DISCRETE_RESIZE_FACTORS = [0.25, 0.5, 0.75, 1.25, 1.5, 1.75]\n",
        "NUM_STAGES_TARGET_CNN=3\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "# --- MetaAgent & RL Configuration ---\n",
        "BASE_META_LR = 5e-4\n",
        "MIN_META_LR = 1e-5\n",
        "LR_ACC_BASE_THRESHOLD = 0.80\n",
        "LR_ACC_TARGET_THRESHOLD = 0.93\n",
        "\n",
        "# --- Actions for complexity reduction ---\n",
        "META_EDIT_NONE = 0\n",
        "META_EDIT_DEEPEN_GNN = 1\n",
        "META_EDIT_WIDEN_GNN_HIDDEN = 2\n",
        "META_EDIT_DEEPEN_MLP_HEAD = 3\n",
        "META_EDIT_SHRINK_GNN_HIDDEN = 4\n",
        "META_EDIT_PRUNE_GNN = 5\n",
        "META_EDIT_PRUNE_MLP_HEAD = 6\n",
        "NUM_META_SELF_EDIT_TYPES = 7\n",
        "META_SELF_EDIT_INTERVAL = 5\n",
        "\n",
        "# --- Factors and min/max constraints for two-way search ---\n",
        "META_GNN_WIDEN_FACTOR = 1.5\n",
        "META_GNN_SHRINK_FACTOR = 1.0 / META_GNN_WIDEN_FACTOR # Symmetric shrinking\n",
        "MIN_GNN_LAYERS = 1\n",
        "MIN_GNN_HIDDEN_DIM = 16\n",
        "MIN_MLP_HEAD_SEQUENTIAL_DEPTH = 1\n",
        "INITIAL_GNN_HIDDEN_DIM = 32; INITIAL_NUM_GNN_LAYERS = 2\n",
        "EDIT_TYPE_EMBED_DIM = 16\n",
        "META_AGENT_PRUNE_THRESHOLD = 15000\n",
        "\n",
        "# --- Complexity Penalty Configuration ---\n",
        "COMPLEXITY_PENALTY_THRESHOLD = 20_000_000\n",
        "COMPLEXITY_PENALTY_ALPHA = 0.2\n",
        "\n",
        "# --- Graph & State Representation ---\n",
        "OP_TYPE_IDS = {'conv2d':1,'relu':2,'maxpool2d':3, 'batchnorm2d': 4, 'batchnorm1d': 7, 'add': 8, 'input_placeholder':5, 'linear': 6, 'dropout': 9, 'none':0}\n",
        "NODE_FEATURE_DIM = 7\n",
        "NORMALIZATION_WIDTH_DIVISOR = 512.0; NORMALIZATION_IDX_DIVISOR = 50.0; NORMALIZATION_SPATIAL_DIVISOR = 32.0\n",
        "GLOBAL_SUMMARY_FEATURE_DIM = 3 + 2\n",
        "MAX_GLOBAL_HISTORY_LEN = 5\n",
        "\n",
        "# =======================================================================\n",
        "#  Custom Modules for Dynamic Graph\n",
        "# =======================================================================\n",
        "class AddWithProjection(nn.Module):\n",
        "    def __init__(self, projection_module=None):\n",
        "        super().__init__()\n",
        "        self.projection = projection_module if projection_module is not None else nn.Identity()\n",
        "    def forward(self, x_primary, x_skip):\n",
        "        x_skip_projected = self.projection(x_skip)\n",
        "        if x_primary.shape[2:] != x_skip_projected.shape[2:]:\n",
        "            x_skip_projected = F.interpolate(x_skip_projected, size=x_primary.shape[2:], mode='bilinear', align_corners=False)\n",
        "        return x_primary + x_skip_projected\n",
        "\n",
        "# =======================================================================\n",
        "#  Net2Net Utilities & Masking Helpers\n",
        "# =======================================================================\n",
        "def _valid_resize_indices(old_oc: int):\n",
        "    valid = []\n",
        "    for idx, f_resize in enumerate(DISCRETE_RESIZE_FACTORS):\n",
        "        new_oc_resize = max(1, int(round(old_oc * f_resize)))\n",
        "        if new_oc_resize != old_oc: valid.append(idx)\n",
        "    if not valid and old_oc > 0 :\n",
        "        try: valid.append(DISCRETE_RESIZE_FACTORS.index(1.0))\n",
        "        except ValueError: pass\n",
        "    if not valid: valid.append(0)\n",
        "    return valid\n",
        "def _mask_logits(logits: torch.Tensor, valid_indices: list):\n",
        "    mask_val = torch.full_like(logits, float('-inf'))\n",
        "    if valid_indices:\n",
        "        valid_indices_tensor = torch.tensor(valid_indices, device=logits.device, dtype=torch.long)\n",
        "        if valid_indices_tensor.numel() > 0:\n",
        "            valid_indices_tensor = valid_indices_tensor[valid_indices_tensor < logits.shape[-1]]\n",
        "            if valid_indices_tensor.numel() > 0:\n",
        "                 mask_val[..., valid_indices_tensor] = 0.0\n",
        "    return logits + mask_val\n",
        "\n",
        "def _invalid_meta_indices(agent):\n",
        "    invalid = []\n",
        "    agent_params = sum(p.numel() for p in agent.parameters())\n",
        "    if agent_params < META_AGENT_PRUNE_THRESHOLD:\n",
        "        invalid.extend([META_EDIT_SHRINK_GNN_HIDDEN, META_EDIT_PRUNE_GNN, META_EDIT_PRUNE_MLP_HEAD])\n",
        "\n",
        "    if agent.current_num_gnn_layers <= MIN_GNN_LAYERS:\n",
        "        invalid.append(META_EDIT_PRUNE_GNN)\n",
        "    if agent.current_gnn_hidden_dim <= MIN_GNN_HIDDEN_DIM:\n",
        "        invalid.append(META_EDIT_SHRINK_GNN_HIDDEN)\n",
        "\n",
        "    eligible_heads = agent.get_mlp_head_names()\n",
        "    all_heads_at_min_depth = all(agent.head_depth_counters.get(name, 1) <= MIN_MLP_HEAD_SEQUENTIAL_DEPTH for name in eligible_heads)\n",
        "    if all_heads_at_min_depth:\n",
        "        invalid.append(META_EDIT_PRUNE_MLP_HEAD)\n",
        "\n",
        "    all_heads_at_max_depth = all(agent.head_depth_counters.get(name, 1) >= 8 for name in eligible_heads)\n",
        "    if all_heads_at_max_depth:\n",
        "        invalid.append(META_EDIT_DEEPEN_MLP_HEAD)\n",
        "\n",
        "    return list(set(invalid))\n",
        "\n",
        "# --- FIX 2: Corrected _resize_linear_layer function ---\n",
        "def _resize_linear_layer(old_linear, new_in_features, new_out_features, device='cpu'):\n",
        "    is_pyg_linear = PYG_AVAILABLE and isinstance(old_linear, PyGLinear)\n",
        "\n",
        "    old_in_features = old_linear.in_channels if is_pyg_linear else old_linear.in_features\n",
        "    old_out_features = old_linear.out_channels if is_pyg_linear else old_linear.out_features\n",
        "\n",
        "    if old_in_features == new_in_features and old_out_features == new_out_features:\n",
        "        return old_linear\n",
        "\n",
        "    new_linear_class = PyGLinear if is_pyg_linear else nn.Linear\n",
        "    new_linear = new_linear_class(new_in_features, new_out_features, bias=(old_linear.bias is not None)).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        min_in = min(old_in_features, new_in_features)\n",
        "        min_out = min(old_out_features, new_out_features)\n",
        "\n",
        "        new_linear.weight.data[:min_out, :min_in] = old_linear.weight.data[:min_out, :min_in].clone()\n",
        "\n",
        "        if new_out_features > old_out_features and old_out_features > 0:\n",
        "            for i in range(old_out_features, new_out_features):\n",
        "                new_linear.weight.data[i, :min_in] = old_linear.weight.data[i % old_out_features, :min_in].clone()\n",
        "\n",
        "        if new_in_features > old_in_features and old_in_features > 0:\n",
        "            scaling_factor = math.sqrt(new_in_features / old_in_features)\n",
        "            for i in range(old_in_features, new_in_features):\n",
        "                new_linear.weight.data[:min_out, i] = old_linear.weight.data[:min_out, i % old_in_features].clone() / scaling_factor\n",
        "\n",
        "        if old_linear.bias is not None and new_linear.bias is not None:\n",
        "            new_linear.bias.data[:min_out] = old_linear.bias.data[:min_out].clone()\n",
        "            if new_out_features > old_out_features and old_out_features > 0:\n",
        "                for i in range(old_out_features, new_out_features):\n",
        "                    new_linear.bias.data[i] = old_linear.bias.data[i % old_out_features].clone()\n",
        "    return new_linear\n",
        "\n",
        "def net2wider_conv_output(conv: nn.Conv2d, factor: float, device='cpu') -> nn.Conv2d:\n",
        "    old_oc = conv.out_channels; new_oc = max(1, int(round(old_oc * factor)))\n",
        "    if new_oc == old_oc : return conv\n",
        "    new_conv_module = nn.Conv2d(conv.in_channels, new_oc, conv.kernel_size, stride=conv.stride, padding=conv.padding, groups=conv.groups, bias=(conv.bias is not None)).to(device)\n",
        "    with torch.no_grad():\n",
        "        new_conv_module.weight.data.fill_(0); min_oc = min(old_oc, new_oc)\n",
        "        if old_oc > 0:\n",
        "            new_conv_module.weight.data[:min_oc] = conv.weight.data[:min_oc].clone()\n",
        "            if new_oc > old_oc:\n",
        "                r_widen = float(new_oc) / old_oc\n",
        "                for i in range(old_oc, new_oc):\n",
        "                    new_conv_module.weight.data[i] = conv.weight.data[i % old_oc].clone() / math.sqrt(r_widen)\n",
        "        if conv.bias is not None and new_conv_module.bias is not None:\n",
        "            new_conv_module.bias.data.fill_(0)\n",
        "            if old_oc > 0:\n",
        "                new_conv_module.bias.data[:min_oc] = conv.bias.data[:min_oc].clone()\n",
        "                if new_oc > old_oc:\n",
        "                    for i in range(old_oc, new_oc): new_conv_module.bias.data[i] = conv.bias.data[i % old_oc].clone()\n",
        "            elif new_oc > 0: nn.init.zeros_(new_conv_module.bias.data)\n",
        "        elif new_conv_module.bias is not None: nn.init.zeros_(new_conv_module.bias.data)\n",
        "    return new_conv_module\n",
        "def net2thinner_conv_output(conv: nn.Conv2d, factor: float, device='cpu') -> nn.Conv2d:\n",
        "    old_oc = conv.out_channels; new_oc = max(1, int(round(old_oc * factor)))\n",
        "    if new_oc == old_oc: return conv\n",
        "    new_conv_module = nn.Conv2d(conv.in_channels, new_oc, conv.kernel_size, stride=conv.stride, padding=conv.padding, groups=conv.groups, bias=(conv.bias is not None)).to(device)\n",
        "    with torch.no_grad():\n",
        "        if old_oc > 0 :\n",
        "            new_conv_module.weight.data = conv.weight.data[:new_oc].clone()\n",
        "            if conv.bias is not None and new_conv_module.bias is not None:\n",
        "                 new_conv_module.bias.data = conv.bias.data[:new_oc].clone()\n",
        "            elif new_conv_module.bias is not None: nn.init.zeros_(new_conv_module.bias.data)\n",
        "    return new_conv_module\n",
        "def resize_conv_output(conv: nn.Conv2d, factor: float, device='cpu') -> nn.Conv2d:\n",
        "    if abs(factor - 1.0) < 1e-6 : return conv\n",
        "    old_oc = conv.out_channels; new_oc = max(1, int(round(old_oc * factor)))\n",
        "    if new_oc == old_oc and old_oc > 0: return conv\n",
        "    if new_oc == old_oc and old_oc == 0 and factor != 1.0 : pass\n",
        "    elif new_oc == old_oc: return conv\n",
        "    if new_oc > old_oc: return net2wider_conv_output(conv, float(new_oc)/old_oc if old_oc > 0 else factor, device)\n",
        "    else: return net2thinner_conv_output(conv, float(new_oc)/old_oc if old_oc > 0 else factor, device)\n",
        "def resize_linear_output(linear: nn.Linear, factor: float, device='cpu') -> nn.Linear:\n",
        "    if abs(factor - 1.0) < 1e-6: return linear\n",
        "    old_of = linear.out_features; new_of = max(1, int(round(old_of * factor)))\n",
        "    if new_of == old_of: return linear\n",
        "    return _resize_linear_layer(linear, linear.in_features, new_of, device)\n",
        "\n",
        "def adapt_conv_input_channels(conv: nn.Conv2d, new_in_channels: int, device='cpu') -> nn.Conv2d:\n",
        "    if conv.in_channels == new_in_channels: return conv\n",
        "    new_in_channels = max(1, new_in_channels); old_ic = conv.in_channels\n",
        "    if conv.groups > 1:\n",
        "        if new_in_channels % conv.groups != 0:\n",
        "            print(f\"  INVALID ADAPTATION: Cannot adapt grouped Conv2d to new_in_channels={new_in_channels} with groups={conv.groups}. The edit is invalid.\")\n",
        "            return None\n",
        "    new_conv_module = nn.Conv2d(new_in_channels, conv.out_channels, conv.kernel_size, stride=conv.stride, padding=conv.padding, groups=conv.groups, bias=(conv.bias is not None)).to(device)\n",
        "    with torch.no_grad():\n",
        "        if old_ic == 0: nn.init.kaiming_normal_(new_conv_module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        else:\n",
        "            w_new_conv_adapt = torch.zeros_like(new_conv_module.weight.data)\n",
        "            oc_per_group = conv.out_channels // conv.groups; new_ic_per_group = new_in_channels // conv.groups; old_ic_per_group = old_ic // conv.groups\n",
        "            for g in range(conv.groups):\n",
        "                in_start_new, in_end_new = g * new_ic_per_group, (g + 1) * new_ic_per_group\n",
        "                in_start_old, in_end_old = g * old_ic_per_group, (g + 1) * old_ic_per_group\n",
        "                out_start, out_end = g * oc_per_group, (g + 1) * oc_per_group\n",
        "                for o_idx in range(out_start, out_end):\n",
        "                    for i_idx in range(in_start_new, in_end_new):\n",
        "                        if i_idx < in_end_old: w_new_conv_adapt[o_idx, i_idx].copy_(conv.weight.data[o_idx, i_idx])\n",
        "                        else:\n",
        "                            orig_i_idx = in_start_old + (i_idx - in_start_new) % old_ic_per_group\n",
        "                            w_new_conv_adapt[o_idx, i_idx].copy_(conv.weight.data[o_idx, orig_i_idx])\n",
        "                            w_new_conv_adapt[o_idx, i_idx] /= max(1.0, new_ic_per_group / old_ic_per_group)\n",
        "            new_conv_module.weight.data.copy_(w_new_conv_adapt)\n",
        "        if conv.bias is not None and new_conv_module.bias is not None: new_conv_module.bias.data.copy_(conv.bias.data)\n",
        "        elif new_conv_module.bias is not None: nn.init.zeros_(new_conv_module.bias.data)\n",
        "    return new_conv_module\n",
        "def adapt_linear_input_features(linear: nn.Linear, new_in_features: int, device='cpu') -> nn.Linear:\n",
        "    if linear.in_features == new_in_features: return linear\n",
        "    return _resize_linear_layer(linear, new_in_features, linear.out_features, device)\n",
        "\n",
        "def adapt_batchnorm_features(bn: nn.Module, new_num_features: int, device='cpu') -> nn.Module:\n",
        "    if bn.num_features == new_num_features: return bn\n",
        "    new_bn = type(bn)(new_num_features).to(device)\n",
        "    with torch.no_grad():\n",
        "        min_feat = min(bn.num_features, new_num_features)\n",
        "        if bn.weight is not None:\n",
        "            new_bn.weight.data[:min_feat] = bn.weight.data[:min_feat].clone()\n",
        "            if new_num_features > bn.num_features:\n",
        "                new_bn.weight.data[bn.num_features:] = bn.weight.data[-1].clone()\n",
        "        if bn.bias is not None:\n",
        "            new_bn.bias.data[:min_feat] = bn.bias.data[:min_feat].clone()\n",
        "            if new_num_features > bn.num_features:\n",
        "                new_bn.bias.data[bn.num_features:] = bn.bias.data[-1].clone()\n",
        "        if bn.running_mean is not None:\n",
        "            new_bn.running_mean[:min_feat] = bn.running_mean[:min_feat].clone()\n",
        "            if new_num_features > bn.num_features:\n",
        "                new_bn.running_mean[bn.num_features:] = bn.running_mean[-1].clone()\n",
        "        if bn.running_var is not None:\n",
        "            new_bn.running_var[:min_feat] = bn.running_var[:min_feat].clone()\n",
        "            if new_num_features > bn.num_features:\n",
        "                new_bn.running_var[bn.num_features:] = bn.running_var[-1].clone()\n",
        "    return new_bn\n",
        "def net2deeper_linear_insert_identity(head_module_owner: nn.Module, head_name: str, device='cpu'):\n",
        "    if not hasattr(head_module_owner, head_name): print(f\"Err: Attr {head_name} not found for deepening\"); return False\n",
        "    original_component = getattr(head_module_owner, head_name)\n",
        "    if isinstance(original_component, nn.Linear):\n",
        "        identity_dim = original_component.out_features\n",
        "        if identity_dim <= 0: print(f\"Cannot insert identity for dim {identity_dim} in Linear head {head_name}\"); return False\n",
        "        identity_layer = nn.Linear(identity_dim, identity_dim, bias=True).to(device)\n",
        "        with torch.no_grad(): identity_layer.weight.data.copy_(torch.eye(identity_dim,device=device)); identity_layer.bias.data.fill_(0)\n",
        "        setattr(head_module_owner,head_name,nn.Sequential(original_component,identity_layer).to(device)); print(f\"  Deepened MLP head '{head_name}' (Linear -> Sequential)\")\n",
        "        return True\n",
        "    elif isinstance(original_component, nn.Sequential):\n",
        "        if not original_component or not isinstance(original_component[-1],nn.Linear): print(f\"Cannot deepen Seq head '{head_name}', last not Linear.\"); return False\n",
        "        identity_dim = original_component[-1].in_features\n",
        "        if identity_dim <= 0: print(f\"Cannot insert identity for dim {identity_dim} in Seq head {head_name}\"); return False\n",
        "        identity_layer = nn.Linear(identity_dim, identity_dim, bias=True).to(device)\n",
        "        with torch.no_grad(): identity_layer.weight.data.copy_(torch.eye(identity_dim,device=device)); identity_layer.bias.data.fill_(0)\n",
        "        new_seq_layers = nn.ModuleList([l for l in original_component[:-1]] + [identity_layer, original_component[-1]])\n",
        "        setattr(head_module_owner,head_name,nn.Sequential(*new_seq_layers).to(device)); print(f\"  Deepened Seq head '{head_name}'.\")\n",
        "        return True\n",
        "    print(f\"Err: Head '{head_name}' type {type(original_component)} not Linear/Seq for deepening.\");\n",
        "    return False\n",
        "\n",
        "def net2thinner_linear_remove_layer(head_module_owner: nn.Module, head_name: str, device='cpu'):\n",
        "    if not hasattr(head_module_owner, head_name): print(f\"Err: Attr {head_name} not found for pruning\"); return False\n",
        "    original_component = getattr(head_module_owner, head_name)\n",
        "    if not isinstance(original_component, nn.Sequential) or len(original_component) <= 2:\n",
        "        print(f\"Cannot prune head '{head_name}': not a Sequential module with more than 2 layers.\"); return False\n",
        "\n",
        "    pruned_layers = nn.ModuleList([l for l in original_component[:-2]] + [original_component[-1]])\n",
        "\n",
        "    if len(pruned_layers) == 1:\n",
        "        setattr(head_module_owner, head_name, pruned_layers[0].to(device))\n",
        "        print(f\"  Pruned MLP head '{head_name}' (Sequential -> Linear)\")\n",
        "    else:\n",
        "        setattr(head_module_owner, head_name, nn.Sequential(*pruned_layers).to(device))\n",
        "        print(f\"  Pruned MLP head '{head_name}'.\")\n",
        "    return True\n",
        "\n",
        "def _get_gcn_conv_linear_submodule(gcn_layer):\n",
        "    if hasattr(gcn_layer, 'lin') and (isinstance(gcn_layer.lin, nn.Linear) or (PYG_AVAILABLE and isinstance(gcn_layer.lin, PyGLinear))):\n",
        "        return gcn_layer.lin\n",
        "    return None\n",
        "if PYG_AVAILABLE:\n",
        "    def resize_gcn_conv_hidden(gcn_layer: pyg_nn.GCNConv, new_hidden_dim: int, prev_layer_out_dim: int, device='cpu'):\n",
        "        old_hidden_dim = gcn_layer.out_channels\n",
        "        if new_hidden_dim == old_hidden_dim: return gcn_layer, False\n",
        "        new_gcn = pyg_nn.GCNConv(prev_layer_out_dim,new_hidden_dim,bias=(gcn_layer.bias is not None), improved=gcn_layer.improved,add_self_loops=gcn_layer.add_self_loops, normalize=gcn_layer.normalize).to(device)\n",
        "        with torch.no_grad():\n",
        "            gcn_lin_original = _get_gcn_conv_linear_submodule(gcn_layer)\n",
        "            if gcn_lin_original is not None:\n",
        "                new_gcn.lin = _resize_linear_layer(gcn_lin_original, prev_layer_out_dim, new_hidden_dim, device)\n",
        "\n",
        "            if gcn_layer.bias is not None and new_gcn.bias is not None:\n",
        "                min_out = min(old_hidden_dim, new_hidden_dim)\n",
        "                new_gcn.bias.data[:min_out] = gcn_layer.bias.data[:min_out].clone()\n",
        "                if new_hidden_dim > old_hidden_dim and old_hidden_dim > 0:\n",
        "                    for i in range(old_hidden_dim, new_hidden_dim):\n",
        "                        new_gcn.bias.data[i] = gcn_layer.bias.data[i % old_hidden_dim].clone()\n",
        "        return new_gcn,True\n",
        "\n",
        "    def adapt_gcn_conv_input_dim(gcn_layer: pyg_nn.GCNConv, new_input_dim: int, device='cpu'):\n",
        "        old_input_dim = gcn_layer.in_channels\n",
        "        if new_input_dim == old_input_dim: return gcn_layer, False\n",
        "        new_gcn = pyg_nn.GCNConv(new_input_dim,gcn_layer.out_channels,bias=(gcn_layer.bias is not None), improved=gcn_layer.improved,add_self_loops=gcn_layer.add_self_loops,normalize=gcn_layer.normalize).to(device)\n",
        "        with torch.no_grad():\n",
        "            gcn_lin_original_adapt = _get_gcn_conv_linear_submodule(gcn_layer)\n",
        "            if gcn_lin_original_adapt is not None:\n",
        "                new_gcn.lin = _resize_linear_layer(gcn_lin_original_adapt, new_input_dim, gcn_layer.out_channels, device)\n",
        "\n",
        "            if gcn_layer.bias is not None and new_gcn.bias is not None:\n",
        "                new_gcn.bias.data.copy_(gcn_layer.bias.data)\n",
        "        return new_gcn, True\n",
        "\n",
        "    def create_identity_gcn_layer(dim: int, device='cpu', **gcn_kwargs):\n",
        "        identity_gcn = pyg_nn.GCNConv(dim,dim,bias=gcn_kwargs.get('bias',True), normalize=gcn_kwargs.get('normalize',True), add_self_loops=gcn_kwargs.get('add_self_loops',True), improved=gcn_kwargs.get('improved',False)).to(device)\n",
        "        with torch.no_grad():\n",
        "            gcn_lin_identity = _get_gcn_conv_linear_submodule(identity_gcn)\n",
        "            if gcn_lin_identity is not None:\n",
        "                if gcn_lin_identity.weight.shape[0] == gcn_lin_identity.weight.shape[1]:\n",
        "                    gcn_lin_identity.weight.data.copy_(torch.eye(dim,device=device))\n",
        "                else:\n",
        "                    nn.init.kaiming_uniform_(gcn_lin_identity.weight, a=math.sqrt(5))\n",
        "                if hasattr(gcn_lin_identity, 'bias') and gcn_lin_identity.bias is not None:\n",
        "                    gcn_lin_identity.bias.data.fill_(0.0)\n",
        "            if identity_gcn.bias is not None:\n",
        "                identity_gcn.bias.data.fill_(0.0)\n",
        "        return identity_gcn\n",
        "\n",
        "# =======================================================================\n",
        "#  Dynamic Models (TargetCNN)\n",
        "# =======================================================================\n",
        "class DynamicStageModule(nn.Module):\n",
        "    def __init__(self, stage_idx_dyn, initial_in_channels_dyn, initial_spatial_size_dyn, max_ops_dyn=None):\n",
        "        super().__init__()\n",
        "        self.stage_idx = stage_idx_dyn\n",
        "        self.initial_in_channels = initial_in_channels_dyn\n",
        "        self.initial_spatial_size = initial_spatial_size_dyn\n",
        "        self.max_ops = max_ops_dyn\n",
        "        self.ops = nn.ModuleList()\n",
        "        self.op_descriptions = []\n",
        "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "\n",
        "    def add_op(self, op_module_dyn, op_description_dyn, insert_at=None):\n",
        "        if self.max_ops is not None and len(self.ops) >= self.max_ops:\n",
        "            return False\n",
        "        if insert_at is None:\n",
        "            self.ops.append(op_module_dyn)\n",
        "            self.op_descriptions.append(op_description_dyn)\n",
        "        else:\n",
        "            self.ops.insert(insert_at, op_module_dyn)\n",
        "            self.op_descriptions.insert(insert_at, op_description_dyn)\n",
        "        return True\n",
        "\n",
        "    def get_op_output_properties(self, op_idx):\n",
        "        if op_idx == -1:\n",
        "            return self.initial_in_channels, self.initial_spatial_size\n",
        "        if 0 <= op_idx < len(self.op_descriptions):\n",
        "            desc = self.op_descriptions[op_idx]\n",
        "            return desc.get('out_channels', 0), desc.get('out_spatial', 0)\n",
        "        raise IndexError(f\"Operator index {op_idx} out of range for stage {self.stage_idx} with {len(self.ops)} ops.\")\n",
        "\n",
        "    def get_current_out_properties(self):\n",
        "        if not self.op_descriptions:\n",
        "            return self.initial_in_channels, self.initial_spatial_size\n",
        "        return self.get_op_output_properties(len(self.ops) - 1)\n",
        "\n",
        "    def forward(self, x_dyn):\n",
        "        outputs_history_dyn = {-1: x_dyn}\n",
        "        is_first_conv_in_model = (self.stage_idx == 0)\n",
        "\n",
        "        for i_dyn, op_desc_item_dyn in enumerate(self.op_descriptions):\n",
        "            op_module_fwd_dyn = self.ops[i_dyn]\n",
        "            input_indices_dyn = op_desc_item_dyn.get('input_indices', [-1])\n",
        "            if not isinstance(input_indices_dyn, list): input_indices_dyn = [input_indices_dyn]\n",
        "\n",
        "            current_op_inputs_dyn = []\n",
        "            for source_op_local_idx_dyn in input_indices_dyn:\n",
        "                if not (-1 <= source_op_local_idx_dyn < i_dyn):\n",
        "                     source_op_local_idx_dyn = (i_dyn - 1) if i_dyn > 0 else -1\n",
        "\n",
        "                if source_op_local_idx_dyn in outputs_history_dyn:\n",
        "                    current_op_inputs_dyn.append(outputs_history_dyn[source_op_local_idx_dyn])\n",
        "                else:\n",
        "                    default_input_key_dyn = (i_dyn-1) if i_dyn > 0 else -1\n",
        "                    current_op_inputs_dyn.append(outputs_history_dyn.get(default_input_key_dyn, x_dyn))\n",
        "            try:\n",
        "                if not current_op_inputs_dyn: op_output_dyn = op_module_fwd_dyn(x_dyn)\n",
        "                elif len(current_op_inputs_dyn) == 1: op_output_dyn = op_module_fwd_dyn(current_op_inputs_dyn[0])\n",
        "                else: op_output_dyn = op_module_fwd_dyn(*current_op_inputs_dyn)\n",
        "            except Exception as e_dyn_fwd:\n",
        "                print(f\"CRITICAL Error in DynamicStageModule op {i_dyn}, type {op_desc_item_dyn.get('type','Unknown')}, stage {self.stage_idx}: {e_dyn_fwd}\"); raise e_dyn_fwd\n",
        "\n",
        "            if self.training and False:\n",
        "                if i_dyn > 1 and 'batchnorm' in self.op_descriptions[i_dyn-1]['type'] and 'conv2d' in self.op_descriptions[i_dyn-2]['type']:\n",
        "                    if not is_first_conv_in_model:\n",
        "                        op_output_dyn = self.dropout(op_output_dyn)\n",
        "                    is_first_conv_in_model = False\n",
        "\n",
        "            outputs_history_dyn[i_dyn] = op_output_dyn\n",
        "        return outputs_history_dyn[len(self.ops)-1] if self.ops else x_dyn\n",
        "\n",
        "class TargetCNN(nn.Module):\n",
        "    def __init__(self, num_classes_cnn=10, num_stages_cnn=NUM_STAGES_TARGET_CNN, init_model_ch_cnn=64, input_spatial_size=32):\n",
        "        super().__init__()\n",
        "        self.num_stages = num_stages_cnn\n",
        "        self.stages = nn.ModuleList()\n",
        "        current_channels_cnn = 3\n",
        "        current_spatial_size = input_spatial_size\n",
        "        self.input_placeholder_desc = {'type': 'input_placeholder', 'out_channels': current_channels_cnn, 'out_spatial': current_spatial_size, 'input_indices': []}\n",
        "        stage_base_channels_cnn = [init_model_ch_cnn, init_model_ch_cnn * 2, init_model_ch_cnn * 4]\n",
        "        for i_cnn_stage in range(num_stages_cnn):\n",
        "            stage_module_cnn = DynamicStageModule(i_cnn_stage, current_channels_cnn, current_spatial_size, max_ops_dyn=None)\n",
        "            target_stage_out_channels_cnn = stage_base_channels_cnn[i_cnn_stage] if i_cnn_stage < len(stage_base_channels_cnn) else stage_base_channels_cnn[-1]\n",
        "            conv1 = nn.Conv2d(current_channels_cnn, target_stage_out_channels_cnn, 3, 1, 1, bias=False).to(DEVICE)\n",
        "            bn1 = nn.BatchNorm2d(target_stage_out_channels_cnn).to(DEVICE)\n",
        "            relu1 = nn.ReLU(inplace=False).to(DEVICE)\n",
        "            stage_module_cnn.add_op(conv1, {'type': 'conv2d', 'out_channels': target_stage_out_channels_cnn, 'out_spatial': current_spatial_size, 'input_indices': [-1]})\n",
        "            stage_module_cnn.add_op(bn1, {'type': 'batchnorm2d', 'out_channels': target_stage_out_channels_cnn, 'out_spatial': current_spatial_size, 'input_indices': [0]})\n",
        "            stage_module_cnn.add_op(relu1, {'type': 'relu', 'out_channels': target_stage_out_channels_cnn, 'out_spatial': current_spatial_size, 'input_indices': [1]})\n",
        "            current_channels_after_block_cnn, current_spatial_after_block = stage_module_cnn.get_current_out_properties()\n",
        "            if i_cnn_stage < num_stages_cnn - 1 :\n",
        "                pool_cnn = nn.MaxPool2d(2, 2).to(DEVICE)\n",
        "                current_spatial_size //= 2\n",
        "                stage_module_cnn.add_op(pool_cnn, {'type': 'maxpool2d', 'out_channels': current_channels_after_block_cnn, 'out_spatial': current_spatial_size, 'input_indices': [2]})\n",
        "            self.stages.append(stage_module_cnn)\n",
        "            current_channels_cnn, current_spatial_size = stage_module_cnn.get_current_out_properties()\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        fc_in_features_cnn, _ = self.get_last_stage_out_properties()\n",
        "        self.classifier = nn.ModuleList([nn.Linear(max(1, fc_in_features_cnn), num_classes_cnn).to(DEVICE)])\n",
        "        self.classifier_op_descriptions = [{'type': 'linear', 'out_features': num_classes_cnn, 'input_indices': [-1]}]\n",
        "    def forward(self,x_cnn_fwd):\n",
        "        for stage_cnn_fwd in self.stages: x_cnn_fwd=stage_cnn_fwd(x_cnn_fwd)\n",
        "        x_cnn_fwd = self.flatten(self.adaptive_pool(x_cnn_fwd))\n",
        "        for op_module in self.classifier:\n",
        "            x_cnn_fwd = op_module(x_cnn_fwd)\n",
        "        return x_cnn_fwd\n",
        "    def get_last_stage_out_properties(self):\n",
        "        if not self.stages:\n",
        "            return self.input_placeholder_desc['out_channels'], self.input_placeholder_desc['out_spatial']\n",
        "        return self.stages[-1].get_current_out_properties()\n",
        "    def get_classifier_in_features(self):\n",
        "        if not self.classifier: return 0\n",
        "        return self.classifier[0].in_features\n",
        "    def get_classifier_out_features(self):\n",
        "        if not self.classifier: return 0\n",
        "        return self.classifier[-1].out_features\n",
        "\n",
        "# =======================================================================\n",
        "#  GNN MetaAgent\n",
        "# =======================================================================\n",
        "if PYG_AVAILABLE:\n",
        "    class MetaAgentGNN(nn.Module):\n",
        "        def __init__(self, node_feature_dim_agent=NODE_FEATURE_DIM, initial_gnn_hidden_dim_agent=INITIAL_GNN_HIDDEN_DIM, initial_num_gnn_layers_agent=INITIAL_NUM_GNN_LAYERS,\n",
        "                     global_history_dim_flat_agent=GLOBAL_SUMMARY_FEATURE_DIM * MAX_GLOBAL_HISTORY_LEN, device_to_use_agent=DEVICE, **kwargs_agent):\n",
        "            super().__init__()\n",
        "            self.device = device_to_use_agent\n",
        "            self.node_feature_dim = node_feature_dim_agent\n",
        "            self.current_gnn_hidden_dim = initial_gnn_hidden_dim_agent\n",
        "            self.current_num_gnn_layers = initial_num_gnn_layers_agent\n",
        "            self.action_space_sizes = {k: v for k, v in kwargs_agent.items()}\n",
        "            self.head_depth_counters = {}\n",
        "            self.edit_type_embedding = nn.Embedding(self.action_space_sizes['num_edit_types'], EDIT_TYPE_EMBED_DIM).to(self.device)\n",
        "            self.policy_head_names = ['head_target_loc_stage', 'head_target_conv_ch_mult', 'head_target_resize_factor']\n",
        "            self.heads_input_dim_global_current = -1\n",
        "            self.gnn_layers = nn.ModuleList()\n",
        "            self._build_gnn_layers()\n",
        "            self._update_mlp_heads()\n",
        "\n",
        "        def _get_current_gnn_output_dim(self):\n",
        "            if self.current_num_gnn_layers == 0:\n",
        "                return self.node_feature_dim\n",
        "            return self.current_gnn_hidden_dim\n",
        "\n",
        "        def _adapt_head_input(self, head, new_input_dim):\n",
        "            if isinstance(head, (nn.Linear, PyGLinear)):\n",
        "                is_pyg = isinstance(head, PyGLinear)\n",
        "                old_in = head.in_channels if is_pyg else head.in_features\n",
        "                old_out = head.out_channels if is_pyg else head.out_features\n",
        "                if old_in != new_input_dim:\n",
        "                    return _resize_linear_layer(head, new_input_dim, old_out, self.device)\n",
        "            elif isinstance(head, nn.Sequential):\n",
        "                first_linear = head[0]\n",
        "                if isinstance(first_linear, (nn.Linear, PyGLinear)):\n",
        "                    is_pyg = isinstance(first_linear, PyGLinear)\n",
        "                    old_in = first_linear.in_channels if is_pyg else first_linear.in_features\n",
        "                    old_out = first_linear.out_channels if is_pyg else first_linear.out_features\n",
        "                    if old_in != new_input_dim:\n",
        "                        head[0] = _resize_linear_layer(first_linear, new_input_dim, old_out, self.device)\n",
        "            return head\n",
        "\n",
        "        def _update_mlp_heads(self):\n",
        "            gnn_output_dim = self._get_current_gnn_output_dim()\n",
        "            global_history_dim_flat = GLOBAL_SUMMARY_FEATURE_DIM * MAX_GLOBAL_HISTORY_LEN\n",
        "            new_base_input_dim = gnn_output_dim + global_history_dim_flat\n",
        "\n",
        "            if new_base_input_dim == self.heads_input_dim_global_current:\n",
        "                return\n",
        "\n",
        "            head_configs = {\n",
        "                'head_target_edit_type': (new_base_input_dim, self.action_space_sizes['num_edit_types']),\n",
        "                'head_meta_self_edit_type': (new_base_input_dim, NUM_META_SELF_EDIT_TYPES),\n",
        "                'head_value': (new_base_input_dim, 1),\n",
        "                'head_target_loc_stage': (new_base_input_dim + EDIT_TYPE_EMBED_DIM, self.action_space_sizes['num_stages_target']),\n",
        "                'head_target_conv_ch_mult': (new_base_input_dim + EDIT_TYPE_EMBED_DIM, len(DISCRETE_CH_MULT_ADD)),\n",
        "                'head_target_resize_factor': (new_base_input_dim + EDIT_TYPE_EMBED_DIM, len(DISCRETE_RESIZE_FACTORS)),\n",
        "                'head_resize_op_selector_scorer': (gnn_output_dim, 1),\n",
        "                'head_skip_source_scorer': (gnn_output_dim, 1),\n",
        "                'head_skip_destination_scorer': (gnn_output_dim, 1),\n",
        "            }\n",
        "\n",
        "            for name, (in_dim, out_dim) in head_configs.items():\n",
        "                if hasattr(self, name):\n",
        "                    setattr(self, name, self._adapt_head_input(getattr(self, name), in_dim))\n",
        "                else:\n",
        "                    setattr(self, name, nn.Linear(in_dim, out_dim).to(self.device))\n",
        "                    if name in self.policy_head_names:\n",
        "                        self.head_depth_counters[name] = 1\n",
        "\n",
        "            self.heads_input_dim_global_current = new_base_input_dim\n",
        "\n",
        "        def _build_gnn_layers(self):\n",
        "            current_in_dim = self.node_feature_dim\n",
        "            if self.current_num_gnn_layers > 0:\n",
        "                for _ in range(self.current_num_gnn_layers):\n",
        "                    out_dim = self.current_gnn_hidden_dim\n",
        "                    self.gnn_layers.append(pyg_nn.GCNConv(current_in_dim, out_dim, bias=True, normalize=True, add_self_loops=True).to(self.device))\n",
        "                    current_in_dim = out_dim\n",
        "\n",
        "        def get_mlp_head_names(self):\n",
        "             return self.policy_head_names\n",
        "\n",
        "        def _process_graph_and_state(self, graph_data, global_states_history_flat):\n",
        "            node_features, edge_index = graph_data.x, graph_data.edge_index\n",
        "            embeddings = node_features\n",
        "            if self.current_num_gnn_layers > 0 and graph_data.num_nodes > 0:\n",
        "                for gnn_layer in self.gnn_layers:\n",
        "                    embeddings = F.relu(gnn_layer(embeddings, edge_index))\n",
        "            elif graph_data.num_nodes == 0:\n",
        "                embeddings = torch.empty(0, self._get_current_gnn_output_dim(), device=self.device)\n",
        "            batch_vector = graph_data.batch\n",
        "            if batch_vector is None and embeddings.numel() > 0:\n",
        "                batch_vector = torch.zeros(embeddings.size(0), dtype=torch.long, device=self.device)\n",
        "            graph_embedding = pyg_nn.global_mean_pool(embeddings, batch_vector) if graph_data.num_nodes > 0 else torch.zeros(1, self._get_current_gnn_output_dim(), device=self.device)\n",
        "            if global_states_history_flat.ndim == 1: global_states_history_flat = global_states_history_flat.unsqueeze(0)\n",
        "            if graph_embedding.ndim == 1: graph_embedding = graph_embedding.unsqueeze(0)\n",
        "            combined_features = torch.cat((graph_embedding, global_states_history_flat), dim=1)\n",
        "            return combined_features, embeddings\n",
        "\n",
        "        def forward(self, graph_data, global_states_history_flat):\n",
        "            combined_features, node_embeddings = self._process_graph_and_state(graph_data, global_states_history_flat)\n",
        "            l_te = self.head_target_edit_type(combined_features)\n",
        "            l_mse = self.head_meta_self_edit_type(combined_features)\n",
        "            value_pred = self.head_value(combined_features)\n",
        "            return l_te, l_mse, value_pred, node_embeddings, combined_features\n",
        "\n",
        "        def get_conditional_logits(self, base_state_embedding, chosen_edit_type_tensor):\n",
        "            type_emb = self.edit_type_embedding(chosen_edit_type_tensor)\n",
        "            conditional_state = torch.cat([base_state_embedding, type_emb], dim=1)\n",
        "            logits = {}\n",
        "            edit_type = chosen_edit_type_tensor.item()\n",
        "            if edit_type == EDIT_TYPE_ADD_CONV_BLOCK:\n",
        "                logits['stage'] = self.head_target_loc_stage(conditional_state)\n",
        "                logits['ch_mult'] = self.head_target_conv_ch_mult(conditional_state)\n",
        "            elif edit_type == EDIT_TYPE_RESIZE_LAYER:\n",
        "                logits['stage'] = self.head_target_loc_stage(conditional_state)\n",
        "                logits['resize_factor'] = self.head_target_resize_factor(conditional_state)\n",
        "            elif edit_type == EDIT_TYPE_ADD_SKIP:\n",
        "                logits['stage'] = self.head_target_loc_stage(conditional_state)\n",
        "            return logits\n",
        "\n",
        "        def deepen_gnn(self, device='cpu'):\n",
        "            print(f\"  Deepening MetaAgentGNN: GNN Layers {self.current_num_gnn_layers} -> {self.current_num_gnn_layers + 1}\")\n",
        "            new_layer_in_dim = self._get_current_gnn_output_dim()\n",
        "            if self.current_num_gnn_layers == 0 :\n",
        "                new_gcn_layer = pyg_nn.GCNConv(self.node_feature_dim, self.current_gnn_hidden_dim, bias=True, normalize=True, add_self_loops=True).to(device)\n",
        "            else:\n",
        "                new_gcn_layer = create_identity_gcn_layer(self.current_gnn_hidden_dim, device=device)\n",
        "            self.gnn_layers.append(new_gcn_layer)\n",
        "            self.current_num_gnn_layers += 1\n",
        "            self._update_mlp_heads()\n",
        "            return True\n",
        "\n",
        "        def widen_gnn_hidden_dim(self, factor=META_GNN_WIDEN_FACTOR, device='cpu'):\n",
        "            old_dim = self.current_gnn_hidden_dim\n",
        "            new_dim = max(MIN_GNN_HIDDEN_DIM, int(round(old_dim * factor)))\n",
        "            if new_dim == old_dim or self.current_num_gnn_layers == 0: return False\n",
        "\n",
        "            print(f\"  Widening MetaAgentGNN: GNN Hidden Dim {old_dim} -> {new_dim}\")\n",
        "            new_gnn_list = nn.ModuleList()\n",
        "            current_in_dim = self.node_feature_dim\n",
        "            for i in range(self.current_num_gnn_layers):\n",
        "                original_gcn = self.gnn_layers[i]\n",
        "                final_gcn, _ = resize_gcn_conv_hidden(original_gcn, new_dim, current_in_dim, device)\n",
        "                new_gnn_list.append(final_gcn)\n",
        "                current_in_dim = new_dim\n",
        "            self.gnn_layers = new_gnn_list\n",
        "            self.current_gnn_hidden_dim = new_dim\n",
        "            self._update_mlp_heads()\n",
        "            return True\n",
        "\n",
        "        def deepen_one_mlp_head(self, head_attr_name, device='cpu'):\n",
        "            original_comp = getattr(self, head_attr_name, None)\n",
        "            if original_comp is None: return False\n",
        "            current_depth = self.head_depth_counters.get(head_attr_name, 1)\n",
        "            if current_depth >= 8: return False\n",
        "            changed = net2deeper_linear_insert_identity(self, head_attr_name, device=device)\n",
        "            if changed:\n",
        "                new_comp = getattr(self, head_attr_name)\n",
        "                self.head_depth_counters[head_attr_name] = len(new_comp) if isinstance(new_comp, nn.Sequential) else 1\n",
        "            return changed\n",
        "\n",
        "        def shrink_gnn_hidden_dim(self, factor=META_GNN_SHRINK_FACTOR, device='cpu'):\n",
        "            return self.widen_gnn_hidden_dim(factor=factor, device=device)\n",
        "\n",
        "        def prune_gnn_layer(self, device='cpu'):\n",
        "            if self.current_num_gnn_layers <= MIN_GNN_LAYERS: return False\n",
        "            print(f\"  Pruning MetaAgentGNN: GNN Layers {self.current_num_gnn_layers} -> {self.current_num_gnn_layers - 1}\")\n",
        "            self.gnn_layers.pop(-1)\n",
        "            self.current_num_gnn_layers -= 1\n",
        "            self._update_mlp_heads()\n",
        "            return True\n",
        "\n",
        "        def prune_one_mlp_head(self, head_attr_name, device='cpu'):\n",
        "            original_comp = getattr(self, head_attr_name, None)\n",
        "            if original_comp is None: return False\n",
        "            current_depth = self.head_depth_counters.get(head_attr_name, 1)\n",
        "            if current_depth <= MIN_MLP_HEAD_SEQUENTIAL_DEPTH: return False\n",
        "            changed = net2thinner_linear_remove_layer(self, head_attr_name, device=device)\n",
        "            if changed:\n",
        "                new_comp = getattr(self, head_attr_name)\n",
        "                self.head_depth_counters[head_attr_name] = len(new_comp) if isinstance(new_comp, nn.Sequential) else 1\n",
        "            return changed\n",
        "\n",
        "# =======================================================================\n",
        "#  Prometheus System\n",
        "# =======================================================================\n",
        "class DEITI:\n",
        "    def __init__(self):\n",
        "        if not PYG_AVAILABLE: raise ImportError(\"PyTorch Geometric required for Prometheus.\")\n",
        "        self.device=DEVICE\n",
        "        self.target_cnn = TargetCNN().to(DEVICE)\n",
        "        self.meta_agent = MetaAgentGNN(\n",
        "            device_to_use_agent=self.device, num_edit_types=NUM_TARGET_CNN_EDIT_TYPES, num_stages_target=NUM_STAGES_TARGET_CNN,\n",
        "            num_ch_mults=len(DISCRETE_CH_MULT_ADD), num_resize_factors=len(DISCRETE_RESIZE_FACTORS),\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        self.criterion_target=nn.CrossEntropyLoss()\n",
        "        self.global_states_history_buffer=[]\n",
        "        self.amp_scaler = torch.amp.GradScaler(enabled=(self.device.type=='cuda'))\n",
        "        self._init_dataloaders()\n",
        "        self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "        self.sched_target = CosineAnnealingLR(self.opt_target, T_max=BASE_POST_EPOCHS)\n",
        "        self.opt_meta = optim.Adam(self.meta_agent.parameters(), lr=BASE_META_LR)\n",
        "\n",
        "        self.frozen_bns = []\n",
        "        self.warmup_state = {'active': False, 'original_lr': LEARNING_RATE, 'param_ratio': 1.0}\n",
        "\n",
        "        self.best_global_accuracy = -1.0\n",
        "        self.best_global_model = None\n",
        "\n",
        "        self.iterations_without_improvement = 0\n",
        "        self.consecutive_dummy_pass_failures = 0\n",
        "\n",
        "    def _init_dataloaders(self):\n",
        "        svhn_mean, svhn_std = (0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614) # Values for SVHN\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.SVHN), # Changed policy to SVHN\n",
        "            transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(svhn_mean, svhn_std), # Use SVHN mean and std\n",
        "            transforms.RandomErasing(p=0.5, scale=(0.02, 0.2)),\n",
        "        ])\n",
        "\n",
        "        val_transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(svhn_mean, svhn_std) # Use SVHN mean and std\n",
        "        ])\n",
        "        try:\n",
        "            # Changed to SVHN dataset\n",
        "            tr_ds = torchvision.datasets.SVHN('./data', split='train', download=True, transform=train_transforms)\n",
        "            val_ds = torchvision.datasets.SVHN('./data', split='test', download=True, transform=val_transforms)\n",
        "        except Exception as e:\n",
        "            print(f\"SVHN download failed: {e}. Using FakeData.\")\n",
        "            # Fallback to FakeData if download fails\n",
        "            fake_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(svhn_mean, svhn_std)\n",
        "            ])\n",
        "            tr_ds = torchvision.datasets.FakeData(size=BATCH_SIZE*50, image_size=(3,32,32), num_classes=10, transform=fake_transform)\n",
        "            val_ds = torchvision.datasets.FakeData(size=BATCH_SIZE*20, image_size=(3,32,32), num_classes=10, transform=fake_transform)\n",
        "\n",
        "        num_workers = 4 if self.device.type == 'cuda' else 0\n",
        "        use_persistent_workers = num_workers > 0\n",
        "        print(f\"Using {num_workers} workers for data loading (persistent: {use_persistent_workers}).\")\n",
        "        self.train_loader = DataLoader(tr_ds, BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=self.device.type=='cuda', drop_last=True, persistent_workers=use_persistent_workers)\n",
        "        self.val_loader = DataLoader(val_ds, BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=self.device.type=='cuda', drop_last=True, persistent_workers=use_persistent_workers)\n",
        "\n",
        "    def _create_target_cnn_graph_data(self):\n",
        "        nodes, src, tgt, op_map, gid = {}, [], [], {}, 0\n",
        "        _, initial_spatial = self.target_cnn.input_placeholder_desc['out_channels'], self.target_cnn.input_placeholder_desc['out_spatial']\n",
        "        current_spatial_map = {-1: initial_spatial}\n",
        "        for si, sm in enumerate(self.target_cnn.stages):\n",
        "            stage_initial_spatial = current_spatial_map.get(si-1, initial_spatial)\n",
        "            op_output_props = {-1: (sm.initial_in_channels, stage_initial_spatial)}\n",
        "            for oi, od in enumerate(sm.op_descriptions):\n",
        "                op_map[(si, oi)] = gid\n",
        "                op_type_str = od.get('type', 'none')\n",
        "                input_indices = od.get('input_indices', [-1])\n",
        "                input_indices = input_indices if isinstance(input_indices, list) else [input_indices]\n",
        "                prev_op_idx = input_indices[0]\n",
        "                _, expected_in_spatial = op_output_props.get(prev_op_idx, (sm.initial_in_channels, stage_initial_spatial))\n",
        "                is_conv = 1.0 if op_type_str == 'conv2d' else 0.0\n",
        "                out_spatial = expected_in_spatial\n",
        "                stride_val = 1\n",
        "                if op_type_str == 'maxpool2d':\n",
        "                    stride_val = sm.ops[oi].stride if hasattr(sm.ops[oi], 'stride') else 1\n",
        "                    out_spatial //= stride_val\n",
        "                elif op_type_str == 'conv2d':\n",
        "                    current_op = sm.ops[oi]\n",
        "                    stride_val = current_op.stride[0] if isinstance(current_op.stride, tuple) else current_op.stride\n",
        "                    out_spatial //= stride_val\n",
        "                op_output_props[oi] = (od.get('out_channels', 0), out_spatial)\n",
        "                od['out_spatial'] = out_spatial\n",
        "                nf = [\n",
        "                    float(OP_TYPE_IDS.get(op_type_str, 0)),\n",
        "                    float(od.get('out_channels', 0)) / NORMALIZATION_WIDTH_DIVISOR,\n",
        "                    float(si) / max(1, NUM_STAGES_TARGET_CNN - 1),\n",
        "                    float(oi) / max(1, len(sm.ops) - 1),\n",
        "                    is_conv,\n",
        "                    float(stride_val -1),\n",
        "                    float(out_spatial) / NORMALIZATION_SPATIAL_DIVISOR,\n",
        "                ]\n",
        "                nodes[gid] = nf\n",
        "                gid += 1\n",
        "            if sm.op_descriptions:\n",
        "                 current_spatial_map[si] = op_output_props[len(sm.ops)-1][1]\n",
        "            else:\n",
        "                 current_spatial_map[si] = stage_initial_spatial\n",
        "        s_out_gid_map = {s: op_map.get((s, len(sm_loop.ops) - 1), -1) for s, sm_loop in enumerate(self.target_cnn.stages) if sm_loop.ops}\n",
        "        last_conv_gid = s_out_gid_map.get(len(self.target_cnn.stages) - 1, -1)\n",
        "        for oi, od in enumerate(self.target_cnn.classifier_op_descriptions):\n",
        "            op_map[('classifier', oi)] = gid\n",
        "            op_type_str = od.get('type', 'none')\n",
        "            nf = [\n",
        "                float(OP_TYPE_IDS.get(op_type_str, 0)),\n",
        "                float(od.get('out_features', 0)) / NORMALIZATION_WIDTH_DIVISOR,\n",
        "                1.0, float(oi) / max(1, len(self.target_cnn.classifier) - 1),\n",
        "                0.0, 0.0, 0.0\n",
        "            ]\n",
        "            nodes[gid] = nf\n",
        "            gid += 1\n",
        "        for si, sm in enumerate(self.target_cnn.stages):\n",
        "            for oi, od in enumerate(sm.op_descriptions):\n",
        "                cur_gid = op_map.get((si, oi), -1)\n",
        "                if cur_gid == -1: continue\n",
        "                in_ids = od.get('input_indices', [-1] if oi == 0 else [oi - 1])\n",
        "                in_ids = in_ids if isinstance(in_ids, list) else [in_ids]\n",
        "                for lsid in in_ids:\n",
        "                    if lsid == -1: sgid = s_out_gid_map.get(si - 1, -1) if si > 0 else -1\n",
        "                    else: sgid = op_map.get((si, lsid), -1)\n",
        "                    if sgid != -1: src.append(sgid); tgt.append(cur_gid)\n",
        "        prev_gid = last_conv_gid\n",
        "        for oi, od in enumerate(self.target_cnn.classifier_op_descriptions):\n",
        "            cur_gid = op_map.get(('classifier', oi), -1)\n",
        "            if cur_gid != -1 and prev_gid != -1:\n",
        "                src.append(prev_gid)\n",
        "                tgt.append(cur_gid)\n",
        "            prev_gid = cur_gid\n",
        "        x = torch.tensor([nodes[i] for i in range(gid)] if gid > 0 else [[0.] * NODE_FEATURE_DIM], dtype=torch.float32, device=self.device)\n",
        "        eidx = torch.tensor([src, tgt], dtype=torch.long, device=self.device) if src else torch.empty((2, 0), dtype=torch.long, device=self.device)\n",
        "        return Data(x=x, edge_index=eidx, batch=torch.zeros(x.size(0), dtype=torch.long, device=self.device) if x.size(0) > 0 else None), op_map, s_out_gid_map\n",
        "\n",
        "    def _ensure_target_cnn_consistency(self):\n",
        "        current_channels, current_spatial = self.target_cnn.input_placeholder_desc['out_channels'], self.target_cnn.input_placeholder_desc['out_spatial']\n",
        "        for stage_module in self.target_cnn.stages:\n",
        "            stage_module.initial_in_channels = current_channels\n",
        "            stage_module.initial_spatial_size = current_spatial\n",
        "            op_output_props = {-1: (current_channels, current_spatial)}\n",
        "            for i, op in enumerate(stage_module.ops):\n",
        "                desc = stage_module.op_descriptions[i]\n",
        "                input_indices = desc.get('input_indices', [-1]); input_indices = input_indices if isinstance(input_indices, list) else [input_indices]\n",
        "                prev_op_idx = input_indices[0] if input_indices else -1\n",
        "                expected_in_channels, expected_in_spatial = op_output_props.get(prev_op_idx, (current_channels, current_spatial))\n",
        "                new_op = None\n",
        "                if isinstance(op, nn.Conv2d):\n",
        "                    if op.in_channels != expected_in_channels: new_op = adapt_conv_input_channels(op, expected_in_channels, self.device)\n",
        "                    if new_op is None and op.in_channels != expected_in_channels: return False\n",
        "                    desc['out_channels'] = op.out_channels if new_op is None else new_op.out_channels\n",
        "                    stride_val = op.stride[0] if isinstance(op.stride, tuple) else op.stride\n",
        "                    desc['out_spatial'] = expected_in_spatial // stride_val\n",
        "                elif isinstance(op, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
        "                    if op.num_features != expected_in_channels: new_op = adapt_batchnorm_features(op, expected_in_channels, self.device)\n",
        "                    desc['out_channels'] = expected_in_channels\n",
        "                    desc['out_spatial'] = expected_in_spatial\n",
        "                elif isinstance(op, nn.ReLU):\n",
        "                    desc['out_channels'] = expected_in_channels; desc['out_spatial'] = expected_in_spatial\n",
        "                elif isinstance(op, nn.MaxPool2d):\n",
        "                    stride_val = op.stride if isinstance(op.stride, int) else op.stride[0]\n",
        "                    desc['out_channels'] = expected_in_channels; desc['out_spatial'] = expected_in_spatial // stride_val\n",
        "                elif isinstance(op, AddWithProjection):\n",
        "                    primary_ch, primary_sp = op_output_props[input_indices[0]]; skip_ch, _ = op_output_props[input_indices[1]]\n",
        "                    if primary_ch != skip_ch: op.projection = nn.Sequential(nn.Conv2d(skip_ch, primary_ch, kernel_size=1, bias=False), nn.BatchNorm2d(primary_ch)).to(self.device)\n",
        "                    else: op.projection = nn.Identity()\n",
        "                    desc['out_channels'], desc['out_spatial'] = primary_ch, primary_sp\n",
        "                if new_op is not None: stage_module.ops[i] = new_op\n",
        "                op_output_props[i] = (desc.get('out_channels'), desc.get('out_spatial'))\n",
        "            current_channels, current_spatial = stage_module.get_current_out_properties()\n",
        "        last_conv_channels, _ = self.target_cnn.get_last_stage_out_properties()\n",
        "        current_features = max(1, last_conv_channels)\n",
        "        for i, op in enumerate(self.target_cnn.classifier):\n",
        "            desc = self.target_cnn.classifier_op_descriptions[i]\n",
        "            new_op = None\n",
        "            if isinstance(op, nn.Linear):\n",
        "                if op.in_features != current_features: new_op = adapt_linear_input_features(op, current_features, self.device)\n",
        "                current_features = op.out_features if new_op is None else new_op.out_features\n",
        "                desc['out_features'] = current_features\n",
        "            elif isinstance(op, nn.BatchNorm1d):\n",
        "                if op.num_features != current_features: new_op = adapt_batchnorm_features(op, current_features, self.device)\n",
        "            elif isinstance(op, nn.ReLU):\n",
        "                pass\n",
        "            if new_op is not None: self.target_cnn.classifier[i] = new_op\n",
        "        return True\n",
        "\n",
        "    def _apply_target_cnn_edit(self, actions):\n",
        "        edit_type = actions['target_edit_type'].item(); changed = False\n",
        "        newly_added_bns = []\n",
        "\n",
        "        if edit_type == EDIT_TYPE_ADD_LINEAR_BLOCK:\n",
        "            prev_out_features = -1\n",
        "            for op in reversed(self.target_cnn.classifier[:-1]):\n",
        "                if hasattr(op, 'out_features'):\n",
        "                    prev_out_features = op.out_features\n",
        "                    break\n",
        "            if prev_out_features == -1:\n",
        "                prev_out_features = self.target_cnn.get_classifier_in_features()\n",
        "\n",
        "            new_linear = nn.Linear(prev_out_features, prev_out_features).to(self.device)\n",
        "            nn.init.eye_(new_linear.weight)\n",
        "            if new_linear.bias is not None: nn.init.zeros_(new_linear.bias)\n",
        "\n",
        "            bn = nn.BatchNorm1d(prev_out_features, momentum=0.1).to(self.device)\n",
        "            with torch.no_grad(): bn.weight.data.fill_(1.0); bn.bias.data.zero_()\n",
        "            bn.eval(); newly_added_bns.append(bn)\n",
        "\n",
        "            new_relu = nn.ReLU(inplace=False).to(self.device)\n",
        "            insert_idx = len(self.target_cnn.classifier) - 1\n",
        "            self.target_cnn.classifier.insert(insert_idx, new_linear); self.target_cnn.classifier.insert(insert_idx + 1, bn); self.target_cnn.classifier.insert(insert_idx + 2, new_relu)\n",
        "            self.target_cnn.classifier_op_descriptions.insert(insert_idx, {'type': 'linear', 'out_features': new_linear.out_features})\n",
        "            self.target_cnn.classifier_op_descriptions.insert(insert_idx + 1, {'type': 'batchnorm1d', 'out_features': new_linear.out_features})\n",
        "            self.target_cnn.classifier_op_descriptions.insert(insert_idx + 2, {'type': 'relu', 'out_features': new_linear.out_features})\n",
        "            changed = True\n",
        "\n",
        "        elif edit_type == EDIT_TYPE_ADD_CONV_BLOCK:\n",
        "            stage_idx = actions['target_loc_stage'].item()\n",
        "            if not (0 <= stage_idx < len(self.target_cnn.stages)): return False, []\n",
        "            stage = self.target_cnn.stages[stage_idx]\n",
        "            in_ch, in_sp = stage.get_current_out_properties(); in_ch = max(1, in_ch)\n",
        "\n",
        "            k = 3; s = 1\n",
        "            identity_conv = nn.Conv2d(in_ch, in_ch, k, stride=s, padding=(k-1)//2, bias=False).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                identity_conv.weight.data.zero_()\n",
        "                center = k // 2\n",
        "                for i in range(in_ch): identity_conv.weight.data[i, i, center, center] = 1.0\n",
        "\n",
        "            bn = nn.BatchNorm2d(in_ch, momentum=0.1).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                bn.weight.data.fill_(1.0)\n",
        "                bn.bias.data.zero_()\n",
        "            bn.eval()\n",
        "            newly_added_bns.append(bn)\n",
        "            relu = nn.ReLU(inplace=False).to(self.device)\n",
        "\n",
        "            insert_idx = len(stage.ops)\n",
        "            in_indices = [-1] if insert_idx == 0 else [insert_idx-1]\n",
        "            stage.add_op(identity_conv, {'type':'conv2d', 'out_channels': in_ch, 'out_spatial': in_sp, 'input_indices':in_indices}, insert_at=insert_idx)\n",
        "            stage.add_op(bn, {'type':'batchnorm2d', 'out_channels': in_ch, 'out_spatial': in_sp, 'input_indices':[insert_idx]}, insert_at=insert_idx+1)\n",
        "            stage.add_op(relu, {'type':'relu', 'out_channels': in_ch, 'out_spatial': in_sp, 'input_indices':[insert_idx+1]}, insert_at=insert_idx+2)\n",
        "\n",
        "            m = DISCRETE_CH_MULT_ADD[actions['target_conv_ch_mult_idx'].item()]\n",
        "            target_out_ch = max(1, int(round(in_ch * m)))\n",
        "\n",
        "            if in_ch != target_out_ch:\n",
        "                factor = float(target_out_ch) / in_ch\n",
        "                widened_conv = resize_conv_output(stage.ops[insert_idx], factor, self.device)\n",
        "                widened_bn = adapt_batchnorm_features(stage.ops[insert_idx+1], widened_conv.out_channels, self.device)\n",
        "\n",
        "                stage.ops[insert_idx] = widened_conv\n",
        "                stage.ops[insert_idx+1] = widened_bn\n",
        "\n",
        "                stage.op_descriptions[insert_idx]['out_channels'] = widened_conv.out_channels\n",
        "                stage.op_descriptions[insert_idx+1]['out_channels'] = widened_conv.out_channels\n",
        "                stage.op_descriptions[insert_idx+2]['out_channels'] = widened_conv.out_channels\n",
        "\n",
        "            changed = True\n",
        "\n",
        "        elif edit_type == EDIT_TYPE_RESIZE_LAYER:\n",
        "            stage_idx = actions['target_loc_stage'].item()\n",
        "            if not (0 <= stage_idx < len(self.target_cnn.stages)): return False, []\n",
        "            stage = self.target_cnn.stages[stage_idx]\n",
        "            op_idx = actions.get('target_actual_op_idx_in_stage', -1)\n",
        "            if op_idx != -1 and 0 <= op_idx < len(stage.ops):\n",
        "                op_mod = stage.ops[op_idx]\n",
        "                factor = DISCRETE_RESIZE_FACTORS[actions['target_resize_factor_idx'].item()]\n",
        "                if isinstance(op_mod, nn.Conv2d):\n",
        "                    new_op = resize_conv_output(op_mod, factor, self.device)\n",
        "                    if new_op is not op_mod: stage.ops[op_idx] = new_op; stage.op_descriptions[op_idx]['out_channels'] = new_op.out_channels; changed = True\n",
        "                elif isinstance(op_mod, nn.Linear):\n",
        "                    new_op = resize_linear_output(op_mod, factor, self.device)\n",
        "                    if new_op is not op_mod: stage.ops[op_idx] = new_op; stage.op_descriptions[op_idx]['out_features'] = new_op.out_features; changed = True\n",
        "        elif edit_type == EDIT_TYPE_ADD_SKIP:\n",
        "            stage_idx = actions['target_loc_stage'].item()\n",
        "            if not (0 <= stage_idx < len(self.target_cnn.stages)): return False, []\n",
        "            stage = self.target_cnn.stages[stage_idx]\n",
        "            source_op_idx = actions.get('source_op_idx', -1); dest_op_idx = actions.get('dest_op_idx', -1)\n",
        "            if source_op_idx != -1 and dest_op_idx != -1:\n",
        "                add_op = AddWithProjection().to(self.device)\n",
        "                _, dest_sp = stage.get_op_output_properties(dest_op_idx)\n",
        "                dest_ch, _ = stage.get_op_output_properties(dest_op_idx)\n",
        "                add_desc = {'type': 'add', 'out_channels': dest_ch, 'out_spatial': dest_sp, 'input_indices': [dest_op_idx, source_op_idx]}\n",
        "                insert_at_idx = dest_op_idx + 1\n",
        "                stage.add_op(add_op, add_desc, insert_at=insert_at_idx)\n",
        "                for i in range(insert_at_idx, len(stage.ops)):\n",
        "                    if stage.op_descriptions[i].get('input_indices') == [dest_op_idx]:\n",
        "                        stage.op_descriptions[i]['input_indices'] = [insert_at_idx]; break\n",
        "                changed = True\n",
        "\n",
        "        if changed:\n",
        "            consistency_ok = self._ensure_target_cnn_consistency()\n",
        "            return consistency_ok, newly_added_bns\n",
        "\n",
        "        return False, []\n",
        "\n",
        "    def _apply_meta_self_edit(self, action):\n",
        "        edit_type = action.item()\n",
        "        changed = False\n",
        "        if edit_type == META_EDIT_DEEPEN_GNN:\n",
        "            changed = self.meta_agent.deepen_gnn(device=self.device)\n",
        "        elif edit_type == META_EDIT_WIDEN_GNN_HIDDEN:\n",
        "            changed = self.meta_agent.widen_gnn_hidden_dim(factor=META_GNN_WIDEN_FACTOR, device=self.device)\n",
        "        elif edit_type == META_EDIT_DEEPEN_MLP_HEAD:\n",
        "            head_names = self.meta_agent.get_mlp_head_names()\n",
        "            if head_names:\n",
        "                changed = self.meta_agent.deepen_one_mlp_head(np.random.choice(head_names), device=self.device)\n",
        "        elif edit_type == META_EDIT_SHRINK_GNN_HIDDEN:\n",
        "            changed = self.meta_agent.shrink_gnn_hidden_dim(factor=META_GNN_SHRINK_FACTOR, device=self.device)\n",
        "        elif edit_type == META_EDIT_PRUNE_GNN:\n",
        "            changed = self.meta_agent.prune_gnn_layer(device=self.device)\n",
        "        elif edit_type == META_EDIT_PRUNE_MLP_HEAD:\n",
        "            head_names = self.meta_agent.get_mlp_head_names()\n",
        "            prunable_heads = [h for h in head_names if self.meta_agent.head_depth_counters.get(h, 1) > MIN_MLP_HEAD_SEQUENTIAL_DEPTH]\n",
        "            if prunable_heads:\n",
        "                changed = self.meta_agent.prune_one_mlp_head(np.random.choice(prunable_heads), device=self.device)\n",
        "\n",
        "        if changed:\n",
        "            print(f\"MetaAgentGNN arch changed. New Params: {sum(p.numel() for p in self.meta_agent.parameters())}. Re-init optimizer.\")\n",
        "            current_lr = self.opt_meta.param_groups[0]['lr']\n",
        "            del self.opt_meta\n",
        "            gc.collect(); torch.cuda.empty_cache()\n",
        "            self.opt_meta = optim.Adam(self.meta_agent.parameters(), lr=current_lr)\n",
        "        return changed\n",
        "\n",
        "    def _sanitize_bn_stats(self):\n",
        "        for m in self.target_cnn.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                m.running_mean.nan_to_num_(nan=0.0, posinf=1e4, neginf=-1e4)\n",
        "                m.running_var.nan_to_num_(nan=1.0, posinf=1e4, neginf=1e-4)\n",
        "                m.running_var.clamp_(min=1e-5)\n",
        "\n",
        "    def _train_target_one_epoch(self, loader, optimizer, scheduler, name=\"Tr\", current_epoch=0):\n",
        "        self.target_cnn.train()\n",
        "\n",
        "        warmup_total_epochs = 5\n",
        "        if self.warmup_state['active']:\n",
        "            if current_epoch < warmup_total_epochs:\n",
        "                lr_scale = (current_epoch + 1) / warmup_total_epochs\n",
        "                lr_scale /= math.sqrt(self.warmup_state['param_ratio'])\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['lr'] = self.warmup_state['original_lr'] * lr_scale\n",
        "\n",
        "            if current_epoch >= warmup_total_epochs:\n",
        "                print(f\"  Warmup complete. Restoring LR to {self.warmup_state['original_lr']:.2e}.\")\n",
        "                for g in optimizer.param_groups: g['lr'] = self.warmup_state['original_lr']\n",
        "                self.warmup_state['active'] = False\n",
        "\n",
        "        if self.frozen_bns and current_epoch >= warmup_total_epochs:\n",
        "            print(f\"  Unfreezing {len(self.frozen_bns)} new BatchNorm layers.\")\n",
        "            for bn in self.frozen_bns:\n",
        "                bn.train()\n",
        "            self.frozen_bns = []\n",
        "\n",
        "        loss_sum, n_batches, correct, total = 0, 0, 0, 0\n",
        "        data_iterator = itertools.islice(loader, BATCHES_PER_EPOCH) if BATCHES_PER_EPOCH is not None else loader\n",
        "        for x,y in data_iterator:\n",
        "            if x.size(0) <= 1: continue\n",
        "            x,y=x.to(self.device),y.to(self.device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(self.device.type,enabled=(self.device.type=='cuda')):\n",
        "                logits=self.target_cnn(x); loss = self.criterion_target(logits, y)\n",
        "\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                continue\n",
        "\n",
        "            self.amp_scaler.scale(loss).backward()\n",
        "            self.amp_scaler.unscale_(optimizer)\n",
        "            clip_grad_norm_(self.target_cnn.parameters(), MAX_GRAD_NORM)\n",
        "            self.amp_scaler.step(optimizer)\n",
        "            self.amp_scaler.update()\n",
        "\n",
        "            loss_sum+=loss.item(); _,pred=logits.max(1); total+=y.size(0); correct+=pred.eq(y).sum().item(); n_batches+=1\n",
        "\n",
        "        if scheduler and not self.warmup_state['active']:\n",
        "            scheduler.step()\n",
        "\n",
        "        self._sanitize_bn_stats()\n",
        "\n",
        "        return loss_sum/max(1,n_batches), correct/max(1,total)\n",
        "\n",
        "    def _validate_target(self, loader):\n",
        "        self.target_cnn.eval()\n",
        "        loss_sum, correct, total, n_batches = 0,0,0,0\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        data_iterator = itertools.islice(loader, BATCHES_PER_EPOCH) if BATCHES_PER_EPOCH is not None else loader\n",
        "        with torch.no_grad():\n",
        "            for x,y in data_iterator:\n",
        "                if x.size(0) <= 1: continue\n",
        "                x,y=x.to(self.device),y.to(self.device)\n",
        "                logits=self.target_cnn(x); loss=criterion(logits,y)\n",
        "                if torch.isnan(loss) or torch.isinf(loss): continue\n",
        "                loss_sum+=loss.item(); _,pred=logits.max(1); total+=y.size(0); correct+=pred.eq(y).sum().item(); n_batches+=1\n",
        "        return loss_sum/max(1,n_batches), correct/max(1,total)\n",
        "\n",
        "    def _validate_edit_with_dummy_pass(self):\n",
        "        self.target_cnn.eval()\n",
        "        try:\n",
        "            dummy_x, _ = next(iter(self.val_loader))\n",
        "            dummy_x = dummy_x.to(self.device)\n",
        "\n",
        "            with torch.no_grad(), autocast(self.device.type, enabled=(self.device.type=='cuda')):\n",
        "                output = self.target_cnn(dummy_x)\n",
        "\n",
        "            if torch.isnan(output).any() or torch.isinf(output).any():\n",
        "                print(\"  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\")\n",
        "                return False\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"  !!! Dummy pass failed with exception: {e}. Edit is invalid. !!!\")\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "        finally:\n",
        "            self.target_cnn.train()\n",
        "\n",
        "    def train_loop(self, iterations=100):\n",
        "        for itr in range(iterations):\n",
        "            print(f\"\\n===== Iteration {itr+1}/{iterations} =====\")\n",
        "            print(f\"Current MetaAgentGNN: GNN Layers={self.meta_agent.current_num_gnn_layers}, GNN Hidden={self.meta_agent.current_gnn_hidden_dim}, Params: {sum(p.numel() for p in self.meta_agent.parameters()):,}\")\n",
        "            print(f\"Current Global Best Accuracy: {self.best_global_accuracy:.4f}\")\n",
        "\n",
        "            if self.opt_target is None:\n",
        "                self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "            for pre_ep in range(PRE_EPOCHS):\n",
        "                self._train_target_one_epoch(self.train_loader, self.opt_target, None, f\"PreE{pre_ep+1}\", current_epoch=pre_ep)\n",
        "            val_loss_b, acc_b = self._validate_target(self.val_loader)\n",
        "            print(f\"PreVal: ValidL={val_loss_b:.4f}, ValidA={acc_b:.4f}\")\n",
        "\n",
        "            pre_edit_model_backup = copy.deepcopy(self.target_cnn)\n",
        "            old_params = sum(p.numel() for p in self.target_cnn.parameters() if p.requires_grad)\n",
        "\n",
        "            graph, op_map, s_out_gid_map = self._create_target_cnn_graph_data()\n",
        "            target_params_M = sum(p.numel() for p in self.target_cnn.parameters() if p.requires_grad)/1e6\n",
        "            norm_m_gl=self.meta_agent.current_num_gnn_layers / (MIN_GNN_LAYERS + 5)\n",
        "            norm_m_gh=self.meta_agent.current_gnn_hidden_dim / (MIN_GNN_HIDDEN_DIM + 256)\n",
        "            g_state=[val_loss_b,acc_b,target_params_M,norm_m_gl,norm_m_gh]; self.global_states_history_buffer.append(g_state)\n",
        "            if len(self.global_states_history_buffer)>MAX_GLOBAL_HISTORY_LEN: self.global_states_history_buffer.pop(0)\n",
        "            hist = list(self.global_states_history_buffer);\n",
        "            while len(hist)<MAX_GLOBAL_HISTORY_LEN: hist.insert(0,[0.]*GLOBAL_SUMMARY_FEATURE_DIM)\n",
        "            g_hist_flat=torch.tensor(hist,dtype=torch.float32,device=self.device).view(1,-1)\n",
        "            actions = {}; lp_t, en_t = 0.0, 0.0\n",
        "            l_te, l_mse, val_p, fne, base_state_embed = self.meta_agent(graph, g_hist_flat)\n",
        "            dist_te = Categorical(logits=l_te); s_te = dist_te.sample()\n",
        "            actions['target_edit_type'] = s_te; lp_t += dist_te.log_prob(s_te); en_t += dist_te.entropy().mean()\n",
        "            edit_type = s_te.item()\n",
        "            conditional_logits = self.meta_agent.get_conditional_logits(base_state_embed, s_te)\n",
        "            if edit_type != EDIT_TYPE_ADD_LINEAR_BLOCK:\n",
        "                l_ts = conditional_logits['stage']; dist_ts = Categorical(logits=l_ts); s_ts = dist_ts.sample()\n",
        "                actions['target_loc_stage'] = s_ts; lp_t += dist_ts.log_prob(s_ts); en_t += dist_ts.entropy().mean()\n",
        "            if edit_type == EDIT_TYPE_ADD_CONV_BLOCK:\n",
        "                l_tc = conditional_logits['ch_mult']; dist_tc = Categorical(logits=l_tc); s_tc = dist_tc.sample()\n",
        "                actions['target_conv_ch_mult_idx'] = s_tc; lp_t += dist_tc.log_prob(s_tc); en_t += dist_tc.entropy().mean()\n",
        "            elif edit_type == EDIT_TYPE_RESIZE_LAYER:\n",
        "                stage=self.target_cnn.stages[s_ts.item()]; candidates=[]\n",
        "                for oi,op in enumerate(stage.ops):\n",
        "                    if isinstance(op, (nn.Conv2d, nn.Linear)):\n",
        "                        valid_idx = _valid_resize_indices(op.out_channels if isinstance(op, nn.Conv2d) else op.out_features)\n",
        "                        if valid_idx:\n",
        "                            gid=op_map.get((s_ts.item(),oi),-1)\n",
        "                            if gid!=-1 and gid<fne.size(0): candidates.append((oi,fne[gid],valid_idx))\n",
        "                a_op_idx_rsz = -1; l_tr = conditional_logits['resize_factor']; dist_rf = Categorical(logits=l_tr)\n",
        "                if candidates:\n",
        "                    scores = self.meta_agent.head_resize_op_selector_scorer(torch.stack([e for _,e,_ in candidates])).squeeze(-1)\n",
        "                    if scores.numel() > 0:\n",
        "                        dist_co = Categorical(logits=scores); s_kth = dist_co.sample(); a_op_idx_rsz = candidates[s_kth.item()][0]\n",
        "                        lp_t += dist_co.log_prob(s_kth); en_t += dist_co.entropy().mean()\n",
        "                        valid_rf_idx = candidates[s_kth.item()][2]\n",
        "                        masked_l_tr = _mask_logits(l_tr.squeeze(0), valid_rf_idx)\n",
        "                        dist_rf = Categorical(logits=masked_l_tr if not torch.all(torch.isinf(masked_l_tr)) else l_tr)\n",
        "                s_rf_idx = dist_rf.sample(); lp_t += dist_rf.log_prob(s_rf_idx); en_t += dist_rf.entropy().mean()\n",
        "                actions['target_resize_factor_idx'] = s_rf_idx; actions['target_actual_op_idx_in_stage'] = a_op_idx_rsz\n",
        "            elif edit_type == EDIT_TYPE_ADD_SKIP:\n",
        "                stage = self.target_cnn.stages[s_ts.item()]; valid_pairs = []\n",
        "                for i in range(-1, len(stage.ops)):\n",
        "                    for j in range(i + 1, len(stage.ops)):\n",
        "                        _, src_sp = stage.get_op_output_properties(i); _, dest_sp = stage.get_op_output_properties(j)\n",
        "                        if src_sp == dest_sp: valid_pairs.append((i, j))\n",
        "                if valid_pairs:\n",
        "                    source_gids = [s_out_gid_map.get(s_ts.item() - 1, -1) if p[0] == -1 else op_map[(s_ts.item(), p[0])] for p in valid_pairs]\n",
        "                    dest_gids = [op_map[(s_ts.item(), p[1])] for p in valid_pairs]\n",
        "                    source_scores = self.meta_agent.head_skip_source_scorer(fne[source_gids]).squeeze()\n",
        "                    dest_scores = self.meta_agent.head_skip_destination_scorer(fne[dest_gids]).squeeze()\n",
        "                    if source_scores.dim() == 0: source_scores = source_scores.unsqueeze(0); dest_scores = dest_scores.unsqueeze(0)\n",
        "                    pair_scores = source_scores + dest_scores; dist_pair = Categorical(logits=pair_scores); chosen_pair_idx = dist_pair.sample()\n",
        "                    actions['source_op_idx'], actions['dest_op_idx'] = valid_pairs[chosen_pair_idx.item()]\n",
        "                    lp_t += dist_pair.log_prob(chosen_pair_idx); en_t += dist_pair.entropy()\n",
        "                else: actions['source_op_idx'] = -1; actions['dest_op_idx'] = -1\n",
        "\n",
        "            log_parts = [f\"TargEdit:Typ={edit_type}\"]\n",
        "            if 'target_loc_stage' in actions: log_parts.append(f\"Stg={actions['target_loc_stage'].item()}\")\n",
        "            if edit_type==EDIT_TYPE_ADD_CONV_BLOCK: log_parts.append(f\"CHM={DISCRETE_CH_MULT_ADD[actions['target_conv_ch_mult_idx'].item()]}\")\n",
        "            elif edit_type==EDIT_TYPE_RESIZE_LAYER: log_parts.append(f\"Op={actions.get('target_actual_op_idx_in_stage',-1)},RszF={DISCRETE_RESIZE_FACTORS[actions.get('target_resize_factor_idx', 0).item()]}\")\n",
        "            elif edit_type==EDIT_TYPE_ADD_SKIP: log_parts.append(f\"Src={actions.get('source_op_idx', -1)}->Dest={actions.get('dest_op_idx', -1)}\")\n",
        "\n",
        "            print(\" \".join(log_parts))\n",
        "\n",
        "            param_ratio = 1.0\n",
        "            t_changed, new_bns = self._apply_target_cnn_edit(actions)\n",
        "\n",
        "            if t_changed:\n",
        "                is_edit_valid = self._validate_edit_with_dummy_pass()\n",
        "                if not is_edit_valid:\n",
        "                    print(\"  Edit rolled back due to dummy pass failure.\")\n",
        "                    self.target_cnn = pre_edit_model_backup\n",
        "                    t_changed = False\n",
        "\n",
        "                    self.consecutive_dummy_pass_failures += 1\n",
        "                    if self.consecutive_dummy_pass_failures >= 3:\n",
        "                        print(\"\\n!!! INSTABILITY DETECTED: 3 consecutive edits failed dummy pass. !!!\")\n",
        "                        print(\"  Attempting to prune (simplify) Meta-Agent...\")\n",
        "\n",
        "                        pruning_actions = [META_EDIT_SHRINK_GNN_HIDDEN, META_EDIT_PRUNE_GNN, META_EDIT_PRUNE_MLP_HEAD]\n",
        "                        valid_pruning_actions = [a for a in pruning_actions if a not in _invalid_meta_indices(self.meta_agent)]\n",
        "\n",
        "                        if valid_pruning_actions:\n",
        "                            chosen_self_edit = np.random.choice(valid_pruning_actions)\n",
        "                            self._apply_meta_self_edit(torch.tensor(chosen_self_edit, device=self.device))\n",
        "                        else:\n",
        "                            print(\"  Meta-Agent is at minimum complexity or below prune threshold. No pruning possible.\")\n",
        "\n",
        "                        self.consecutive_dummy_pass_failures = 0\n",
        "                else:\n",
        "                    self.consecutive_dummy_pass_failures = 0\n",
        "\n",
        "            if t_changed:\n",
        "                del pre_edit_model_backup\n",
        "                gc.collect()\n",
        "                new_params = sum(p.numel() for p in self.target_cnn.parameters() if p.requires_grad)\n",
        "                param_ratio = new_params / max(1, old_params)\n",
        "                print(f\"  TargetCNN arch changed. New Params: {new_params:,} (Ratio: {param_ratio:.2f})\")\n",
        "\n",
        "                print(\"  Creating fresh optimizer for new architecture.\")\n",
        "                del self.opt_target, self.sched_target; gc.collect(); torch.cuda.empty_cache()\n",
        "                self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "                self.amp_scaler = torch.amp.GradScaler(enabled=(self.device.type=='cuda'))\n",
        "\n",
        "                if param_ratio > 1.2:\n",
        "                    print(f\"  Large parameter jump. Activating LR warmup.\")\n",
        "                    self.warmup_state = {'active': True, 'original_lr': LEARNING_RATE, 'param_ratio': param_ratio}\n",
        "                else:\n",
        "                    self.warmup_state['active'] = False\n",
        "                self.frozen_bns = new_bns\n",
        "            else:\n",
        "                print(\"  Edit was invalid or a no-op. Restoring pre-edit model.\")\n",
        "                self.target_cnn = pre_edit_model_backup\n",
        "\n",
        "                del self.opt_target, self.sched_target; gc.collect(); torch.cuda.empty_cache()\n",
        "                self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "                self.amp_scaler = torch.amp.GradScaler(enabled=(self.device.type=='cuda'))\n",
        "\n",
        "            post_epochs = int(BASE_POST_EPOCHS * max(1.0, param_ratio))\n",
        "            post_epochs = min(post_epochs, 100)\n",
        "            print(f\"  Training for {post_epochs} post-edit epochs.\")\n",
        "            self.sched_target = CosineAnnealingLR(self.opt_target, T_max=post_epochs)\n",
        "\n",
        "            for post_ep in range(post_epochs):\n",
        "                train_loss, train_acc = self._train_target_one_epoch(self.train_loader,self.opt_target, self.sched_target, f\"PostE{post_ep+1}\", current_epoch=post_ep)\n",
        "                if (post_ep + 1) % 5 == 0 or post_ep == post_epochs - 1:\n",
        "                    print(f\"  PostE{post_ep+1}: TrainL={train_loss:.4f}, TrainA={train_acc:.4f}\")\n",
        "            val_loss_a, acc_a = self._validate_target(self.val_loader)\n",
        "            print(f\"PostVal: ValidL={val_loss_a:.4f}, ValidA={acc_a:.4f}\")\n",
        "\n",
        "            new_best_found = False\n",
        "            if acc_a > self.best_global_accuracy:\n",
        "                print(f\"  *** New Best Global Accuracy! {self.best_global_accuracy:.4f} -> {acc_a:.4f} ***\")\n",
        "                self.best_global_accuracy = acc_a\n",
        "                if self.best_global_model is not None:\n",
        "                    del self.best_global_model\n",
        "                self.best_global_model = copy.deepcopy(self.target_cnn)\n",
        "                self.iterations_without_improvement = 0\n",
        "                new_best_found = True\n",
        "            else:\n",
        "                self.iterations_without_improvement += 1\n",
        "                print(f\"  No improvement for {self.iterations_without_improvement} iterations.\")\n",
        "\n",
        "            reward = 100 * (acc_a - acc_b)\n",
        "            penalty = 0.0\n",
        "            current_params = sum(p.numel() for p in self.target_cnn.parameters())\n",
        "            if current_params > COMPLEXITY_PENALTY_THRESHOLD:\n",
        "                excess_params_M = (current_params - COMPLEXITY_PENALTY_THRESHOLD) / 1e6\n",
        "                penalty = (excess_params_M ** 2) * COMPLEXITY_PENALTY_ALPHA\n",
        "\n",
        "            final_reward = reward - penalty\n",
        "            print(f\"Reward: {reward:.4f} | Penalty: {penalty:.2f} | Final Reward: {final_reward:.4f}\")\n",
        "\n",
        "            advantage = final_reward - val_p.detach().squeeze().item()\n",
        "            actor_loss = -(lp_t)*advantage\n",
        "            critic_loss = F.mse_loss(val_p.squeeze(), torch.tensor(final_reward, device=self.device, dtype=torch.float32))\n",
        "            meta_loss=actor_loss.mean()+ 0.5 * critic_loss - 0.0005*(en_t)\n",
        "            if torch.isnan(meta_loss)or torch.isinf(meta_loss):\n",
        "                print(\"MetaLoss NaN/Inf! Skipping meta-update.\"); gc.collect(); continue\n",
        "\n",
        "            progress = (max(0, acc_a - LR_ACC_BASE_THRESHOLD)) / max(1e-6, LR_ACC_TARGET_THRESHOLD - LR_ACC_BASE_THRESHOLD)\n",
        "            progress = min(1.0, progress)\n",
        "            new_meta_lr = BASE_META_LR - progress * (BASE_META_LR - MIN_META_LR)\n",
        "            for param_group in self.opt_meta.param_groups: param_group['lr'] = new_meta_lr\n",
        "\n",
        "            self.opt_meta.zero_grad(); meta_loss.backward(); clip_grad_norm_(self.meta_agent.parameters(),MAX_GRAD_NORM); self.opt_meta.step()\n",
        "            print(f\"MetaL:{meta_loss.item():.4f}(A:{actor_loss.mean().item():.4f},C:{critic_loss.item():.4f},E:{en_t.item():.4f}) | MetaLR: {new_meta_lr:.2e}\")\n",
        "\n",
        "            if self.iterations_without_improvement >= 5:\n",
        "                print(\"\\n!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\")\n",
        "\n",
        "                print(f\"  Reverting Target CNN to best known model (Acc: {self.best_global_accuracy:.4f}).\")\n",
        "                if self.best_global_model is not None:\n",
        "                    self.target_cnn = copy.deepcopy(self.best_global_model)\n",
        "                    del self.opt_target; gc.collect()\n",
        "                    self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "                print(\"  Attempting to upgrade (grow) Meta-Agent...\")\n",
        "                growth_actions = [META_EDIT_DEEPEN_GNN, META_EDIT_WIDEN_GNN_HIDDEN, META_EDIT_DEEPEN_MLP_HEAD]\n",
        "                valid_growth_actions = [a for a in growth_actions if a not in _invalid_meta_indices(self.meta_agent)]\n",
        "\n",
        "                if valid_growth_actions:\n",
        "                    chosen_self_edit = np.random.choice(valid_growth_actions)\n",
        "                    self._apply_meta_self_edit(torch.tensor(chosen_self_edit, device=self.device))\n",
        "                else:\n",
        "                    print(\"  Meta-Agent cannot grow further. No self-edit possible.\")\n",
        "\n",
        "                self.iterations_without_improvement = 0\n",
        "                print(\"  Stagnation counter reset. Continuing search with upgraded agent.\\n\")\n",
        "\n",
        "            revert_threshold = 0.04\n",
        "            if self.best_global_model is not None and acc_a < (self.best_global_accuracy - revert_threshold) and not new_best_found:\n",
        "                print(f\"  !!! Accuracy dropped by >{revert_threshold:.0%}. Reverting to the global best model (Acc: {self.best_global_accuracy:.4f}). !!!\")\n",
        "\n",
        "                del self.target_cnn; gc.collect()\n",
        "                self.target_cnn = copy.deepcopy(self.best_global_model)\n",
        "\n",
        "                del self.opt_target, self.sched_target; gc.collect()\n",
        "                self.opt_target = optim.AdamW(self.target_cnn.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "                self.sched_target = CosineAnnealingLR(self.opt_target, T_max=BASE_POST_EPOCHS)\n",
        "                self.amp_scaler = torch.amp.GradScaler(enabled=(self.device.type=='cuda'))\n",
        "\n",
        "            gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY3Awdagg28s",
        "outputId": "fb775d87-f4a2-42b1-b04e-8d6c469af68a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model checkpoints will be saved to: /content/drive/My Drive/DEITI_Checkpoints\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 182M/182M [00:49<00:00, 3.69MB/s]\n",
            "100%|██████████| 64.3M/64.3M [00:14<00:00, 4.30MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 4 workers for data loading (persistent: True).\n",
            "\n",
            "\n",
            "==================== STAGE 1: BROAD SEARCH ====================\n",
            "Init TargetCNN P: 373,834\n",
            "Init MetaAgentGNN: L=2,H=32,P=3,059\n",
            "\n",
            "--- Initial Validation ---\n",
            "InitVal: L=2.3076,A=0.0784\n",
            "\n",
            "===== Iteration 1/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.0784\n",
            "PreVal: ValidL=1.8913, ValidA=0.3330\n",
            "TargEdit:Typ=0 Stg=0 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 521,546 (Ratio: 1.40)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 34 post-edit epochs.\n",
            "  PostE5: TrainL=1.2183, TrainA=0.5976\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.8768, TrainA=0.7152\n",
            "  PostE15: TrainL=0.7488, TrainA=0.7586\n",
            "  PostE20: TrainL=0.6757, TrainA=0.7833\n",
            "  PostE25: TrainL=0.6255, TrainA=0.7999\n",
            "  PostE30: TrainL=0.5857, TrainA=0.8124\n",
            "  PostE34: TrainL=0.5683, TrainA=0.8178\n",
            "PostVal: ValidL=0.2447, ValidA=0.9319\n",
            "  *** New Best Global Accuracy! 0.0784 -> 0.9319 ***\n",
            "Reward: 59.8946 | Penalty: 0.00 | Final Reward: 59.8946\n",
            "MetaL:2066.0801(A:260.7235,C:3610.7163,E:3.4698) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 2/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9319\n",
            "PreVal: ValidL=0.2448, ValidA=0.9322\n",
            "TargEdit:Typ=2 Stg=1 Src=1->Dest=2\n",
            "  TargetCNN arch changed. New Params: 521,546 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.6854, TrainA=0.7808\n",
            "  PostE10: TrainL=0.6364, TrainA=0.7956\n",
            "  PostE15: TrainL=0.5966, TrainA=0.8109\n",
            "  PostE20: TrainL=0.5557, TrainA=0.8232\n",
            "  PostE25: TrainL=0.5413, TrainA=0.8283\n",
            "PostVal: ValidL=0.2374, ValidA=0.9347\n",
            "  *** New Best Global Accuracy! 0.9319 -> 0.9347 ***\n",
            "Reward: 0.2502 | Penalty: 0.00 | Final Reward: 0.2502\n",
            "MetaL:0.8006(A:0.7883,C:0.0288,E:4.2288) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 3/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9347\n",
            "PreVal: ValidL=0.2388, ValidA=0.9337\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 587,850 (Ratio: 1.13)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 28 post-edit epochs.\n",
            "  PostE5: TrainL=0.6354, TrainA=0.7976\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.5862, TrainA=0.8113\n",
            "  PostE15: TrainL=0.5414, TrainA=0.8267\n",
            "  PostE20: TrainL=0.5021, TrainA=0.8394\n",
            "  PostE25: TrainL=0.4801, TrainA=0.8464\n",
            "  PostE28: TrainL=0.4747, TrainA=0.8484\n",
            "PostVal: ValidL=0.1994, ValidA=0.9450\n",
            "  *** New Best Global Accuracy! 0.9347 -> 0.9450 ***\n",
            "Reward: 1.1238 | Penalty: 0.00 | Final Reward: 1.1238\n",
            "MetaL:2.6647(A:1.7864,C:1.7580,E:1.3778) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 4/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9450\n",
            "PreVal: ValidL=0.1997, ValidA=0.9450\n",
            "TargEdit:Typ=2 Stg=2 Src=-1->Dest=2\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.5586, TrainA=0.8205\n",
            "  PostE10: TrainL=0.5289, TrainA=0.8290\n",
            "  PostE15: TrainL=0.4875, TrainA=0.8437\n",
            "  PostE20: TrainL=0.4554, TrainA=0.8545\n",
            "  PostE25: TrainL=0.4450, TrainA=0.8575\n",
            "PostVal: ValidL=0.1912, ValidA=0.9484\n",
            "  *** New Best Global Accuracy! 0.9450 -> 0.9484 ***\n",
            "Reward: 0.3348 | Penalty: 0.00 | Final Reward: 0.3348\n",
            "MetaL:1.7224(A:1.6527,C:0.1437,E:4.2173) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 5/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9484\n",
            "PreVal: ValidL=0.1922, ValidA=0.9481\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 654,154 (Ratio: 1.11)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.5308, TrainA=0.8297\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.5010, TrainA=0.8392\n",
            "  PostE15: TrainL=0.4710, TrainA=0.8493\n",
            "  PostE20: TrainL=0.4356, TrainA=0.8612\n",
            "  PostE25: TrainL=0.4217, TrainA=0.8663\n",
            "  PostE27: TrainL=0.4201, TrainA=0.8657\n",
            "PostVal: ValidL=0.1771, ValidA=0.9521\n",
            "  *** New Best Global Accuracy! 0.9484 -> 0.9521 ***\n",
            "Reward: 0.3964 | Penalty: 0.00 | Final Reward: 0.3964\n",
            "MetaL:0.7255(A:0.5992,C:0.2540,E:1.3532) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 6/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9521\n",
            "PreVal: ValidL=0.1764, ValidA=0.9524\n",
            "TargEdit:Typ=0 Stg=1 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 801,866 (Ratio: 1.23)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 30 post-edit epochs.\n",
            "  PostE5: TrainL=0.4763, TrainA=0.8473\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.4425, TrainA=0.8592\n",
            "  PostE15: TrainL=0.4214, TrainA=0.8656\n",
            "  PostE20: TrainL=0.3842, TrainA=0.8791\n",
            "  PostE25: TrainL=0.3614, TrainA=0.8847\n",
            "  PostE30: TrainL=0.3398, TrainA=0.8916\n",
            "PostVal: ValidL=0.1515, ValidA=0.9592\n",
            "  *** New Best Global Accuracy! 0.9521 -> 0.9592 ***\n",
            "Reward: 0.6812 | Penalty: 0.00 | Final Reward: 0.6812\n",
            "MetaL:3.2356(A:2.9346,C:0.6053,E:3.3811) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 7/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9592\n",
            "PreVal: ValidL=0.1481, ValidA=0.9609\n",
            "TargEdit:Typ=0 Stg=2 CHM=0.5\n",
            "  TargetCNN arch changed. New Params: 1,064,266 (Ratio: 1.33)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 33 post-edit epochs.\n",
            "  PostE5: TrainL=0.4073, TrainA=0.8717\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.3936, TrainA=0.8755\n",
            "  PostE15: TrainL=0.3735, TrainA=0.8813\n",
            "  PostE20: TrainL=0.3455, TrainA=0.8899\n",
            "  PostE25: TrainL=0.3191, TrainA=0.9000\n",
            "  PostE30: TrainL=0.2963, TrainA=0.9078\n",
            "  PostE33: TrainL=0.2902, TrainA=0.9094\n",
            "PostVal: ValidL=0.1347, ValidA=0.9646\n",
            "  *** New Best Global Accuracy! 0.9592 -> 0.9646 ***\n",
            "Reward: 0.3656 | Penalty: 0.00 | Final Reward: 0.3656\n",
            "MetaL:1.7593(A:1.6629,C:0.1962,E:3.3747) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 8/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9646\n",
            "PreVal: ValidL=0.1346, ValidA=0.9647\n",
            "TargEdit:Typ=1 Stg=0 Op=0,RszF=1.5\n",
            "  TargetCNN arch changed. New Params: 1,102,058 (Ratio: 1.04)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.3498, TrainA=0.8895\n",
            "  PostE10: TrainL=0.3241, TrainA=0.8969\n",
            "  PostE15: TrainL=0.2983, TrainA=0.9057\n",
            "  PostE20: TrainL=0.2715, TrainA=0.9136\n",
            "  PostE25: TrainL=0.2669, TrainA=0.9163\n",
            "PostVal: ValidL=0.1306, ValidA=0.9650\n",
            "  *** New Best Global Accuracy! 0.9646 -> 0.9650 ***\n",
            "Reward: 0.0308 | Penalty: 0.00 | Final Reward: 0.0308\n",
            "MetaL:0.3561(A:0.3558,C:0.0054,E:4.8802) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 9/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9650\n",
            "PreVal: ValidL=0.1309, ValidA=0.9652\n",
            "TargEdit:Typ=1 Stg=1 Op=5,RszF=1.5\n",
            "  TargetCNN arch changed. New Params: 1,323,370 (Ratio: 1.20)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 30 post-edit epochs.\n",
            "  PostE5: TrainL=0.3223, TrainA=0.8979\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  PostE10: TrainL=0.3245, TrainA=0.8970\n",
            "  PostE15: TrainL=0.3037, TrainA=0.9037\n",
            "  PostE20: TrainL=0.2840, TrainA=0.9100\n",
            "  PostE25: TrainL=0.2641, TrainA=0.9158\n",
            "  PostE30: TrainL=0.2478, TrainA=0.9217\n",
            "PostVal: ValidL=0.1318, ValidA=0.9666\n",
            "  *** New Best Global Accuracy! 0.9650 -> 0.9666 ***\n",
            "Reward: 0.1424 | Penalty: 0.00 | Final Reward: 0.1424\n",
            "MetaL:0.6923(A:0.6842,C:0.0211,E:4.8753) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 10/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9666\n",
            "PreVal: ValidL=0.1306, ValidA=0.9666\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 1,389,674 (Ratio: 1.05)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 26 post-edit epochs.\n",
            "  PostE5: TrainL=0.3073, TrainA=0.9028\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2898, TrainA=0.9076\n",
            "  PostE15: TrainL=0.2653, TrainA=0.9162\n",
            "  PostE20: TrainL=0.2426, TrainA=0.9230\n",
            "  PostE25: TrainL=0.2317, TrainA=0.9263\n",
            "  PostE26: TrainL=0.2319, TrainA=0.9259\n",
            "PostVal: ValidL=0.1286, ValidA=0.9670\n",
            "  *** New Best Global Accuracy! 0.9666 -> 0.9670 ***\n",
            "Reward: 0.0423 | Penalty: 0.00 | Final Reward: 0.0423\n",
            "MetaL:0.0147(A:0.0153,C:0.0002,E:1.3355) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 11/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9670\n",
            "PreVal: ValidL=0.1288, ValidA=0.9673\n",
            "TargEdit:Typ=1 Stg=2 Op=3,RszF=1.75\n",
            "  TargetCNN arch changed. New Params: 1,635,626 (Ratio: 1.18)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 29 post-edit epochs.\n",
            "  PostE5: TrainL=0.2912, TrainA=0.9078\n",
            "  PostE10: TrainL=0.2775, TrainA=0.9119\n",
            "  PostE15: TrainL=0.2524, TrainA=0.9196\n",
            "  PostE20: TrainL=0.2349, TrainA=0.9238\n",
            "  PostE25: TrainL=0.2243, TrainA=0.9280\n",
            "  PostE29: TrainL=0.2230, TrainA=0.9285\n",
            "PostVal: ValidL=0.1279, ValidA=0.9677\n",
            "  *** New Best Global Accuracy! 0.9670 -> 0.9677 ***\n",
            "Reward: 0.0462 | Penalty: 0.00 | Final Reward: 0.0462\n",
            "MetaL:-0.1742(A:-0.1724,C:0.0013,E:4.8483) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 12/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9677\n",
            "PreVal: ValidL=0.1284, ValidA=0.9681\n",
            "TargEdit:Typ=1 Stg=2 Op=0,RszF=1.5\n",
            "  TargetCNN arch changed. New Params: 2,115,114 (Ratio: 1.29)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 32 post-edit epochs.\n",
            "  PostE5: TrainL=0.2695, TrainA=0.9144\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  PostE10: TrainL=0.2752, TrainA=0.9134\n",
            "  PostE15: TrainL=0.2658, TrainA=0.9155\n",
            "  PostE20: TrainL=0.2473, TrainA=0.9215\n",
            "  PostE25: TrainL=0.2282, TrainA=0.9265\n",
            "  PostE30: TrainL=0.2067, TrainA=0.9331\n",
            "  PostE32: TrainL=0.2046, TrainA=0.9346\n",
            "PostVal: ValidL=0.1324, ValidA=0.9686\n",
            "  *** New Best Global Accuracy! 0.9677 -> 0.9686 ***\n",
            "Reward: 0.0423 | Penalty: 0.00 | Final Reward: 0.0423\n",
            "MetaL:-0.2708(A:-0.2702,C:0.0036,E:4.8295) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 13/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9686\n",
            "PreVal: ValidL=0.1306, ValidA=0.9685\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 2,181,418 (Ratio: 1.03)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.2670, TrainA=0.9149\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2463, TrainA=0.9222\n",
            "  PostE15: TrainL=0.2241, TrainA=0.9275\n",
            "  PostE20: TrainL=0.2037, TrainA=0.9335\n",
            "  PostE25: TrainL=0.1950, TrainA=0.9361\n",
            "PostVal: ValidL=0.1358, ValidA=0.9677\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.0808 | Penalty: 0.00 | Final Reward: -0.0808\n",
            "MetaL:-0.2435(A:-0.2790,C:0.0723,E:1.3073) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 14/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9686\n",
            "PreVal: ValidL=0.1345, ValidA=0.9678\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 2,247,722 (Ratio: 1.03)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.2585, TrainA=0.9182\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2348, TrainA=0.9241\n",
            "  PostE15: TrainL=0.2134, TrainA=0.9320\n",
            "  PostE20: TrainL=0.1962, TrainA=0.9351\n",
            "  PostE25: TrainL=0.1857, TrainA=0.9397\n",
            "PostVal: ValidL=0.1329, ValidA=0.9692\n",
            "  *** New Best Global Accuracy! 0.9686 -> 0.9692 ***\n",
            "Reward: 0.1385 | Penalty: 0.00 | Final Reward: 0.1385\n",
            "MetaL:-0.1124(A:-0.1185,C:0.0136,E:1.2995) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 15/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9692\n",
            "PreVal: ValidL=0.1332, ValidA=0.9695\n",
            "TargEdit:Typ=1 Stg=1 Op=5,RszF=1.5\n",
            "  TargetCNN arch changed. New Params: 2,690,282 (Ratio: 1.20)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 29 post-edit epochs.\n",
            "  PostE5: TrainL=0.2489, TrainA=0.9196\n",
            "  PostE10: TrainL=0.2315, TrainA=0.9249\n",
            "  PostE15: TrainL=0.2150, TrainA=0.9303\n",
            "  PostE20: TrainL=0.1943, TrainA=0.9366\n",
            "  PostE25: TrainL=0.1798, TrainA=0.9417\n",
            "  PostE29: TrainL=0.1724, TrainA=0.9429\n",
            "PostVal: ValidL=0.1390, ValidA=0.9684\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.1078 | Penalty: 0.00 | Final Reward: -0.1078\n",
            "MetaL:-1.8684(A:-1.9475,C:0.1630,E:4.7440) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 16/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9692\n",
            "PreVal: ValidL=0.1383, ValidA=0.9678\n",
            "TargEdit:Typ=1 Stg=1 Op=0,RszF=1.5\n",
            "  TargetCNN arch changed. New Params: 2,930,026 (Ratio: 1.09)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.2309, TrainA=0.9258\n",
            "  PostE10: TrainL=0.2206, TrainA=0.9284\n",
            "  PostE15: TrainL=0.1980, TrainA=0.9352\n",
            "  PostE20: TrainL=0.1799, TrainA=0.9403\n",
            "  PostE25: TrainL=0.1703, TrainA=0.9429\n",
            "  PostE27: TrainL=0.1706, TrainA=0.9443\n",
            "PostVal: ValidL=0.1425, ValidA=0.9694\n",
            "  *** New Best Global Accuracy! 0.9692 -> 0.9694 ***\n",
            "Reward: 0.1616 | Penalty: 0.00 | Final Reward: 0.1616\n",
            "MetaL:-0.9827(A:-1.0024,C:0.0440,E:4.7049) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 17/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9694\n",
            "PreVal: ValidL=0.1418, ValidA=0.9696\n",
            "TargEdit:Typ=2 Stg=2 Src=-1->Dest=2\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.2225, TrainA=0.9275\n",
            "  PostE10: TrainL=0.2067, TrainA=0.9328\n",
            "  PostE15: TrainL=0.1891, TrainA=0.9371\n",
            "  PostE20: TrainL=0.1703, TrainA=0.9432\n",
            "  PostE25: TrainL=0.1589, TrainA=0.9479\n",
            "PostVal: ValidL=0.1476, ValidA=0.9692\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.0385 | Penalty: 0.00 | Final Reward: -0.0385\n",
            "MetaL:-2.8800(A:-2.9879,C:0.2211,E:5.3778) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 18/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9694\n",
            "PreVal: ValidL=0.1482, ValidA=0.9688\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 5,419,498 (Ratio: 1.85)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 46 post-edit epochs.\n",
            "  PostE5: TrainL=0.2284, TrainA=0.9253\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2385, TrainA=0.9238\n",
            "  PostE15: TrainL=0.2210, TrainA=0.9286\n",
            "  PostE20: TrainL=0.2063, TrainA=0.9325\n",
            "  PostE25: TrainL=0.1934, TrainA=0.9373\n",
            "  PostE30: TrainL=0.1788, TrainA=0.9406\n",
            "  PostE35: TrainL=0.1607, TrainA=0.9467\n",
            "  PostE40: TrainL=0.1464, TrainA=0.9510\n",
            "  PostE45: TrainL=0.1376, TrainA=0.9541\n",
            "  PostE46: TrainL=0.1365, TrainA=0.9548\n",
            "PostVal: ValidL=0.1563, ValidA=0.9695\n",
            "  *** New Best Global Accuracy! 0.9694 -> 0.9695 ***\n",
            "Reward: 0.0731 | Penalty: 0.00 | Final Reward: 0.0731\n",
            "MetaL:-1.0165(A:-1.0934,C:0.1569,E:3.0401) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 19/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9695\n",
            "PreVal: ValidL=0.1562, ValidA=0.9689\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 5,485,802 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.2108, TrainA=0.9340\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1850, TrainA=0.9389\n",
            "  PostE15: TrainL=0.1639, TrainA=0.9457\n",
            "  PostE20: TrainL=0.1415, TrainA=0.9526\n",
            "  PostE25: TrainL=0.1321, TrainA=0.9563\n",
            "PostVal: ValidL=0.1605, ValidA=0.9688\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.0077 | Penalty: 0.00 | Final Reward: -0.0077\n",
            "MetaL:-0.3262(A:-0.5764,C:0.5017,E:1.1737) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 20/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9695\n",
            "PreVal: ValidL=0.1590, ValidA=0.9685\n",
            "TargEdit:Typ=2 Stg=0 Src=4->Dest=5\n",
            "  TargetCNN arch changed. New Params: 5,485,802 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1916, TrainA=0.9368\n",
            "  PostE10: TrainL=0.1783, TrainA=0.9413\n",
            "  PostE15: TrainL=0.1546, TrainA=0.9496\n",
            "  PostE20: TrainL=0.1351, TrainA=0.9556\n",
            "  PostE25: TrainL=0.1293, TrainA=0.9564\n",
            "PostVal: ValidL=0.1629, ValidA=0.9694\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.0885 | Penalty: 0.00 | Final Reward: 0.0885\n",
            "MetaL:-6.2167(A:-6.6468,C:0.8648,E:4.7118) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 21/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9695\n",
            "PreVal: ValidL=0.1574, ValidA=0.9693\n",
            "TargEdit:Typ=0 Stg=1 CHM=0.5\n",
            "  TargetCNN arch changed. New Params: 5,984,042 (Ratio: 1.09)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.2141, TrainA=0.9315\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1847, TrainA=0.9386\n",
            "  PostE15: TrainL=0.1571, TrainA=0.9480\n",
            "  PostE20: TrainL=0.1403, TrainA=0.9543\n",
            "  PostE25: TrainL=0.1271, TrainA=0.9575\n",
            "  PostE27: TrainL=0.1245, TrainA=0.9579\n",
            "PostVal: ValidL=0.1539, ValidA=0.9699\n",
            "  *** New Best Global Accuracy! 0.9695 -> 0.9699 ***\n",
            "Reward: 0.0577 | Penalty: 0.00 | Final Reward: 0.0577\n",
            "MetaL:-3.9994(A:-4.4840,C:0.9719,E:2.5885) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 22/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9699\n",
            "PreVal: ValidL=0.1523, ValidA=0.9704\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 6,050,346 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1926, TrainA=0.9374\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1838, TrainA=0.9423\n",
            "  PostE15: TrainL=0.1500, TrainA=0.9503\n",
            "  PostE20: TrainL=0.1299, TrainA=0.9571\n",
            "  PostE25: TrainL=0.1166, TrainA=0.9618\n",
            "PostVal: ValidL=0.1550, ValidA=0.9682\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.2232 | Penalty: 0.00 | Final Reward: -0.2232\n",
            "MetaL:0.4148(A:-0.7543,C:2.3393,E:0.9841) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 23/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9699\n",
            "PreVal: ValidL=0.1551, ValidA=0.9690\n",
            "TargEdit:Typ=1 Stg=1 Op=11,RszF=1.25\n",
            "  TargetCNN arch changed. New Params: 6,672,570 (Ratio: 1.10)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.1806, TrainA=0.9429\n",
            "  PostE10: TrainL=0.1693, TrainA=0.9448\n",
            "  PostE15: TrainL=0.1414, TrainA=0.9541\n",
            "  PostE20: TrainL=0.1218, TrainA=0.9585\n",
            "  PostE25: TrainL=0.1128, TrainA=0.9624\n",
            "  PostE27: TrainL=0.1087, TrainA=0.9640\n",
            "PostVal: ValidL=0.1534, ValidA=0.9695\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.0500 | Penalty: 0.00 | Final Reward: 0.0500\n",
            "MetaL:-4.7538(A:-5.5014,C:1.5000,E:4.7291) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 24/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9699\n",
            "PreVal: ValidL=0.1521, ValidA=0.9698\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 6,738,874 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1670, TrainA=0.9455\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1550, TrainA=0.9491\n",
            "  PostE15: TrainL=0.1291, TrainA=0.9570\n",
            "  PostE20: TrainL=0.1125, TrainA=0.9628\n",
            "  PostE25: TrainL=0.1085, TrainA=0.9640\n",
            "PostVal: ValidL=0.1590, ValidA=0.9700\n",
            "  *** New Best Global Accuracy! 0.9699 -> 0.9700 ***\n",
            "Reward: 0.0154 | Penalty: 0.00 | Final Reward: 0.0154\n",
            "MetaL:0.0650(A:-0.8033,C:1.7375,E:1.0324) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 25/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9700\n",
            "PreVal: ValidL=0.1577, ValidA=0.9701\n",
            "TargEdit:Typ=0 Stg=2 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 7,700,282 (Ratio: 1.14)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 28 post-edit epochs.\n",
            "  PostE5: TrainL=0.1868, TrainA=0.9419\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1544, TrainA=0.9495\n",
            "  PostE15: TrainL=0.1388, TrainA=0.9546\n",
            "  PostE20: TrainL=0.1191, TrainA=0.9609\n",
            "  PostE25: TrainL=0.1042, TrainA=0.9650\n",
            "  PostE28: TrainL=0.0996, TrainA=0.9665\n",
            "PostVal: ValidL=0.1830, ValidA=0.9672\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.2848 | Penalty: 0.00 | Final Reward: -0.2848\n",
            "MetaL:-5.2815(A:-6.8049,C:3.0491,E:2.3486) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 26/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9700\n",
            "PreVal: ValidL=0.1582, ValidA=0.9704\n",
            "TargEdit:Typ=1 Stg=1 Op=11,RszF=0.75\n",
            "  TargetCNN arch changed. New Params: 6,922,502 (Ratio: 0.90)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1622, TrainA=0.9477\n",
            "  PostE10: TrainL=0.1428, TrainA=0.9534\n",
            "  PostE15: TrainL=0.1223, TrainA=0.9597\n",
            "  PostE20: TrainL=0.1037, TrainA=0.9645\n",
            "  PostE25: TrainL=0.0972, TrainA=0.9671\n",
            "PostVal: ValidL=0.2533, ValidA=0.9682\n",
            "  No improvement for 2 iterations.\n",
            "Reward: -0.2155 | Penalty: 0.00 | Final Reward: -0.2155\n",
            "MetaL:-5.3124(A:-6.8433,C:3.0664,E:4.5486) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 27/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9700\n",
            "PreVal: ValidL=0.5074, ValidA=0.9657\n",
            "TargEdit:Typ=1 Stg=1 Op=0,RszF=1.25\n",
            "  TargetCNN arch changed. New Params: 7,102,310 (Ratio: 1.03)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1468, TrainA=0.9513\n",
            "  PostE10: TrainL=0.1323, TrainA=0.9560\n",
            "  PostE15: TrainL=0.1212, TrainA=0.9619\n",
            "  PostE20: TrainL=0.1005, TrainA=0.9669\n",
            "  PostE25: TrainL=0.0948, TrainA=0.9689\n",
            "PostVal: ValidL=0.1609, ValidA=0.9717\n",
            "  *** New Best Global Accuracy! 0.9700 -> 0.9717 ***\n",
            "Reward: 0.6004 | Penalty: 0.00 | Final Reward: 0.6004\n",
            "MetaL:-4.1867(A:-4.7179,C:1.0669,E:4.5257) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 28/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9717\n",
            "PreVal: ValidL=0.1617, ValidA=0.9712\n",
            "TargEdit:Typ=1 Stg=2 Op=6,RszF=1.25\n",
            "  TargetCNN arch changed. New Params: 7,356,998 (Ratio: 1.04)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1453, TrainA=0.9529\n",
            "  PostE10: TrainL=0.1259, TrainA=0.9586\n",
            "  PostE15: TrainL=0.1094, TrainA=0.9632\n",
            "  PostE20: TrainL=0.0937, TrainA=0.9687\n",
            "  PostE25: TrainL=0.0935, TrainA=0.9691\n",
            "PostVal: ValidL=0.1589, ValidA=0.9716\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.0423 | Penalty: 0.00 | Final Reward: 0.0423\n",
            "MetaL:-6.0808(A:-7.2813,C:2.4052,E:4.1811) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 29/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9717\n",
            "PreVal: ValidL=0.1584, ValidA=0.9717\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,423,302 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1391, TrainA=0.9548\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1257, TrainA=0.9585\n",
            "  PostE15: TrainL=0.1055, TrainA=0.9649\n",
            "  PostE20: TrainL=0.0940, TrainA=0.9691\n",
            "  PostE25: TrainL=0.0880, TrainA=0.9707\n",
            "PostVal: ValidL=0.6084, ValidA=0.9654\n",
            "  No improvement for 2 iterations.\n",
            "Reward: -0.6312 | Penalty: 0.00 | Final Reward: -0.6312\n",
            "MetaL:1.5477(A:-1.0724,C:5.2410,E:0.9297) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 30/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9717\n",
            "PreVal: ValidL=0.3278, ValidA=0.9687\n",
            "TargEdit:Typ=1 Stg=0 Op=0,RszF=0.75\n",
            "  TargetCNN arch changed. New Params: 7,394,958 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1453, TrainA=0.9534\n",
            "  PostE10: TrainL=0.1191, TrainA=0.9603\n",
            "  PostE15: TrainL=0.1048, TrainA=0.9656\n",
            "  PostE20: TrainL=0.0879, TrainA=0.9707\n",
            "  PostE25: TrainL=0.0841, TrainA=0.9721\n",
            "PostVal: ValidL=0.1964, ValidA=0.9714\n",
            "  No improvement for 3 iterations.\n",
            "Reward: 0.2694 | Penalty: 0.00 | Final Reward: 0.2694\n",
            "MetaL:-4.2375(A:-5.1451,C:1.8188,E:3.7500) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 31/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9717\n",
            "PreVal: ValidL=0.2235, ValidA=0.9711\n",
            "TargEdit:Typ=1 Stg=1 Op=0,RszF=0.5\n",
            "  TargetCNN arch changed. New Params: 6,945,438 (Ratio: 0.94)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1328, TrainA=0.9564\n",
            "  PostE10: TrainL=0.1216, TrainA=0.9593\n",
            "  PostE15: TrainL=0.1036, TrainA=0.9660\n",
            "  PostE20: TrainL=0.0904, TrainA=0.9695\n",
            "  PostE25: TrainL=0.0852, TrainA=0.9714\n",
            "PostVal: ValidL=0.2003, ValidA=0.9713\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 0.0154 | Penalty: 0.00 | Final Reward: 0.0154\n",
            "MetaL:-8.2034(A:-9.6001,C:2.7979,E:4.4473) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 32/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=32, Params: 3,059\n",
            "Current Global Best Accuracy: 0.9717\n",
            "PreVal: ValidL=0.2498, ValidA=0.9707\n",
            "TargEdit:Typ=1 Stg=1 Op=5,RszF=1.25\n",
            "  TargetCNN arch changed. New Params: 7,396,590 (Ratio: 1.06)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 26 post-edit epochs.\n",
            "  PostE5: TrainL=0.1314, TrainA=0.9570\n",
            "  PostE10: TrainL=0.1163, TrainA=0.9620\n",
            "  PostE15: TrainL=0.1047, TrainA=0.9656\n",
            "  PostE20: TrainL=0.0842, TrainA=0.9719\n",
            "  PostE25: TrainL=0.0823, TrainA=0.9724\n",
            "  PostE26: TrainL=0.0810, TrainA=0.9726\n",
            "PostVal: ValidL=0.2861, ValidA=0.9693\n",
            "  No improvement for 5 iterations.\n",
            "Reward: -0.1385 | Penalty: 0.00 | Final Reward: -0.1385\n",
            "MetaL:-6.3813(A:-7.9455,C:3.1328,E:4.4409) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9717).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Widening MetaAgentGNN: GNN Hidden Dim 32 -> 48\n",
            "MetaAgentGNN arch changed. New Params: 4915. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 33/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9717\n",
            "PreVal: ValidL=0.1896, ValidA=0.9584\n",
            "TargEdit:Typ=2 Stg=0 Src=5->Dest=7\n",
            "  TargetCNN arch changed. New Params: 7,102,310 (Ratio: 1.00)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1467, TrainA=0.9510\n",
            "  PostE10: TrainL=0.1317, TrainA=0.9561\n",
            "  PostE15: TrainL=0.1130, TrainA=0.9627\n",
            "  PostE20: TrainL=0.0949, TrainA=0.9683\n",
            "  PostE25: TrainL=0.0935, TrainA=0.9683\n",
            "PostVal: ValidL=0.1551, ValidA=0.9729\n",
            "  *** New Best Global Accuracy! 0.9717 -> 0.9729 ***\n",
            "Reward: 1.4470 | Penalty: 0.00 | Final Reward: 1.4470\n",
            "MetaL:24.3091(A:20.0571,C:8.5089,E:4.7558) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 34/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1556, ValidA=0.9729\n",
            "TargEdit:Typ=1 Stg=2 Op=6,RszF=0.75\n",
            "  TargetCNN arch changed. New Params: 6,847,622 (Ratio: 0.96)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1399, TrainA=0.9547\n",
            "  PostE10: TrainL=0.1205, TrainA=0.9602\n",
            "  PostE15: TrainL=0.1032, TrainA=0.9656\n",
            "  PostE20: TrainL=0.0900, TrainA=0.9699\n",
            "  PostE25: TrainL=0.0865, TrainA=0.9705\n",
            "PostVal: ValidL=0.7322, ValidA=0.9676\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.5311 | Penalty: 0.00 | Final Reward: -0.5311\n",
            "MetaL:6.5098(A:6.0511,C:0.9218,E:4.4404) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 35/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.5229, ValidA=0.9684\n",
            "TargEdit:Typ=2 Stg=1 Src=6->Dest=11\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1299, TrainA=0.9577\n",
            "  PostE10: TrainL=0.1166, TrainA=0.9608\n",
            "  PostE15: TrainL=0.0981, TrainA=0.9672\n",
            "  PostE20: TrainL=0.0903, TrainA=0.9698\n",
            "  PostE25: TrainL=0.0840, TrainA=0.9719\n",
            "PostVal: ValidL=0.1853, ValidA=0.9719\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.3541 | Penalty: 0.00 | Final Reward: 0.3541\n",
            "MetaL:12.7205(A:11.1577,C:3.1316,E:5.9948) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 36/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1794, ValidA=0.9719\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 6,913,926 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1276, TrainA=0.9592\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1192, TrainA=0.9605\n",
            "  PostE15: TrainL=0.1015, TrainA=0.9663\n",
            "  PostE20: TrainL=0.0871, TrainA=0.9713\n",
            "  PostE25: TrainL=0.0804, TrainA=0.9731\n",
            "PostVal: ValidL=0.1670, ValidA=0.9714\n",
            "  No improvement for 3 iterations.\n",
            "Reward: -0.0462 | Penalty: 0.00 | Final Reward: -0.0462\n",
            "MetaL:1.7146(A:0.7455,C:1.9394,E:1.0870) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 37/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1662, ValidA=0.9714\n",
            "TargEdit:Typ=2 Stg=2 Src=-1->Dest=4\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1279, TrainA=0.9588\n",
            "  PostE10: TrainL=0.1169, TrainA=0.9629\n",
            "  PostE15: TrainL=0.0983, TrainA=0.9676\n",
            "  PostE20: TrainL=0.0843, TrainA=0.9714\n",
            "  PostE25: TrainL=0.0789, TrainA=0.9737\n",
            "PostVal: ValidL=0.5255, ValidA=0.9686\n",
            "  No improvement for 4 iterations.\n",
            "Reward: -0.2771 | Penalty: 0.00 | Final Reward: -0.2771\n",
            "MetaL:7.6495(A:6.9815,C:1.3419,E:5.7939) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 38/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=2, GNN Hidden=48, Params: 4,915\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.4493, ValidA=0.9689\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 6,980,230 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1269, TrainA=0.9586\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1171, TrainA=0.9631\n",
            "  PostE15: TrainL=0.0972, TrainA=0.9680\n",
            "  PostE20: TrainL=0.0836, TrainA=0.9723\n",
            "  PostE25: TrainL=0.0805, TrainA=0.9728\n",
            "PostVal: ValidL=0.2167, ValidA=0.9675\n",
            "  No improvement for 5 iterations.\n",
            "Reward: -0.1347 | Penalty: 0.00 | Final Reward: -0.1347\n",
            "MetaL:1.5098(A:0.6781,C:1.6644,E:1.0798) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepening MetaAgentGNN: GNN Layers 2 -> 3\n",
            "MetaAgentGNN arch changed. New Params: 7267. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 39/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1699, ValidA=0.9666\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1423, TrainA=0.9533\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1268, TrainA=0.9584\n",
            "  PostE15: TrainL=0.1097, TrainA=0.9634\n",
            "  PostE20: TrainL=0.0919, TrainA=0.9690\n",
            "  PostE25: TrainL=0.0870, TrainA=0.9711\n",
            "PostVal: ValidL=0.2240, ValidA=0.9699\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.3310 | Penalty: 0.00 | Final Reward: 0.3310\n",
            "MetaL:2.4527(A:0.9208,C:3.0649,E:1.0822) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 40/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.2371, ValidA=0.9698\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1461, TrainA=0.9557\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1229, TrainA=0.9599\n",
            "  PostE15: TrainL=0.1047, TrainA=0.9657\n",
            "  PostE20: TrainL=0.0948, TrainA=0.9688\n",
            "  PostE25: TrainL=0.0845, TrainA=0.9713\n",
            "PostVal: ValidL=0.1696, ValidA=0.9703\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.0500 | Penalty: 0.00 | Final Reward: 0.0500\n",
            "MetaL:1.8425(A:0.7647,C:2.1566,E:1.0744) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 41/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1563, ValidA=0.9710\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1358, TrainA=0.9560\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1205, TrainA=0.9604\n",
            "  PostE15: TrainL=0.1038, TrainA=0.9655\n",
            "  PostE20: TrainL=0.0897, TrainA=0.9698\n",
            "  PostE25: TrainL=0.0884, TrainA=0.9707\n",
            "PostVal: ValidL=0.1543, ValidA=0.9724\n",
            "  No improvement for 3 iterations.\n",
            "Reward: 0.1385 | Penalty: 0.00 | Final Reward: 0.1385\n",
            "MetaL:2.1143(A:0.8451,C:2.5394,E:1.0864) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 42/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1546, ValidA=0.9723\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,367,526 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1329, TrainA=0.9570\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1217, TrainA=0.9613\n",
            "  PostE15: TrainL=0.1055, TrainA=0.9655\n",
            "  PostE20: TrainL=0.0939, TrainA=0.9707\n",
            "  PostE25: TrainL=0.0810, TrainA=0.9730\n",
            "PostVal: ValidL=0.2138, ValidA=0.9697\n",
            "  No improvement for 4 iterations.\n",
            "Reward: -0.2579 | Penalty: 0.00 | Final Reward: -0.2579\n",
            "MetaL:1.3293(A:0.6173,C:1.4251,E:1.0704) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 43/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,267\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.4026, ValidA=0.9677\n",
            "TargEdit:Typ=3\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1238, TrainA=0.9602\n",
            "  PostE10: TrainL=0.1161, TrainA=0.9623\n",
            "  PostE15: TrainL=0.0982, TrainA=0.9670\n",
            "  PostE20: TrainL=0.0852, TrainA=0.9714\n",
            "  PostE25: TrainL=0.0784, TrainA=0.9738\n",
            "PostVal: ValidL=0.1644, ValidA=0.9708\n",
            "  No improvement for 5 iterations.\n",
            "Reward: 0.3117 | Penalty: 0.00 | Final Reward: 0.3117\n",
            "MetaL:2.4658(A:0.8875,C:3.1576,E:1.0555) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepened MLP head 'head_target_resize_factor' (Linear -> Sequential)\n",
            "MetaAgentGNN arch changed. New Params: 7309. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 44/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1679, ValidA=0.9661\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1410, TrainA=0.9536\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1296, TrainA=0.9566\n",
            "  PostE15: TrainL=0.1128, TrainA=0.9630\n",
            "  PostE20: TrainL=0.0961, TrainA=0.9686\n",
            "  PostE25: TrainL=0.0927, TrainA=0.9697\n",
            "PostVal: ValidL=0.4851, ValidA=0.9621\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.3964 | Penalty: 0.00 | Final Reward: -0.3964\n",
            "MetaL:1.1612(A:0.5712,C:1.1811,E:1.0781) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 45/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.3318, ValidA=0.9652\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1417, TrainA=0.9563\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1222, TrainA=0.9604\n",
            "  PostE15: TrainL=0.1066, TrainA=0.9647\n",
            "  PostE20: TrainL=0.0911, TrainA=0.9694\n",
            "  PostE25: TrainL=0.0890, TrainA=0.9702\n",
            "PostVal: ValidL=0.3352, ValidA=0.9696\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.4387 | Penalty: 0.00 | Final Reward: 0.4387\n",
            "MetaL:2.7622(A:0.9636,C:3.5983,E:1.0601) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 46/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.7563, ValidA=0.9668\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1360, TrainA=0.9566\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1220, TrainA=0.9605\n",
            "  PostE15: TrainL=0.1041, TrainA=0.9662\n",
            "  PostE20: TrainL=0.0888, TrainA=0.9705\n",
            "  PostE25: TrainL=0.0848, TrainA=0.9720\n",
            "PostVal: ValidL=8.3343, ValidA=0.9488\n",
            "  No improvement for 3 iterations.\n",
            "Reward: -1.8011 | Penalty: 0.00 | Final Reward: -1.8011\n",
            "MetaL:-0.1049(A:-0.1491,C:0.0896,E:1.0558) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 47/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=10.1889, ValidA=0.9455\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,367,526 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1343, TrainA=0.9576\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1199, TrainA=0.9615\n",
            "  PostE15: TrainL=0.1027, TrainA=0.9668\n",
            "  PostE20: TrainL=0.0869, TrainA=0.9713\n",
            "  PostE25: TrainL=0.0814, TrainA=0.9727\n",
            "PostVal: ValidL=0.1643, ValidA=0.9709\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 2.5439 | Penalty: 0.00 | Final Reward: 2.5439\n",
            "MetaL:8.5162(A:1.8737,C:13.2859,E:0.9051) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 48/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=48, Params: 7,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1639, ValidA=0.9711\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,433,830 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1264, TrainA=0.9589\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1156, TrainA=0.9622\n",
            "  PostE15: TrainL=0.1016, TrainA=0.9668\n",
            "  PostE20: TrainL=0.0868, TrainA=0.9710\n",
            "  PostE25: TrainL=0.0846, TrainA=0.9734\n",
            "PostVal: ValidL=0.1572, ValidA=0.9711\n",
            "  No improvement for 5 iterations.\n",
            "Reward: -0.0077 | Penalty: 0.00 | Final Reward: -0.0077\n",
            "MetaL:4.8673(A:2.1531,C:5.4298,E:1.2747) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Widening MetaAgentGNN: GNN Hidden Dim 48 -> 72\n",
            "MetaAgentGNN arch changed. New Params: 13957. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 49/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,957\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1567, ValidA=0.9681\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1486, TrainA=0.9525\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1271, TrainA=0.9578\n",
            "  PostE15: TrainL=0.1097, TrainA=0.9640\n",
            "  PostE20: TrainL=0.0959, TrainA=0.9675\n",
            "  PostE25: TrainL=0.0907, TrainA=0.9705\n",
            "PostVal: ValidL=0.2907, ValidA=0.9648\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.3271 | Penalty: 0.00 | Final Reward: -0.3271\n",
            "MetaL:0.7290(A:0.2566,C:0.9456,E:0.7903) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 50/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,957\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.8407, ValidA=0.9539\n",
            "TargEdit:Typ=2 Stg=1 Src=1->Dest=2\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1443, TrainA=0.9546\n",
            "  PostE10: TrainL=0.1214, TrainA=0.9600\n",
            "  PostE15: TrainL=0.1048, TrainA=0.9656\n",
            "  PostE20: TrainL=0.0931, TrainA=0.9691\n",
            "  PostE25: TrainL=0.0845, TrainA=0.9716\n",
            "PostVal: ValidL=1.6704, ValidA=0.9563\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.2348 | Penalty: 0.00 | Final Reward: 0.2348\n",
            "MetaL:7.1184(A:6.4445,C:1.3537,E:6.0308) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 51/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,957\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=1.1835, ValidA=0.9604\n",
            "TargEdit:Typ=3\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1301, TrainA=0.9573\n",
            "  PostE10: TrainL=0.1138, TrainA=0.9622\n",
            "  PostE15: TrainL=0.0991, TrainA=0.9675\n",
            "  PostE20: TrainL=0.0835, TrainA=0.9720\n",
            "  PostE25: TrainL=0.0807, TrainA=0.9727\n",
            "PostVal: ValidL=0.6887, ValidA=0.9699\n",
            "  No improvement for 3 iterations.\n",
            "Reward: 0.9467 | Penalty: 0.00 | Final Reward: 0.9467\n",
            "MetaL:1.3412(A:0.1227,C:2.4373,E:0.3306) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 52/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,957\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=2.9749, ValidA=0.9671\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1337, TrainA=0.9563\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1186, TrainA=0.9612\n",
            "  PostE15: TrainL=0.1045, TrainA=0.9671\n",
            "  PostE20: TrainL=0.0882, TrainA=0.9717\n",
            "  PostE25: TrainL=0.0816, TrainA=0.9722\n",
            "PostVal: ValidL=1.3555, ValidA=0.9699\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 0.2848 | Penalty: 0.00 | Final Reward: 0.2848\n",
            "MetaL:0.4710(A:0.2279,C:0.4870,E:0.8645) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 53/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,957\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.6678, ValidA=0.9706\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1264, TrainA=0.9590\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1132, TrainA=0.9628\n",
            "  PostE15: TrainL=0.0988, TrainA=0.9676\n",
            "  PostE20: TrainL=0.0848, TrainA=0.9715\n",
            "  PostE25: TrainL=0.0797, TrainA=0.9734\n",
            "PostVal: ValidL=9.3812, ValidA=0.9553\n",
            "  No improvement for 5 iterations.\n",
            "Reward: -1.5317 | Penalty: 0.00 | Final Reward: -1.5317\n",
            "MetaL:0.0367(A:-0.3229,C:0.7201,E:0.9069) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepened Seq head 'head_target_resize_factor'.\n",
            "MetaAgentGNN arch changed. New Params: 13999. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 54/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,999\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1707, ValidA=0.9660\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1411, TrainA=0.9553\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1250, TrainA=0.9585\n",
            "  PostE15: TrainL=0.1083, TrainA=0.9639\n",
            "  PostE20: TrainL=0.0925, TrainA=0.9693\n",
            "  PostE25: TrainL=0.0883, TrainA=0.9701\n",
            "PostVal: ValidL=0.1728, ValidA=0.9713\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.5272 | Penalty: 0.00 | Final Reward: 0.5272\n",
            "MetaL:1.4087(A:0.4983,C:1.8217,E:0.9065) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 55/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,999\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1685, ValidA=0.9711\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1409, TrainA=0.9538\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1368, TrainA=0.9571\n",
            "  PostE15: TrainL=0.1058, TrainA=0.9657\n",
            "  PostE20: TrainL=0.0916, TrainA=0.9702\n",
            "  PostE25: TrainL=0.0844, TrainA=0.9714\n",
            "PostVal: ValidL=0.1613, ValidA=0.9715\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.0385 | Penalty: 0.00 | Final Reward: 0.0385\n",
            "MetaL:0.6758(A:0.3864,C:0.5798,E:1.0144) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 56/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,999\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1628, ValidA=0.9716\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1381, TrainA=0.9557\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1190, TrainA=0.9614\n",
            "  PostE15: TrainL=0.1059, TrainA=0.9655\n",
            "  PostE20: TrainL=0.0929, TrainA=0.9689\n",
            "  PostE25: TrainL=0.0840, TrainA=0.9718\n",
            "PostVal: ValidL=3.1917, ValidA=0.9504\n",
            "  No improvement for 3 iterations.\n",
            "Reward: -2.1205 | Penalty: 0.00 | Final Reward: -2.1205\n",
            "MetaL:0.6784(A:-0.4410,C:2.2396,E:0.7843) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 57/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,999\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=5.0577, ValidA=0.9450\n",
            "TargEdit:Typ=3\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1278, TrainA=0.9581\n",
            "  PostE10: TrainL=0.1167, TrainA=0.9616\n",
            "  PostE15: TrainL=0.0999, TrainA=0.9676\n",
            "  PostE20: TrainL=0.0888, TrainA=0.9717\n",
            "  PostE25: TrainL=0.0840, TrainA=0.9731\n",
            "PostVal: ValidL=4.3090, ValidA=0.9536\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 0.8544 | Penalty: 0.00 | Final Reward: 0.8544\n",
            "MetaL:0.8055(A:0.2772,C:1.0572,E:0.7886) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 58/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=3, GNN Hidden=72, Params: 13,999\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=4.7799, ValidA=0.9524\n",
            "TargEdit:Typ=2 Stg=1 Src=9->Dest=11\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1200, TrainA=0.9614\n",
            "  PostE10: TrainL=0.1117, TrainA=0.9631\n",
            "  PostE15: TrainL=0.0938, TrainA=0.9686\n",
            "  PostE20: TrainL=0.0818, TrainA=0.9727\n",
            "  PostE25: TrainL=0.0772, TrainA=0.9742\n",
            "PostVal: ValidL=0.1569, ValidA=0.9721\n",
            "  No improvement for 5 iterations.\n",
            "Reward: 1.9743 | Penalty: 0.00 | Final Reward: 1.9743\n",
            "MetaL:17.6564(A:15.1171,C:5.0840,E:5.4187) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepening MetaAgentGNN: GNN Layers 3 -> 4\n",
            "MetaAgentGNN arch changed. New Params: 19255. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 59/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,255\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1499, ValidA=0.9703\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1412, TrainA=0.9534\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1231, TrainA=0.9592\n",
            "  PostE15: TrainL=0.1080, TrainA=0.9654\n",
            "  PostE20: TrainL=0.0932, TrainA=0.9687\n",
            "  PostE25: TrainL=0.0865, TrainA=0.9703\n",
            "PostVal: ValidL=0.1636, ValidA=0.9712\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.0924 | Penalty: 0.00 | Final Reward: 0.0924\n",
            "MetaL:0.8865(A:0.3083,C:1.1573,E:0.7947) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 60/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,255\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1589, ValidA=0.9711\n",
            "TargEdit:Typ=0 Stg=1 CHM=1.0\n",
            "  TargetCNN arch changed. New Params: 7,825,254 (Ratio: 1.09)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 27 post-edit epochs.\n",
            "  PostE5: TrainL=0.1557, TrainA=0.9486\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1401, TrainA=0.9539\n",
            "  PostE15: TrainL=0.1226, TrainA=0.9605\n",
            "  PostE20: TrainL=0.0983, TrainA=0.9666\n",
            "  PostE25: TrainL=0.0879, TrainA=0.9699\n",
            "  PostE27: TrainL=0.0897, TrainA=0.9697\n",
            "PostVal: ValidL=0.1578, ValidA=0.9711\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.0000 | Penalty: 0.00 | Final Reward: 0.0000\n",
            "MetaL:5.8454(A:5.2604,C:1.1726,E:2.6777) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 61/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,255\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1572, ValidA=0.9710\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,891,558 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1535, TrainA=0.9514\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1327, TrainA=0.9566\n",
            "  PostE15: TrainL=0.1098, TrainA=0.9632\n",
            "  PostE20: TrainL=0.0927, TrainA=0.9696\n",
            "  PostE25: TrainL=0.0864, TrainA=0.9711\n",
            "PostVal: ValidL=0.1553, ValidA=0.9718\n",
            "  No improvement for 3 iterations.\n",
            "Reward: 0.0770 | Penalty: 0.00 | Final Reward: 0.0770\n",
            "MetaL:0.6881(A:0.3150,C:0.7471,E:0.8583) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 62/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,255\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1775, ValidA=0.9716\n",
            "TargEdit:Typ=2 Stg=1 Src=11->Dest=16\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1375, TrainA=0.9551\n",
            "  PostE10: TrainL=0.1252, TrainA=0.9593\n",
            "  PostE15: TrainL=0.1056, TrainA=0.9649\n",
            "  PostE20: TrainL=0.0909, TrainA=0.9690\n",
            "  PostE25: TrainL=0.0851, TrainA=0.9713\n",
            "PostVal: ValidL=0.1606, ValidA=0.9712\n",
            "  No improvement for 4 iterations.\n",
            "Reward: -0.0385 | Penalty: 0.00 | Final Reward: -0.0385\n",
            "MetaL:4.0260(A:3.8763,C:0.3051,E:5.7924) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 63/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,255\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1598, ValidA=0.9711\n",
            "TargEdit:Typ=1 Stg=2 Op=6,RszF=0.75\n",
            "  TargetCNN arch changed. New Params: 7,636,870 (Ratio: 0.97)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1294, TrainA=0.9574\n",
            "  PostE10: TrainL=0.1183, TrainA=0.9611\n",
            "  PostE15: TrainL=0.1000, TrainA=0.9675\n",
            "  PostE20: TrainL=0.0858, TrainA=0.9718\n",
            "  PostE25: TrainL=0.0800, TrainA=0.9735\n",
            "PostVal: ValidL=0.1569, ValidA=0.9719\n",
            "  No improvement for 5 iterations.\n",
            "Reward: 0.0770 | Penalty: 0.00 | Final Reward: 0.0770\n",
            "MetaL:3.2737(A:3.0942,C:0.3628,E:3.8370) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepened Seq head 'head_target_resize_factor'.\n",
            "MetaAgentGNN arch changed. New Params: 19297. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 64/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,297\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1554, ValidA=0.9701\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1548, TrainA=0.9528\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1257, TrainA=0.9588\n",
            "  PostE15: TrainL=0.1086, TrainA=0.9639\n",
            "  PostE20: TrainL=0.0947, TrainA=0.9688\n",
            "  PostE25: TrainL=0.0943, TrainA=0.9686\n",
            "PostVal: ValidL=0.1555, ValidA=0.9720\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.1924 | Penalty: 0.00 | Final Reward: 0.1924\n",
            "MetaL:0.6495(A:0.3379,C:0.6242,E:0.9391) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 65/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,297\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1533, ValidA=0.9722\n",
            "TargEdit:Typ=2 Stg=1 Src=1->Dest=2\n",
            "  TargetCNN arch changed. New Params: 6,219,882 (Ratio: 0.87)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.3377, TrainA=0.8915\n",
            "  PostE10: TrainL=0.2813, TrainA=0.9094\n",
            "  PostE15: TrainL=0.2376, TrainA=0.9226\n",
            "  PostE20: TrainL=0.2061, TrainA=0.9327\n",
            "  PostE25: TrainL=0.1954, TrainA=0.9356\n",
            "PostVal: ValidL=0.1446, ValidA=0.9658\n",
            "  No improvement for 2 iterations.\n",
            "Reward: -0.6350 | Penalty: 0.00 | Final Reward: -0.6350\n",
            "MetaL:-0.3062(A:-0.3045,C:0.0025,E:5.8629) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 66/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,297\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1447, ValidA=0.9660\n",
            "TargEdit:Typ=2 Stg=1 Src=3->Dest=4\n",
            "  TargetCNN arch changed. New Params: 7,064,106 (Ratio: 1.14)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 28 post-edit epochs.\n",
            "  PostE5: TrainL=0.3187, TrainA=0.8963\n",
            "  PostE10: TrainL=0.2781, TrainA=0.9093\n",
            "  PostE15: TrainL=0.2447, TrainA=0.9190\n",
            "  PostE20: TrainL=0.2241, TrainA=0.9261\n",
            "  PostE25: TrainL=0.2118, TrainA=0.9291\n",
            "  PostE28: TrainL=0.2121, TrainA=0.9299\n",
            "PostVal: ValidL=0.3190, ValidA=0.9614\n",
            "  No improvement for 3 iterations.\n",
            "Reward: -0.4618 | Penalty: 0.00 | Final Reward: -0.4618\n",
            "MetaL:1.1954(A:1.1812,C:0.0345,E:6.0657) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 67/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,297\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.2399, ValidA=0.9630\n",
            "TargEdit:Typ=0 Stg=1 CHM=2.0\n",
            "  TargetCNN arch changed. New Params: 8,931,306 (Ratio: 1.26)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Large parameter jump. Activating LR warmup.\n",
            "  Training for 31 post-edit epochs.\n",
            "  PostE5: TrainL=0.2493, TrainA=0.9199\n",
            "  Warmup complete. Restoring LR to 1.00e-03.\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.2238, TrainA=0.9269\n",
            "  PostE15: TrainL=0.1990, TrainA=0.9351\n",
            "  PostE20: TrainL=0.1746, TrainA=0.9422\n",
            "  PostE25: TrainL=0.1484, TrainA=0.9513\n",
            "  PostE30: TrainL=0.1348, TrainA=0.9551\n",
            "  PostE31: TrainL=0.1335, TrainA=0.9553\n",
            "PostVal: ValidL=0.1595, ValidA=0.9688\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 0.5734 | Penalty: 0.00 | Final Reward: 0.5734\n",
            "MetaL:5.1328(A:4.5083,C:1.2516,E:2.4685) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 68/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,297\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1612, ValidA=0.9689\n",
            "TargEdit:Typ=3\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1851, TrainA=0.9390\n",
            "  PostE10: TrainL=0.1675, TrainA=0.9448\n",
            "  PostE15: TrainL=0.1470, TrainA=0.9517\n",
            "  PostE20: TrainL=0.1265, TrainA=0.9574\n",
            "  PostE25: TrainL=0.1185, TrainA=0.9590\n",
            "PostVal: ValidL=0.1694, ValidA=0.9684\n",
            "  No improvement for 5 iterations.\n",
            "Reward: -0.0423 | Penalty: 0.00 | Final Reward: -0.0423\n",
            "MetaL:0.3037(A:0.1718,C:0.2647,E:0.8569) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepened MLP head 'head_target_loc_stage' (Linear -> Sequential)\n",
            "MetaAgentGNN arch changed. New Params: 19309. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 69/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1581, ValidA=0.9671\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1403, TrainA=0.9541\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1248, TrainA=0.9588\n",
            "  PostE15: TrainL=0.1108, TrainA=0.9638\n",
            "  PostE20: TrainL=0.0908, TrainA=0.9694\n",
            "  PostE25: TrainL=0.0859, TrainA=0.9710\n",
            "PostVal: ValidL=0.1554, ValidA=0.9723\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.5157 | Penalty: 0.00 | Final Reward: 0.5157\n",
            "MetaL:0.8810(A:0.3705,C:1.0218,E:0.8767) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 70/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1538, ValidA=0.9725\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1430, TrainA=0.9560\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1220, TrainA=0.9598\n",
            "  PostE15: TrainL=0.1067, TrainA=0.9657\n",
            "  PostE20: TrainL=0.0907, TrainA=0.9686\n",
            "  PostE25: TrainL=0.0855, TrainA=0.9716\n",
            "PostVal: ValidL=1.1355, ValidA=0.9677\n",
            "  No improvement for 2 iterations.\n",
            "Reward: -0.4849 | Penalty: 0.00 | Final Reward: -0.4849\n",
            "MetaL:-0.0038(A:-0.0034,C:0.0001,E:0.8928) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 71/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=1.4509, ValidA=0.9659\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1329, TrainA=0.9570\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1313, TrainA=0.9591\n",
            "  PostE15: TrainL=0.1057, TrainA=0.9666\n",
            "  PostE20: TrainL=0.0903, TrainA=0.9705\n",
            "  PostE25: TrainL=0.0823, TrainA=0.9722\n",
            "PostVal: ValidL=0.1590, ValidA=0.9714\n",
            "  No improvement for 3 iterations.\n",
            "Reward: 0.5580 | Penalty: 0.00 | Final Reward: 0.5580\n",
            "MetaL:1.1325(A:0.4725,C:1.3211,E:0.9532) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 72/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1627, ValidA=0.9708\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,367,526 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1376, TrainA=0.9571\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1176, TrainA=0.9614\n",
            "  PostE15: TrainL=0.1024, TrainA=0.9672\n",
            "  PostE20: TrainL=0.0940, TrainA=0.9701\n",
            "  PostE25: TrainL=0.0839, TrainA=0.9720\n",
            "PostVal: ValidL=0.1866, ValidA=0.9706\n",
            "  No improvement for 4 iterations.\n",
            "Reward: -0.0231 | Penalty: 0.00 | Final Reward: -0.0231\n",
            "MetaL:0.3536(A:0.2214,C:0.2654,E:0.9339) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 73/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=4, GNN Hidden=72, Params: 19,309\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.2966, ValidA=0.9699\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,433,830 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1304, TrainA=0.9579\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1210, TrainA=0.9622\n",
            "  PostE15: TrainL=0.1026, TrainA=0.9669\n",
            "  PostE20: TrainL=0.0873, TrainA=0.9721\n",
            "  PostE25: TrainL=0.0842, TrainA=0.9719\n",
            "PostVal: ValidL=0.1566, ValidA=0.9704\n",
            "  No improvement for 5 iterations.\n",
            "Reward: 0.0423 | Penalty: 0.00 | Final Reward: 0.0423\n",
            "MetaL:0.4474(A:0.2343,C:0.4272,E:0.8813) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepening MetaAgentGNN: GNN Layers 4 -> 5\n",
            "MetaAgentGNN arch changed. New Params: 24565. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 74/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=72, Params: 24,565\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1641, ValidA=0.9684\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1460, TrainA=0.9525\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1255, TrainA=0.9590\n",
            "  PostE15: TrainL=0.1118, TrainA=0.9628\n",
            "  PostE20: TrainL=0.0947, TrainA=0.9679\n",
            "  PostE25: TrainL=0.0874, TrainA=0.9707\n",
            "PostVal: ValidL=0.3575, ValidA=0.9675\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.0885 | Penalty: 0.00 | Final Reward: -0.0885\n",
            "MetaL:0.3712(A:0.2480,C:0.2474,E:1.0007) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 75/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=72, Params: 24,565\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.2282, ValidA=0.9698\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1375, TrainA=0.9560\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1415, TrainA=0.9559\n",
            "  PostE15: TrainL=0.1091, TrainA=0.9639\n",
            "  PostE20: TrainL=0.0937, TrainA=0.9688\n",
            "  PostE25: TrainL=0.0860, TrainA=0.9720\n",
            "PostVal: ValidL=0.1575, ValidA=0.9706\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.0847 | Penalty: 0.00 | Final Reward: 0.0847\n",
            "MetaL:0.4254(A:0.2228,C:0.4061,E:0.8534) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 76/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=72, Params: 24,565\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1589, ValidA=0.9704\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1335, TrainA=0.9566\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1231, TrainA=0.9590\n",
            "  PostE15: TrainL=0.1051, TrainA=0.9654\n",
            "  PostE20: TrainL=0.0859, TrainA=0.9713\n",
            "  PostE25: TrainL=0.0841, TrainA=0.9715\n",
            "PostVal: ValidL=0.1709, ValidA=0.9696\n",
            "  No improvement for 3 iterations.\n",
            "Reward: -0.0808 | Penalty: 0.00 | Final Reward: -0.0808\n",
            "MetaL:0.3250(A:0.2077,C:0.2354,E:0.9405) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 77/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=72, Params: 24,565\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1630, ValidA=0.9699\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,367,526 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1359, TrainA=0.9566\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1213, TrainA=0.9608\n",
            "  PostE15: TrainL=0.1080, TrainA=0.9648\n",
            "  PostE20: TrainL=0.0962, TrainA=0.9700\n",
            "  PostE25: TrainL=0.0858, TrainA=0.9715\n",
            "PostVal: ValidL=0.1600, ValidA=0.9704\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 0.0500 | Penalty: 0.00 | Final Reward: 0.0500\n",
            "MetaL:0.4042(A:0.2323,C:0.3447,E:0.9055) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 78/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=72, Params: 24,565\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1597, ValidA=0.9703\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,433,830 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1347, TrainA=0.9568\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1202, TrainA=0.9626\n",
            "  PostE15: TrainL=0.1018, TrainA=0.9665\n",
            "  PostE20: TrainL=0.0837, TrainA=0.9721\n",
            "  PostE25: TrainL=0.0785, TrainA=0.9738\n",
            "PostVal: ValidL=0.1614, ValidA=0.9711\n",
            "  No improvement for 5 iterations.\n",
            "Reward: 0.0770 | Penalty: 0.00 | Final Reward: 0.0770\n",
            "MetaL:0.4162(A:0.2356,C:0.3621,E:0.8997) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepening MetaAgentGNN: GNN Layers 5 -> 6\n",
            "MetaAgentGNN arch changed. New Params: 29821. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 79/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=72, Params: 29,821\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1584, ValidA=0.9683\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1396, TrainA=0.9539\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1268, TrainA=0.9573\n",
            "  PostE15: TrainL=0.1062, TrainA=0.9646\n",
            "  PostE20: TrainL=0.0921, TrainA=0.9690\n",
            "  PostE25: TrainL=0.0870, TrainA=0.9707\n",
            "PostVal: ValidL=0.1885, ValidA=0.9712\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.2886 | Penalty: 0.00 | Final Reward: 0.2886\n",
            "MetaL:0.6641(A:0.3370,C:0.6552,E:0.9247) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 80/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=72, Params: 29,821\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.2497, ValidA=0.9699\n",
            "TargEdit:Typ=2 Stg=1 Src=5->Dest=11\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1339, TrainA=0.9567\n",
            "  PostE10: TrainL=0.1301, TrainA=0.9598\n",
            "  PostE15: TrainL=0.1074, TrainA=0.9647\n",
            "  PostE20: TrainL=0.0896, TrainA=0.9705\n",
            "  PostE25: TrainL=0.0846, TrainA=0.9719\n",
            "PostVal: ValidL=0.1770, ValidA=0.9711\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.1155 | Penalty: 0.00 | Final Reward: 0.1155\n",
            "MetaL:3.9707(A:3.7741,C:0.3991,E:5.8315) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 81/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=72, Params: 29,821\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1689, ValidA=0.9711\n",
            "TargEdit:Typ=2 Stg=2 Src=-1->Dest=2\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1288, TrainA=0.9577\n",
            "  PostE10: TrainL=0.1154, TrainA=0.9617\n",
            "  PostE15: TrainL=0.0994, TrainA=0.9670\n",
            "  PostE20: TrainL=0.0888, TrainA=0.9703\n",
            "  PostE25: TrainL=0.0820, TrainA=0.9729\n",
            "PostVal: ValidL=0.1711, ValidA=0.9717\n",
            "  No improvement for 3 iterations.\n",
            "Reward: 0.0577 | Penalty: 0.00 | Final Reward: 0.0577\n",
            "MetaL:3.9710(A:3.7922,C:0.3632,E:5.6448) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 82/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=72, Params: 29,821\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1689, ValidA=0.9720\n",
            "TargEdit:Typ=2 Stg=2 Src=3->Dest=6\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1232, TrainA=0.9600\n",
            "  PostE10: TrainL=0.1103, TrainA=0.9625\n",
            "  PostE15: TrainL=0.0986, TrainA=0.9671\n",
            "  PostE20: TrainL=0.0952, TrainA=0.9710\n",
            "  PostE25: TrainL=0.0784, TrainA=0.9740\n",
            "PostVal: ValidL=0.1681, ValidA=0.9722\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 0.0192 | Penalty: 0.00 | Final Reward: 0.0192\n",
            "MetaL:3.4968(A:3.3528,C:0.2935,E:5.6342) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 83/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=72, Params: 29,821\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1670, ValidA=0.9723\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1261, TrainA=0.9582\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1144, TrainA=0.9620\n",
            "  PostE15: TrainL=0.0939, TrainA=0.9683\n",
            "  PostE20: TrainL=0.0815, TrainA=0.9726\n",
            "  PostE25: TrainL=0.0774, TrainA=0.9742\n",
            "PostVal: ValidL=12.6837, ValidA=0.9230\n",
            "  No improvement for 5 iterations.\n",
            "Reward: -4.9338 | Penalty: 0.00 | Final Reward: -4.9338\n",
            "MetaL:7.9258(A:-1.8358,C:19.5242,E:0.9183) | MetaLR: 3.66e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Widening MetaAgentGNN: GNN Hidden Dim 72 -> 108\n",
            "MetaAgentGNN arch changed. New Params: 63661. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "  !!! Accuracy dropped by >4%. Reverting to the global best model (Acc: 0.9729). !!!\n",
            "\n",
            "===== Iteration 84/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,661\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1596, ValidA=0.9667\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1369, TrainA=0.9545\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1266, TrainA=0.9581\n",
            "  PostE15: TrainL=0.1072, TrainA=0.9644\n",
            "  PostE20: TrainL=0.0947, TrainA=0.9685\n",
            "  PostE25: TrainL=0.0883, TrainA=0.9706\n",
            "PostVal: ValidL=0.1603, ValidA=0.9703\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.3502 | Penalty: 0.00 | Final Reward: 0.3502\n",
            "MetaL:6.1290(A:0.0044,C:12.2492,E:0.0098) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 85/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,661\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1602, ValidA=0.9703\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1415, TrainA=0.9567\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1237, TrainA=0.9596\n",
            "  PostE15: TrainL=0.1052, TrainA=0.9652\n",
            "  PostE20: TrainL=0.0887, TrainA=0.9711\n",
            "  PostE25: TrainL=0.0846, TrainA=0.9724\n",
            "PostVal: ValidL=0.1572, ValidA=0.9714\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.1078 | Penalty: 0.00 | Final Reward: 0.1078\n",
            "MetaL:5.2455(A:0.0039,C:10.4833,E:0.0093) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 86/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,661\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1604, ValidA=0.9710\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1354, TrainA=0.9571\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1198, TrainA=0.9605\n",
            "  PostE15: TrainL=0.1027, TrainA=0.9670\n",
            "  PostE20: TrainL=0.0929, TrainA=0.9693\n",
            "  PostE25: TrainL=0.0847, TrainA=0.9715\n",
            "PostVal: ValidL=0.5931, ValidA=0.9677\n",
            "  No improvement for 3 iterations.\n",
            "Reward: -0.3310 | Penalty: 0.00 | Final Reward: -0.3310\n",
            "MetaL:3.8255(A:0.0031,C:7.6447,E:0.0088) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 87/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,661\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=2.5398, ValidA=0.9609\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,367,526 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1341, TrainA=0.9575\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1220, TrainA=0.9605\n",
            "  PostE15: TrainL=0.1022, TrainA=0.9662\n",
            "  PostE20: TrainL=0.0884, TrainA=0.9705\n",
            "  PostE25: TrainL=0.0792, TrainA=0.9741\n",
            "PostVal: ValidL=9.0463, ValidA=0.9631\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 0.2155 | Penalty: 0.00 | Final Reward: 0.2155\n",
            "MetaL:5.2107(A:0.0035,C:10.4144,E:0.0085) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 88/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,661\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=11.3200, ValidA=0.9616\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,433,830 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1289, TrainA=0.9588\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1177, TrainA=0.9614\n",
            "  PostE15: TrainL=0.0981, TrainA=0.9675\n",
            "  PostE20: TrainL=0.0872, TrainA=0.9713\n",
            "  PostE25: TrainL=0.0831, TrainA=0.9719\n",
            "PostVal: ValidL=0.1615, ValidA=0.9701\n",
            "  No improvement for 5 iterations.\n",
            "Reward: 0.8582 | Penalty: 0.00 | Final Reward: 0.8582\n",
            "MetaL:7.5240(A:0.0025,C:15.0431,E:0.0054) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Deepened Seq head 'head_target_loc_stage'.\n",
            "MetaAgentGNN arch changed. New Params: 63673. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 89/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,673\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1581, ValidA=0.9669\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1444, TrainA=0.9535\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1300, TrainA=0.9583\n",
            "  PostE15: TrainL=0.1087, TrainA=0.9640\n",
            "  PostE20: TrainL=0.0938, TrainA=0.9686\n",
            "  PostE25: TrainL=0.0865, TrainA=0.9710\n",
            "PostVal: ValidL=0.1637, ValidA=0.9713\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.4387 | Penalty: 0.00 | Final Reward: 0.4387\n",
            "MetaL:8.8114(A:0.0005,C:17.6217,E:0.0012) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 90/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,673\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1638, ValidA=0.9712\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1367, TrainA=0.9549\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1256, TrainA=0.9589\n",
            "  PostE15: TrainL=0.1059, TrainA=0.9650\n",
            "  PostE20: TrainL=0.0928, TrainA=0.9688\n",
            "  PostE25: TrainL=0.0835, TrainA=0.9720\n",
            "PostVal: ValidL=0.1644, ValidA=0.9705\n",
            "  No improvement for 2 iterations.\n",
            "Reward: -0.0654 | Penalty: 0.00 | Final Reward: -0.0654\n",
            "MetaL:3.1455(A:0.0004,C:6.2901,E:0.0017) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 91/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,673\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1732, ValidA=0.9704\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1355, TrainA=0.9570\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1201, TrainA=0.9608\n",
            "  PostE15: TrainL=0.1031, TrainA=0.9665\n",
            "  PostE20: TrainL=0.0916, TrainA=0.9694\n",
            "  PostE25: TrainL=0.0834, TrainA=0.9717\n",
            "PostVal: ValidL=0.1556, ValidA=0.9718\n",
            "  No improvement for 3 iterations.\n",
            "Reward: 0.1424 | Penalty: 0.00 | Final Reward: 0.1424\n",
            "MetaL:4.4343(A:0.0015,C:8.8656,E:0.0044) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 92/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,673\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1579, ValidA=0.9717\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,367,526 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1350, TrainA=0.9566\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1127, TrainA=0.9632\n",
            "  PostE15: TrainL=0.1008, TrainA=0.9665\n",
            "  PostE20: TrainL=0.0874, TrainA=0.9708\n",
            "  PostE25: TrainL=0.0809, TrainA=0.9727\n",
            "PostVal: ValidL=0.1563, ValidA=0.9719\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 0.0231 | Penalty: 0.00 | Final Reward: 0.0231\n",
            "MetaL:2.8393(A:0.0038,C:5.6710,E:0.0120) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 93/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=108, Params: 63,673\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1551, ValidA=0.9713\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,433,830 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1394, TrainA=0.9566\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1137, TrainA=0.9625\n",
            "  PostE15: TrainL=0.0978, TrainA=0.9682\n",
            "  PostE20: TrainL=0.0831, TrainA=0.9723\n",
            "  PostE25: TrainL=0.0783, TrainA=0.9737\n",
            "PostVal: ValidL=0.1545, ValidA=0.9704\n",
            "  No improvement for 5 iterations.\n",
            "Reward: -0.0885 | Penalty: 0.00 | Final Reward: -0.0885\n",
            "MetaL:3.5962(A:0.0027,C:7.1869,E:0.0081) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Widening MetaAgentGNN: GNN Hidden Dim 108 -> 162\n",
            "MetaAgentGNN arch changed. New Params: 138733. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 94/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=162, Params: 138,733\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1559, ValidA=0.9667\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1454, TrainA=0.9538\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1276, TrainA=0.9585\n",
            "  PostE15: TrainL=0.1109, TrainA=0.9629\n",
            "  PostE20: TrainL=0.0971, TrainA=0.9696\n",
            "  PostE25: TrainL=0.0866, TrainA=0.9711\n",
            "PostVal: ValidL=0.3675, ValidA=0.9660\n",
            "  No improvement for 1 iterations.\n",
            "Reward: -0.0693 | Penalty: 0.00 | Final Reward: -0.0693\n",
            "MetaL:630.0629(A:0.0000,C:1260.1257,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 95/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=162, Params: 138,733\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.2447, ValidA=0.9684\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1415, TrainA=0.9541\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1304, TrainA=0.9597\n",
            "  PostE15: TrainL=0.1069, TrainA=0.9639\n",
            "  PostE20: TrainL=0.0925, TrainA=0.9690\n",
            "  PostE25: TrainL=0.0846, TrainA=0.9720\n",
            "PostVal: ValidL=1.2632, ValidA=0.9594\n",
            "  No improvement for 2 iterations.\n",
            "Reward: -0.9044 | Penalty: 0.00 | Final Reward: -0.9044\n",
            "MetaL:604.6493(A:0.0000,C:1209.2986,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 96/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=162, Params: 138,733\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.6608, ValidA=0.9637\n",
            "TargEdit:Typ=3\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1394, TrainA=0.9567\n",
            "  PostE10: TrainL=0.1218, TrainA=0.9598\n",
            "  PostE15: TrainL=0.1028, TrainA=0.9654\n",
            "  PostE20: TrainL=0.0910, TrainA=0.9692\n",
            "  PostE25: TrainL=0.0836, TrainA=0.9725\n",
            "PostVal: ValidL=1.1210, ValidA=0.9534\n",
            "  No improvement for 3 iterations.\n",
            "Reward: -1.0276 | Penalty: 0.00 | Final Reward: -1.0276\n",
            "MetaL:600.4649(A:0.0000,C:1200.9298,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 97/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=162, Params: 138,733\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=1.3689, ValidA=0.9529\n",
            "TargEdit:Typ=3\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1299, TrainA=0.9590\n",
            "  PostE10: TrainL=0.1155, TrainA=0.9622\n",
            "  PostE15: TrainL=0.0980, TrainA=0.9676\n",
            "  PostE20: TrainL=0.0884, TrainA=0.9705\n",
            "  PostE25: TrainL=0.0817, TrainA=0.9727\n",
            "PostVal: ValidL=0.8085, ValidA=0.9637\n",
            "  No improvement for 4 iterations.\n",
            "Reward: 1.0776 | Penalty: 0.00 | Final Reward: 1.0776\n",
            "MetaL:660.6859(A:0.0000,C:1321.3717,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 98/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=6, GNN Hidden=162, Params: 138,733\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.8076, ValidA=0.9639\n",
            "TargEdit:Typ=3\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "\n",
            "!!! INSTABILITY DETECTED: 3 consecutive edits failed dummy pass. !!!\n",
            "  Attempting to prune (simplify) Meta-Agent...\n",
            "  Pruning MetaAgentGNN: GNN Layers 6 -> 5\n",
            "MetaAgentGNN arch changed. New Params: 112327. Re-init optimizer.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1199, TrainA=0.9610\n",
            "  PostE10: TrainL=0.1130, TrainA=0.9642\n",
            "  PostE15: TrainL=0.0984, TrainA=0.9678\n",
            "  PostE20: TrainL=0.0837, TrainA=0.9724\n",
            "  PostE25: TrainL=0.0782, TrainA=0.9737\n",
            "PostVal: ValidL=0.1694, ValidA=0.9708\n",
            "  No improvement for 5 iterations.\n",
            "Reward: 0.6889 | Penalty: 0.00 | Final Reward: 0.6889\n",
            "MetaL:633.7800(A:0.0000,C:1267.5601,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Widening MetaAgentGNN: GNN Hidden Dim 162 -> 243\n",
            "MetaAgentGNN arch changed. New Params: 246706. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 99/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=243, Params: 246,706\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1564, ValidA=0.9680\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1472, TrainA=0.9541\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1304, TrainA=0.9567\n",
            "  PostE15: TrainL=0.1127, TrainA=0.9622\n",
            "  PostE20: TrainL=0.0942, TrainA=0.9681\n",
            "  PostE25: TrainL=0.0885, TrainA=0.9703\n",
            "PostVal: ValidL=0.1615, ValidA=0.9711\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.3117 | Penalty: 0.00 | Final Reward: 0.3117\n",
            "MetaL:1857.6824(A:0.0000,C:3715.3647,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 100/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=243, Params: 246,706\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1621, ValidA=0.9708\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1466, TrainA=0.9544\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1276, TrainA=0.9580\n",
            "  PostE15: TrainL=0.1107, TrainA=0.9629\n",
            "  PostE20: TrainL=0.0917, TrainA=0.9694\n",
            "  PostE25: TrainL=0.0845, TrainA=0.9714\n",
            "PostVal: ValidL=0.1771, ValidA=0.9721\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.1232 | Penalty: 0.00 | Final Reward: 0.1232\n",
            "MetaL:1836.3600(A:0.0000,C:3672.7200,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 101/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=243, Params: 246,706\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1877, ValidA=0.9713\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1339, TrainA=0.9567\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1229, TrainA=0.9601\n",
            "  PostE15: TrainL=0.1028, TrainA=0.9662\n",
            "  PostE20: TrainL=0.0895, TrainA=0.9696\n",
            "  PostE25: TrainL=0.0836, TrainA=0.9722\n",
            "PostVal: ValidL=0.1585, ValidA=0.9709\n",
            "  No improvement for 3 iterations.\n",
            "Reward: -0.0346 | Penalty: 0.00 | Final Reward: -0.0346\n",
            "MetaL:1813.6107(A:0.0000,C:3627.2214,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 102/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=243, Params: 246,706\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1594, ValidA=0.9709\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,367,526 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1415, TrainA=0.9569\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1183, TrainA=0.9613\n",
            "  PostE15: TrainL=0.1052, TrainA=0.9661\n",
            "  PostE20: TrainL=0.0893, TrainA=0.9707\n",
            "  PostE25: TrainL=0.0818, TrainA=0.9727\n",
            "PostVal: ValidL=0.6672, ValidA=0.9666\n",
            "  No improvement for 4 iterations.\n",
            "Reward: -0.4349 | Penalty: 0.00 | Final Reward: -0.4349\n",
            "MetaL:1770.7208(A:0.0000,C:3541.4417,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 103/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=243, Params: 246,706\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.8643, ValidA=0.9654\n",
            "TargEdit:Typ=3\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1262, TrainA=0.9582\n",
            "  PostE10: TrainL=0.1181, TrainA=0.9633\n",
            "  PostE15: TrainL=0.0962, TrainA=0.9689\n",
            "  PostE20: TrainL=0.0857, TrainA=0.9717\n",
            "  PostE25: TrainL=0.0814, TrainA=0.9730\n",
            "PostVal: ValidL=0.1910, ValidA=0.9710\n",
            "  No improvement for 5 iterations.\n",
            "Reward: 0.5542 | Penalty: 0.00 | Final Reward: 0.5542\n",
            "MetaL:1806.1586(A:0.0000,C:3612.3171,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "!!! STAGNATION DETECTED: 5 iterations without improvement. !!!\n",
            "  Reverting Target CNN to best known model (Acc: 0.9729).\n",
            "  Attempting to upgrade (grow) Meta-Agent...\n",
            "  Widening MetaAgentGNN: GNN Hidden Dim 243 -> 364\n",
            "MetaAgentGNN arch changed. New Params: 545213. Re-init optimizer.\n",
            "  Stagnation counter reset. Continuing search with upgraded agent.\n",
            "\n",
            "\n",
            "===== Iteration 104/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=364, Params: 545,213\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1727, ValidA=0.9663\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,168,614 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1437, TrainA=0.9540\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1314, TrainA=0.9567\n",
            "  PostE15: TrainL=0.1093, TrainA=0.9641\n",
            "  PostE20: TrainL=0.0951, TrainA=0.9683\n",
            "  PostE25: TrainL=0.0865, TrainA=0.9710\n",
            "PostVal: ValidL=0.1623, ValidA=0.9717\n",
            "  No improvement for 1 iterations.\n",
            "Reward: 0.5388 | Penalty: 0.00 | Final Reward: 0.5388\n",
            "MetaL:102480.7656(A:0.0000,C:204961.5312,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 105/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=364, Params: 545,213\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1632, ValidA=0.9716\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,234,918 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1437, TrainA=0.9551\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1299, TrainA=0.9587\n",
            "  PostE15: TrainL=0.1067, TrainA=0.9647\n",
            "  PostE20: TrainL=0.0918, TrainA=0.9694\n",
            "  PostE25: TrainL=0.0897, TrainA=0.9703\n",
            "PostVal: ValidL=0.1644, ValidA=0.9725\n",
            "  No improvement for 2 iterations.\n",
            "Reward: 0.0847 | Penalty: 0.00 | Final Reward: 0.0847\n",
            "MetaL:103105.8594(A:0.0000,C:206211.7188,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 106/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=364, Params: 545,213\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=0.1542, ValidA=0.9720\n",
            "TargEdit:Typ=3\n",
            "  TargetCNN arch changed. New Params: 7,301,222 (Ratio: 1.01)\n",
            "  Creating fresh optimizer for new architecture.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1375, TrainA=0.9546\n",
            "  Unfreezing 1 new BatchNorm layers.\n",
            "  PostE10: TrainL=0.1237, TrainA=0.9595\n",
            "  PostE15: TrainL=0.1043, TrainA=0.9650\n",
            "  PostE20: TrainL=0.0889, TrainA=0.9700\n",
            "  PostE25: TrainL=0.0846, TrainA=0.9721\n",
            "PostVal: ValidL=1.6944, ValidA=0.9602\n",
            "  No improvement for 3 iterations.\n",
            "Reward: -1.1776 | Penalty: 0.00 | Final Reward: -1.1776\n",
            "MetaL:103067.4531(A:0.0000,C:206134.9062,E:0.0000) | MetaLR: 1.00e-05\n",
            "\n",
            "===== Iteration 107/150 =====\n",
            "Current MetaAgentGNN: GNN Layers=5, GNN Hidden=364, Params: 545,213\n",
            "Current Global Best Accuracy: 0.9729\n",
            "PreVal: ValidL=3.5317, ValidA=0.9527\n",
            "TargEdit:Typ=3\n",
            "  !!! Dummy pass failed: NaN/Inf detected in output. Edit is invalid. !!!\n",
            "  Edit rolled back due to dummy pass failure.\n",
            "  Edit was invalid or a no-op. Restoring pre-edit model.\n",
            "  Training for 25 post-edit epochs.\n",
            "  PostE5: TrainL=0.1293, TrainA=0.9589\n",
            "  PostE10: TrainL=0.1245, TrainA=0.9592\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# ==============================================================================\n",
        "#  ENTRY POINT 1: STAGE 1 - BROAD SEARCH AND SAVING (Saves both models)\n",
        "# ==============================================================================\n",
        "#\n",
        "if __name__ == '__main__':\n",
        "    if not PYG_AVAILABLE:\n",
        "        print(\"Exiting: PyTorch Geometric is required for this script.\")\n",
        "        exit()\n",
        "\n",
        "    torch.manual_seed(420)\n",
        "    np.random.seed(420)\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- Mount Google Drive for saving checkpoints ---\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        SAVE_DIR = \"/content/drive/My Drive/DEITI_Checkpoints\"\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab. Models will be saved locally to './checkpoints'.\")\n",
        "        SAVE_DIR = \"./checkpoints\"\n",
        "\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "    print(f\"Model checkpoints will be saved to: {SAVE_DIR}\")\n",
        "\n",
        "    deiti_system = DEITI()\n",
        "\n",
        "    # --- STAGE 1: Broad Search ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" STAGE 1: BROAD SEARCH \" + \"=\"*20)\n",
        "    try:\n",
        "        print(f\"Init TargetCNN P: {sum(p.numel() for p in deiti_system.target_cnn.parameters() if p.requires_grad):,}\")\n",
        "        print(f\"Init MetaAgentGNN: L={deiti_system.meta_agent.current_num_gnn_layers},H={deiti_system.meta_agent.current_gnn_hidden_dim},P={sum(p.numel() for p in deiti_system.meta_agent.parameters()):,}\")\n",
        "\n",
        "        print(\"\\n--- Initial Validation ---\")\n",
        "        initial_loss, initial_acc = deiti_system._validate_target(deiti_system.val_loader)\n",
        "        print(f\"InitVal: L={initial_loss:.4f},A={initial_acc:.4f}\")\n",
        "\n",
        "        deiti_system.best_global_accuracy = initial_acc\n",
        "        deiti_system.best_global_model = copy.deepcopy(deiti_system.target_cnn)\n",
        "\n",
        "        deiti_system.train_loop(iterations=150)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nFATAL RUNTIME ERROR in STAGE 1: {e}\")\n",
        "        traceback.print_exc()\n",
        "        print(\"Stopping execution.\")\n",
        "        raise e\n",
        "\n",
        "    # --- Save the Stage 1 checkpoint ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" SAVING STAGE 1 CHECKPOINT \" + \"=\"*20)\n",
        "    target_cnn_path = os.path.join(SAVE_DIR, \"target_cnn_model_svhn.pth\")\n",
        "    meta_agent_path = os.path.join(SAVE_DIR, \"meta_agent_model_svhn.pth\") # New path for meta-agent\n",
        "    acc_path = os.path.join(SAVE_DIR, \"best_accuracy_svhn.txt\")\n",
        "\n",
        "    try:\n",
        "        if deiti_system.best_global_model:\n",
        "            print(f\"Saving best TargetCNN from search with accuracy: {deiti_system.best_global_accuracy:.4f}\")\n",
        "            torch.save(deiti_system.best_global_model, target_cnn_path)\n",
        "            print(f\"Successfully saved Stage 1 best TargetCNN object to: {target_cnn_path}\")\n",
        "\n",
        "            # Save the final meta-agent from the end of the search\n",
        "            print(\"Saving final Meta-Agent state...\")\n",
        "            torch.save(deiti_system.meta_agent, meta_agent_path)\n",
        "            print(f\"Successfully saved Meta-Agent object to: {meta_agent_path}\")\n",
        "\n",
        "            # Save the best validation accuracy from the search loop\n",
        "            with open(acc_path, \"w\") as f:\n",
        "                f.write(str(deiti_system.best_global_accuracy))\n",
        "            print(f\"Saved best accuracy ({deiti_system.best_global_accuracy:.4f}) to: {acc_path}\")\n",
        "        else:\n",
        "            print(\"No best model was tracked during Stage 1. Cannot save checkpoint.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! FAILED to save checkpoint: {e} !!!\")\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGf_cqedmfZP"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# ==============================================================================\n",
        "#  ENTRY POINT 2: STAGE 2 - LOADING AND FOCUSED, REPEATABLE SEARCH\n",
        "# ==============================================================================\n",
        "#\n",
        "if __name__ == '__main__':\n",
        "    if not PYG_AVAILABLE:\n",
        "        print(\"Exiting: PyTorch Geometric is required for this script.\")\n",
        "        exit()\n",
        "\n",
        "    torch.manual_seed(1337) # Use a different seed for Stage 2 if desired\n",
        "    np.random.seed(1337)\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # --- Mount Google Drive to load checkpoints ---\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        SAVE_DIR = \"/content/drive/My Drive/DEITI_Checkpoints\"\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab. Loading models from local './checkpoints'.\")\n",
        "        SAVE_DIR = \"./checkpoints\"\n",
        "\n",
        "    if not os.path.exists(SAVE_DIR):\n",
        "        print(f\"ERROR: Save directory '{SAVE_DIR}' does not exist. Cannot load model.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Define checkpoint paths ---\n",
        "    target_cnn_path = os.path.join(SAVE_DIR, \"target_cnn_model_svhn.pth\")\n",
        "    meta_agent_path = os.path.join(SAVE_DIR, \"meta_agent_model_svhn.pth\") # New path for meta-agent\n",
        "    acc_path = os.path.join(SAVE_DIR, \"best_accuracy_svhn.txt\")\n",
        "\n",
        "    # --- Load the saved models and accuracy from previous run ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" LOADING PREVIOUS BEST MODELS \" + \"=\"*20)\n",
        "    best_target_cnn_previous = None\n",
        "    loaded_meta_agent = None\n",
        "    previous_best_acc = 0.0\n",
        "\n",
        "    try:\n",
        "        # Load the TargetCNN\n",
        "        best_target_cnn_previous = torch.load(target_cnn_path, map_location=DEVICE, weights_only=False)\n",
        "        best_target_cnn_previous.to(DEVICE)\n",
        "        print(f\"Successfully loaded TargetCNN from: {target_cnn_path}\")\n",
        "\n",
        "        # Load the Meta-Agent\n",
        "        loaded_meta_agent = torch.load(meta_agent_path, map_location=DEVICE, weights_only=False)\n",
        "        loaded_meta_agent.to(DEVICE)\n",
        "        print(f\"Successfully loaded Meta-Agent from: {meta_agent_path}\")\n",
        "\n",
        "        # Load the accuracy\n",
        "        with open(acc_path, \"r\") as f:\n",
        "            previous_best_acc = float(f.read())\n",
        "        print(f\"Loaded previous best accuracy: {previous_best_acc:.4f}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"!!! CHECKPOINT FILE NOT FOUND: {e}. Cannot proceed. !!!\")\n",
        "        exit()\n",
        "    except Exception as e:\n",
        "        print(f\"!!! FAILED to load checkpoint: {e} !!!\")\n",
        "        traceback.print_exc()\n",
        "        exit()\n",
        "\n",
        "    # --- STAGE 2: Focused Search ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" STAGE 2: FOCUSED SEARCH \" + \"=\"*20)\n",
        "\n",
        "    # Initialize a new DEITI system\n",
        "    deiti_system_stage2 = DEITI()\n",
        "\n",
        "    # Replace the default models with the loaded ones\n",
        "    del deiti_system_stage2.target_cnn\n",
        "    del deiti_system_stage2.meta_agent\n",
        "    gc.collect()\n",
        "    deiti_system_stage2.target_cnn = best_target_cnn_previous\n",
        "    deiti_system_stage2.meta_agent = loaded_meta_agent\n",
        "\n",
        "    # Reset optimizer, as it's tied to the new model parameters\n",
        "    deiti_system_stage2.opt_target = None\n",
        "    deiti_system_stage2.opt_meta = optim.Adam(deiti_system_stage2.meta_agent.parameters(), lr=BASE_META_LR) # Re-create optimizer for loaded agent\n",
        "\n",
        "    print(\"Resetting global best trackers for this search session.\")\n",
        "    deiti_system_stage2.best_global_accuracy = previous_best_acc\n",
        "    if deiti_system_stage2.best_global_model is not None:\n",
        "        del deiti_system_stage2.best_global_model\n",
        "    deiti_system_stage2.best_global_model = copy.deepcopy(deiti_system_stage2.target_cnn)\n",
        "\n",
        "    print(f\"Starting Stage 2 with TargetCNN params: {sum(p.numel() for p in deiti_system_stage2.target_cnn.parameters()):,}, Acc: {previous_best_acc:.4f}\")\n",
        "    print(f\"Continuing with Meta-Agent params: {sum(p.numel() for p in deiti_system_stage2.meta_agent.parameters()):,}\")\n",
        "\n",
        "    # Run the training loop for Stage 2\n",
        "    try:\n",
        "        deiti_system_stage2.train_loop(iterations=150)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nFATAL RUNTIME ERROR in STAGE 2: {e}\")\n",
        "        traceback.print_exc()\n",
        "        print(\"Stopping execution.\")\n",
        "        raise e\n",
        "\n",
        "    # --- Save the final best model and accuracy from Stage 2 ---\n",
        "    print(\"\\n\\n\" + \"=\"*20 + \" SAVING FINAL CHECKPOINT FOR THIS STAGE \" + \"=\"*20)\n",
        "\n",
        "    try:\n",
        "        if deiti_system_stage2.best_global_model:\n",
        "            print(f\"Saving final best TargetCNN with accuracy: {deiti_system_stage2.best_global_accuracy:.4f}\")\n",
        "            torch.save(deiti_system_stage2.best_global_model, target_cnn_path)\n",
        "            print(f\"Successfully saved final TargetCNN object to: {target_cnn_path}\")\n",
        "\n",
        "            # Save the final meta-agent state from this run\n",
        "            print(\"Saving final Meta-Agent state...\")\n",
        "            torch.save(deiti_system_stage2.meta_agent, meta_agent_path)\n",
        "            print(f\"Successfully saved Meta-Agent object to: {meta_agent_path}\")\n",
        "\n",
        "            # Overwrite the accuracy file with the new best accuracy for the next run\n",
        "            with open(acc_path, \"w\") as f:\n",
        "                f.write(str(deiti_system_stage2.best_global_accuracy))\n",
        "            print(f\"Updated accuracy file for next run: {acc_path}\")\n",
        "        else:\n",
        "            print(\"No better model was found in this run. Checkpoint remains unchanged.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! FAILED to save final checkpoint: {e} !!!\")\n",
        "        traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP5gjrNqOOhmk9GYAjG2Eqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}